{
  "docusaurus-plugin-content-docs": {
    "default": {
      "loadedVersions": [
        {
          "versionName": "current",
          "label": "Next",
          "banner": null,
          "badge": false,
          "noIndex": false,
          "className": "docs-version-current",
          "path": "/docs",
          "tagsPath": "/docs/tags",
          "isLast": true,
          "routePriority": -1,
          "sidebarFilePath": "/root/blog/sidebars.js",
          "contentPath": "/root/blog/docs",
          "contentPathLocalized": "/root/blog/i18n/zh-CN/docusaurus-plugin-content-docs/current",
          "docs": [
            {
              "unversionedId": "stack/free",
              "id": "stack/free",
              "title": "free命令",
              "description": "free 命令是 Linux 系统中的一个实用工具，用于显示系统中的内存使用情况。通过运行 free 命令，您可以获取有关系统中可用内存、已使用内存以及缓存和交换空间的详细信息。",
              "source": "@site/docs/stack/04-free命令.md",
              "sourceDirName": "stack",
              "slug": "/free",
              "permalink": "/docs/free",
              "draft": false,
              "tags": [],
              "version": "current",
              "sidebarPosition": 4,
              "frontMatter": {
                "id": "free",
                "slug": "/free",
                "title": "free命令",
                "last_update": {
                  "date": "2024/03/13"
                }
              },
              "sidebar": "stack",
              "previous": {
                "title": "lscpu命令",
                "permalink": "/docs/lscpu"
              },
              "next": {
                "title": "top命令",
                "permalink": "/docs/top"
              }
            },
            {
              "unversionedId": "stack/introduction",
              "id": "stack/introduction",
              "title": "README",
              "description": "这里记录笔记",
              "source": "@site/docs/stack/00-introduction.md",
              "sourceDirName": "stack",
              "slug": "/Stack",
              "permalink": "/docs/Stack",
              "draft": false,
              "tags": [],
              "version": "current",
              "sidebarPosition": 0,
              "frontMatter": {
                "id": "introduction",
                "slug": "/Stack",
                "title": "README",
                "last_update": {
                  "date": "2023/04/22"
                }
              },
              "sidebar": "stack",
              "next": {
                "title": "Centos7系统---内核升级",
                "permalink": "/docs/Centos7系统---内核升级"
              }
            },
            {
              "unversionedId": "stack/kernel-update",
              "id": "stack/kernel-update",
              "title": "Centos7系统---内核升级",
              "description": "前言",
              "source": "@site/docs/stack/01-Centos7系统---内核升级.md",
              "sourceDirName": "stack",
              "slug": "/Centos7系统---内核升级",
              "permalink": "/docs/Centos7系统---内核升级",
              "draft": false,
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "id": "kernel-update",
                "slug": "/Centos7系统---内核升级",
                "title": "Centos7系统---内核升级",
                "last_update": {
                  "date": "2023/02/16"
                }
              },
              "sidebar": "stack",
              "previous": {
                "title": "README",
                "permalink": "/docs/Stack"
              },
              "next": {
                "title": "时间同步服务",
                "permalink": "/docs/时间同步服务"
              }
            },
            {
              "unversionedId": "stack/lscpu",
              "id": "stack/lscpu",
              "title": "lscpu命令",
              "description": "简介",
              "source": "@site/docs/stack/03-lscpu命令.md",
              "sourceDirName": "stack",
              "slug": "/lscpu",
              "permalink": "/docs/lscpu",
              "draft": false,
              "tags": [],
              "version": "current",
              "sidebarPosition": 3,
              "frontMatter": {
                "id": "lscpu",
                "slug": "/lscpu",
                "title": "lscpu命令",
                "last_update": {
                  "date": "2024/03/13"
                }
              },
              "sidebar": "stack",
              "previous": {
                "title": "时间同步服务",
                "permalink": "/docs/时间同步服务"
              },
              "next": {
                "title": "free命令",
                "permalink": "/docs/free"
              }
            },
            {
              "unversionedId": "stack/time-sync",
              "id": "stack/time-sync",
              "title": "时间同步服务",
              "description": "前言",
              "source": "@site/docs/stack/02-时间同步服务.md",
              "sourceDirName": "stack",
              "slug": "/时间同步服务",
              "permalink": "/docs/时间同步服务",
              "draft": false,
              "tags": [],
              "version": "current",
              "sidebarPosition": 2,
              "frontMatter": {
                "id": "time-sync",
                "slug": "/时间同步服务",
                "title": "时间同步服务",
                "last_update": {
                  "date": "2024/03/01"
                }
              },
              "sidebar": "stack",
              "previous": {
                "title": "Centos7系统---内核升级",
                "permalink": "/docs/Centos7系统---内核升级"
              },
              "next": {
                "title": "lscpu命令",
                "permalink": "/docs/lscpu"
              }
            },
            {
              "unversionedId": "stack/top",
              "id": "stack/top",
              "title": "top命令",
              "description": "简介",
              "source": "@site/docs/stack/05-top命令.md",
              "sourceDirName": "stack",
              "slug": "/top",
              "permalink": "/docs/top",
              "draft": false,
              "tags": [],
              "version": "current",
              "sidebarPosition": 5,
              "frontMatter": {
                "id": "top",
                "slug": "/top",
                "title": "top命令",
                "last_update": {
                  "date": "2024/03/13"
                }
              },
              "sidebar": "stack",
              "previous": {
                "title": "free命令",
                "permalink": "/docs/free"
              }
            }
          ],
          "drafts": [],
          "sidebars": {
            "stack": [
              {
                "type": "doc",
                "id": "stack/introduction"
              },
              {
                "type": "doc",
                "id": "stack/kernel-update"
              },
              {
                "type": "doc",
                "id": "stack/time-sync"
              },
              {
                "type": "doc",
                "id": "stack/lscpu"
              },
              {
                "type": "doc",
                "id": "stack/free"
              },
              {
                "type": "doc",
                "id": "stack/top"
              }
            ]
          }
        }
      ]
    }
  },
  "docusaurus-plugin-content-pages": {
    "default": [
      {
        "type": "mdx",
        "permalink": "/about",
        "source": "@site/src/pages/about.mdx",
        "title": "自我介绍",
        "description": "河山的自我介绍",
        "frontMatter": {
          "id": "about",
          "title": "自我介绍",
          "description": "河山的自我介绍",
          "hide_table_of_contents": true
        }
      },
      {
        "type": "mdx",
        "permalink": "/blog/first-blog",
        "source": "@site/src/pages/blog/first-blog.md",
        "title": "Docusaurus-Theme-Zen",
        "description": "logo",
        "frontMatter": {
          "slug": "first-blog",
          "title": "Docusaurus-Theme-Zen",
          "date": "2023-04-12T14:09:58.000Z",
          "tags": [
            "阅读更多"
          ],
          "authors": "RiverMountain"
        }
      },
      {
        "type": "jsx",
        "permalink": "/friends/",
        "source": "@site/src/pages/friends/index.tsx"
      },
      {
        "type": "jsx",
        "permalink": "/project/",
        "source": "@site/src/pages/project/index.tsx"
      },
      {
        "type": "jsx",
        "permalink": "/resource/",
        "source": "@site/src/pages/resource/index.tsx"
      }
    ]
  },
  "docusaurus-plugin-debug": {},
  "docusaurus-theme-classic": {},
  "docusaurus-plugin-image-zoom": {},
  "docusaurus-plugin-sass": {},
  "docusaurus-plugin-baidu-tongji": {},
  "docusaurus-plugin-baidu-push": {},
  "docusaurus-plugin-content-blog": {
    "default": {
      "blogSidebarTitle": "最近更新",
      "blogPosts": [
        {
          "id": "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
          "metadata": {
            "permalink": "/一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/05-Kubernetes1.29版本高可用安装.md",
            "source": "@site/blog/05-Kubernetes1.29版本高可用安装.md",
            "title": "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
            "description": "rocky",
            "date": "2024-06-24T19:40:00.000Z",
            "formattedDate": "2024年6月24日",
            "tags": [
              {
                "label": "kubernetes",
                "permalink": "/tags/kubernetes"
              },
              {
                "label": "centos",
                "permalink": "/tags/centos"
              },
              {
                "label": "rocky",
                "permalink": "/tags/rocky"
              }
            ],
            "readingTime": 28.673333333333332,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
              "title": "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
              "date": "2024-06-24 19:40",
              "tags": [
                "kubernetes",
                "centos",
                "rocky"
              ],
              "authors": "Week"
            },
            "nextItem": {
              "title": "Rocky Linux 9.3 系统安装",
              "permalink": "/Rocky Linux 9.3 系统安装"
            }
          },
          "content": "<!-- ![logo](/assets/images/docker.png) -->\r\n![rocky](/assets/images/docker.png)\r\n\r\n### 写在前面\r\n\r\n​\t一直想沉淀一篇Kubernetes高可用安装的文章，之前都是参考网上的博客，虽然安装的大致逻辑是有，可具体的细节没有梳理，还是模糊。\r\n\r\n​\t正好前段时间 Kubernetes 10周年际，借此时间，钻研一番。\r\n\r\n​\t本文使用的安装方式是: Kubeadm\r\n\r\n<!-- truncate -->\r\n\r\n## 预备知识\r\n\r\n​\tKubernetes是容器编排工具，即统一管理，控制，调度容器。\r\n\r\n​\tKubernetes整体，由控制面（Control Plan）和数据面（Data Plan）组成。\r\n\r\n​\tKubernetes控制面，是整个集群的核心，大脑，控制面出现问题，瘫痪，Kubernetes会无法正常使用。\r\n\r\n​\tKubernetes控制面，是由多个组件共同协作，完成相对应的工作。组件包括：\r\n\r\n  * Kube-Apiserver: 整个集群的通信入口, 大脑。\r\n\r\n  * Kube-Scheduler：将资源调度分配到数据面。\r\n  * Kube-Controller-Manager：集群资源的控制器。\r\n  * Etcd：整个集群的数据库。\r\n\r\nKubernetes数据面，即每一个node, 是工作节点。每个工作节点上需要部署多个组件, 完成与控制面通信等工作。组件包括：\r\n\r\n  * Kubelet: 用于与控制面通信, 可以理解为是agent代理程序。 \r\n  * Kubelet-Proxy: 用于代理和负载均衡每一个节点上的Pod（应用程序）。\r\n  * Container_Runtime: 容器运行时, 即能够提供Container（容器）运行环境的程序, 或者服务，称之为Container_Runtime, 例如Docker, Containerd。\r\n\r\nCgroup Driver: Ggroup 驱动：Linux内核提供的一种功能，用于限制，控制，隔离进程的资源（内存，cpu等）使用。容器领域的概念中, 需要对每一个容器进行资源的限制, 就需要借助这个驱动实现。而在Kubernetes集群中由Kubelet进行调用Container_Runtime, 再由Container_Runtime去创建容器, 因此：\r\n\r\n  ​\t需要在创建容器运行时服务时, 指定使用的Cgroup Driver类型。（需要和kubelet一致）\r\n\r\n  ​\t需要在创建Kubelet的时候, 指定使用的Cgroup Driver类型。（需要和容器运行时一致）\r\n\r\n  ​\t默认Cgroup Driver的类型有: cgroupfs , systemd。\r\n\r\n！注意：如果Linux系统的init进程是systemd, 那么不推荐使用cgroupfs作为kubelet或者容器运行时的Cgroup Driver, 原因是systemd进程会创建一个cgroupfs drivers, 即systemd, 如果使用cgroupfs, 则系统当中会存在两个cgroup driver, 进而导致systemd cgroup driver的不稳定。\r\n\r\n  ​\t\t在Kubernetes1.22版本, 使用kubeadm安装方式, kubelet 默认使用 systemd 。\r\n\r\n  ​\t\t在Kubernetes1.28版本, kubelet会自动检测匹配容器运行时的cgroup驱动程序。\r\n\r\n\r\n\r\n​那么, 实现Kubernetes的高可用, 本质上是控制面组件的高可用。\r\n\r\n\r\n\r\n## Kubernetes高可用\r\n\r\n​\t高可用, 其实就是当控制面故障时, 整个集群依然能够正常使用的效果。\r\n\r\n​\t因此, 可以对控制面进行多副本创建, 之后再结合VIP, 反向代理, 负载均衡服务, 提供统一的访问入口, 将请求代理到后端的每一个控制面上。实现高可用。\r\n\r\n​\t在这其中, 有一个特殊的组件: Etcd。\r\n\r\n​\tEtcd作为整个Kubernetes集群的数据库, 它的重要性是更不可说的, 但其本身并不是Kubernetes的原生组件, 而是一个单独的开源项目。既然是独立的, 那么就可以进行分布式。\r\n\r\n​\t目前对Etcd的高可用实现方案, 官方提供了两种:\r\n\r\n 1. 叠加式(不推荐): 与Kubernetes控制面叠加部署到一起, 官方架构图如下: \r\n\r\n    ![Stacked etcd topology](https://kubernetes.io/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg)\r\n\r\n    可以看到, 每一个控制面节点上都部署了一个Etcd的实例, 它们之间由共同构建成Etcd的集群。\r\n\r\n    但是这种方式也有一个弊端, 若一个控制面挂掉之后, Etcd也随之不可用。进而提高了故障的代价。因此也是不推荐的。（适用于资源有限的情况）\r\n\r\n 2. 外部式: 外部搭建独立的Etcd集群, Kubernetes直接远程连接使用。官方架构图如下: \r\n\r\n    ![External etcd topology](https://kubernetes.io/images/kubeadm/kubeadm-ha-topology-external-etcd.svg)\r\n\r\n​\t这种情况下, 若控制面挂掉, 也不会影响其访问的Etcd, 提高了容错, 降低了故障成本。维护起来也方便。\r\n\r\n\r\n\r\n## 反向代理, 负载均衡\r\n\r\n​\t反向代理, 即将客户端发送过来的请求转发给后端的服务器, 起到保护后端服务器的目的。\r\n\r\n​\t负载均衡, 即将客户端发送过来的请求根据相应的规则, 算法, 应该怎样给到后端的服务器, 起到减轻单台服务器压力的目的。\r\n\r\n​\t目前市面上实现这两种功能的服务有很多, 比如nginx, keepalived, haproxy, lvs，load balancer等等。\r\n\r\n​\t本文使用keepalived + haproxy的方式进行实现。\r\n\r\n​\t碍于资源, 本文的架构使用叠加式的方式部署。但安装方法, 步骤都是通用的。\r\n\r\n\r\n\r\n## 安装准备\r\n\r\n架构设计: \r\n\r\n| 节点名   | IP地址         | 描述   |\r\n| -------- | -------------- | ------ |\r\n| k8s-mn01 | 192.168.10.11  | 控制面 |\r\n| k8s-mn02 | 192.168.10.22  | 控制面 |\r\n| k8s-mn03 | 192.168.10.33  | 控制面 |\r\n| k8s-wn01 | 192.168.10.100 | 数据面 |\r\n| k8s-wn02 | 192.168.10.200 | 数据面 |\r\n\r\n为了方便Kubernetes集群的后续使用, 减少问题出现的频率, 需要在安装之前做准备。\r\n\r\n需要考虑的因素 (每台节点都需要操作)：\r\n\r\n 1. 操作系统的选用: **本文安装选用Rocky9.3**。当然也可以选择开源明星: Centos系列。\r\n\r\n    **不管是Rocky, 还是Centos, 本文的安装方法都是通用的。**\r\n\r\n 2. 每台机器的资源分配：内存，CPU，磁盘\r\n\r\n    * 内存: 根据业务需求，分配每个节点的内存。64G，128G，512G等。**测试学习不低于2G**。\r\n\r\n    * CPU：根据业务需求，分配每个节点的CPU核数。8核，16核，32核，64核，128核等。\r\n\r\n      ​\t **测试学习不低于2核**。\r\n\r\n    * 磁盘：根据业务需求，分配每个节点的磁盘容量。1T，2T，nT等, **测试学习不低于50G**。\r\n\r\n      在磁盘划分时, 需要注意以下路径, 最好能给一块单独的空间, 且是LVM卷(方便扩容)。\r\n\r\n      | 路径                               | 容量       | 描述                                                         | 备注        |\r\n      | ---------------------------------- | ---------- | ------------------------------------------------------------ | ----------- |\r\n      | /var/lib/docker 或 /var/lib/docker | 200G--不限 | 容器存储路径, 根据需求自定义。                               | 建议LVM卷   |\r\n      | /var/lib/etcd                      | 50G--不限  | Etcd存储路径，根据需求自定义, 使用固态磁盘，受限于磁盘速度, 故速度越快越好。 | 必须SSD磁盘 |\r\n\r\n​\t\t当然, 你也完全可以给/分配足够大的磁盘空间。\r\n\r\n2. 内核版本: 5.*, 需要将操作系统内核更新到最新的稳定版。(如果内核版本过低, 则Kubernetes需要调用, 使用内核提供的模块, 参数没有, 出现问题)\r\n\r\n   ```shell\r\n   # 查看内核版本\r\n   uname -r\r\n   5.14.0-362.8.1.el9_3.x86_64\r\n   ```\r\n\r\n3. 节点之间的网络是需要流畅的, 统一的。\r\n\r\n4. 节点之间的主机名是唯一的。\r\n\r\n   ```shell\r\n   # 配置节点的主机名 (自定义)\r\n   # 节点1执行: \r\n   hostnamectl set-hostname k8s-mn01\r\n   # 节点2执行: \r\n   hostnamectl set-hostname k8s-mn02\r\n   # 节点3执行\r\n   hostnamectl set-hostname k8s-mn03\r\n   # 节点4执行\r\n   hostnamectl set-hostname k8s-wn01\r\n   # 节点5执行\r\n   hostnamectl set-hostname k8s-wn02\r\n   \r\n   # 添加解析记录, 使节点直接也可以使用主机名进行访问通信\r\n   # 每台节点执行\r\n   cat << EOF >> /etc/hosts\r\n   192.168.10.11 k8s-mn01\r\n   192.168.10.22 k8s-mn02\r\n   192.168.10.33 k8s-mn03\r\n   192.168.10.100 k8s-wn01\r\n   192.168.10.200 k8s-wn02\r\n   EOF\r\n   ```\r\n\r\n5. 节点之间能够互相免密登录\r\n\r\n   ```shell\r\n   # 每个节点执行\r\n   # 生成密钥对文件, 传输公钥到目标节点\r\n   ssh-keygen -t rsa -b 2048 \r\n   一路回车\r\n   \r\n   ssh-copy-id root@目标节点IP\r\n   ```\r\n\r\n6. 节点的软件仓库源.repo, 是可用的。\r\n\r\n   ```shell\r\n   # 每个节点执行\r\n   # 替换成为阿里源\r\n   sed -e 's|^mirrorlist=|#mirrorlist=|g' \\\r\n       -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\r\n       -i.bak \\\r\n       /etc/yum.repos.d/[Rr]ocky*.repo\r\n       \r\n   # 安装所需软件包\r\n   dnf -y install ipvsadm\r\n   ```\r\n\r\n7. Firewalld, Selinux等安全机制关闭\r\n\r\n   ```shell\r\n   # 每台节点执行\r\n   systemctl stop firewalld\r\n   systemctl disable firewalld\r\n   \r\n   setenforce 0\r\n   sed -i \"/SELINUX/ s/enforcing/disabled/g\" /etc/selinux/config\r\n   ```\r\n\r\n8. 保证端口: 6443；2379-2380；10250；10259；10257；30000-32767  未被占用, 可被监听。\r\n\r\n9. 禁用Swap分区\r\n\r\n   ```shell\r\n   # 每台节点执行\r\n   vim /etc/fstab\r\n   #/dev/mapper/rl-swap     none                    swap    defaults        0 0\r\n   \r\n   swapoff -a\r\n   ```\r\n\r\n10. 节点之间的时间是统一的\r\n\r\n    ```shell\r\n    # 批注：由于RHEL8之后，官方就弃用了ntpdate软件包命令，而是建议使用chrony服务，而Rocky对标的是RHEL9，也是同样。(Centos系列完全可以使用该配置, 以下的dnf等同于yum, 所以以下的dnf命令均可以替换为yum)\r\n    \r\n    # 安装，配置chrony服务\r\n    $ dnf -y install chrony\r\n    $ vim /etc/chrony.conf\r\n      1 # Use public servers from the pool.ntp.org project.\r\n      2 # Please consider joining the pool (https://www.pool.ntp.org/join.html).\r\n      3 #pool 2.rocky.pool.ntp.org iburst\r\n      4 pool ntp.aliyun.com iburst\r\n    \r\n     26 # Allow NTP client access from local network.\r\n     27 allow 192.168.10.0/24\r\n     \r\n    $ systemctl start chronyd\r\n    $ systemctl enable chronyd\r\n    $ chronyc sources\r\n    MS Name/IP address         Stratum Poll Reach LastRx Last sample               \r\n    ===============================================================================\r\n    ^? 203.107.6.88                  2   6     3     2    -65ms[  -65ms] +/-   38ms\r\n    $ date\r\n    Mon Jun 10 21:21:37 CST 2024\r\n    \r\n    # 批注：上方修改chrony.conf中两处的含义是；\r\n    # pool ntp.aliyun.com iburst: 指定使用的上游时间服务器地址\r\n    # allow 192.168.10.0/24: 允许192.168.10.0/24网段的所有机器连接自己来同步时间\r\n    \r\n    # timedatectl查看时区是否为Asia/Shanghai\r\n    $ timedatectl\r\n                   Local time: Mon 2024-06-10 21:35:43 CST\r\n               Universal time: Mon 2024-06-10 13:35:43 UTC\r\n                     RTC time: Mon 2024-06-10 13:35:43\r\n                    Time zone: Asia/Shanghai (CST, +0800)\r\n    System clock synchronized: yes\r\n                  NTP service: active\r\n              RTC in local TZ: no\r\n    # 若时区不是Asia/Shanghai，则使用该命令修改\r\n    $ timedatectl set-timezone Asia/Shanghai\r\n    \r\n    # 其它节点只需要与该时间服务器同步即可, 即修改/etc/chrony.conf文件, 指定时间服务器地址: pool 时间服务器地址 iburst\r\n    ```\r\n\r\n11. 内核进行优化\r\n\r\n​\t注:通过配置sysctl.conf文件，对内核优化，优化方面有：系统方面，用户方面，容器方面。\r\n\r\n​\t优化的目的：提高系统的性能，服务运行需要。若不对其进行优化，在前期运行可能没有问    \t\t\t\t题，但在后期会因为一些内核参数的值限制，导致系统，服务的运行不稳定。\r\n\r\n​\t若不优化, 你在后面维护Kubernetes过程中, 大概率会遇到 Too many open files 等报错。\r\n\r\n​\t以下参数中: net.bridge.bridge-nf-call-iptables  = 1\r\n​                net.bridge.bridge-nf-call-ip6tables = 1\r\n​                net.ipv4.ip_forward                 = 1\r\n\r\n​\t是Kubernetes集群安装, 运行所需要的。\r\n\r\n```shell\r\n$ vi /etc/sysctl.conf \r\nfs.file-max = 202808\r\nnet.core.netdev_max_backlog = 262144\r\nnet.core.somaxconn = 262144\r\nnet.ipv4.tcp_max_orphans = 262144\r\nnet.ipv4.tcp_max_syn_backlog = 262144\r\nnet.ipv4.tcp_synack_retries = 1\r\nnet.ipv4.tcp_syn_retries = 1\r\nnet.ipv4.ip_local_port_range = 15000 65000\r\nnet.ipv4.tcp_keepalive_intvl = 60\r\nnet.ipv4.tcp_keepalive_probes = 3\r\nnet.ipv4.tcp_keepalive_time = 1500\r\nnet.ipv4.tcp_syncookies = 1\r\nnet.ipv4.tcp_fin_timeout = 30\r\nnet.ipv4.tcp_max_tw_buckets = 6000\r\nnet.ipv4.tcp_timestamps = 0\r\nnet.ipv4.tcp_timestamps = 0\r\nnet.ipv4.tcp_tw_reuse = 1\r\nnet.ipv4.tcp_timestamps = 1\r\nnet.core.rmem_default = 6291456\r\nnet.core.wmem_default = 6291456\r\nnet.core.rmem_max = 12582912\r\nnet.core.wmem_max = 12582912\r\nnet.ipv4.tcp_rmem = 10240 87380 12582912\r\nnet.ipv4.tcp_wmem = 10240 87380 12582912\r\nnet.ipv4.tcp_keepalive_time=600\r\nnet.ipv4.tcp_keepalive_intvl=30 \r\nnet.ipv4.tcp_keepalive_probes=10  \r\nnet.ipv6.conf.all.disable_ipv6=1 \r\nnet.ipv6.conf.default.disable_ipv6=1 \r\nnet.ipv6.conf.lo.disable_ipv6=1 \r\nnet.ipv4.neigh.default.gc_stale_time=120 \r\nnet.ipv4.conf.all.rp_filter=0  \r\nnet.ipv4.conf.default.rp_filter=0\r\nnet.ipv4.conf.default.arp_announce=2\r\nnet.ipv4.conf.lo.arp_announce=2\r\nnet.ipv4.conf.all.arp_announce=2\r\nnet.ipv4.ip_local_port_range= 45001 65000\r\nnet.ipv4.ip_forward=1\r\nnet.ipv4.tcp_max_tw_buckets=6000\r\nnet.ipv4.tcp_syncookies=1\r\nnet.ipv4.tcp_synack_retries=2\r\nnet.bridge.bridge-nf-call-ip6tables=1\r\nnet.bridge.bridge-nf-call-iptables=1 \r\nnet.netfilter.nf_conntrack_max=2310720 \r\nnet.ipv6.neigh.default.gc_thresh1=8192\r\nnet.ipv6.neigh.default.gc_thresh2=32768\r\nnet.ipv6.neigh.default.gc_thresh3=65536\r\nnet.core.netdev_max_backlog=16384    \r\nnet.core.rmem_max=16777216         \r\nnet.core.wmem_max=16777216         \r\nnet.ipv4.tcp_max_syn_backlog=8096  \r\nnet.core.somaxconn = 32768           \r\nfs.inotify.max_user_instances=8192   \r\nfs.inotify.max_user_watches=524288   。\r\nfs.file-max=52706963                \r\nfs.nr_open=52706963                  \r\nkernel.pid_max=4194303             \r\nnet.bridge.bridge-nf-call-arptables=1 \r\nvm.swappiness=0                       \r\nvm.overcommit_memory=1                \r\nvm.panic_on_oom=0                     \r\nvm.max_map_count=262144\r\n\r\n# 加载上述内核参数生效所需要的模块，并加载生效\r\nsudo modprobe overlay\r\nsudo modprobe br_netfilter\r\nsysctl -p\r\n```\r\n\r\n\r\n\r\n内核参数说明 (以下顺序不分先后): \r\n\r\n| 内核参数                             | 含义                                                 |\r\n| ------------------------------------ | ---------------------------------------------------- |\r\n| fs.file-max                          | 系统可以分配的最大文件句柄（或打开文件）数量。       |\r\n| net.core.netdev_max_backlog          | 内核可以为每个网络设备内部排队的最大数据包数量。     |\r\n| net.core.somaxconn                   | 可以在监听套接字排队中排队的最大连接数。             |\r\n| net.ipv4.tcp_max_orphans             | 内核开始丢弃连接之前允许的最大孤立套接字数量。       |\r\n| net.ipv4.tcp_max_syn_backlog         | 等待被接受的不完整连接的最大数量。                   |\r\n| net.ipv4.tcp_synack_retries          | 在放弃由远程端点发起的TCP连接尝试之前的重试次数。    |\r\n| net.ipv4.tcp_syn_retries             | 在放弃本地发起的TCP连接尝试之前的重试次数。          |\r\n| net.ipv4.ip_local_port_range         | 可用于传出连接的本地端口范围。                       |\r\n| net.ipv4.tcp_keepalive_intvl         | 连续TCP保活探测之间的时间间隔。                      |\r\n| net.ipv4.tcp_keepalive_probes        | 在考虑连接已死亡之前发送的TCP保活探测数量。          |\r\n| net.ipv4.tcp_keepalive_time          | 最后发送的数据包与第一个TCP保活探测之间的时间间隔。  |\r\n| net.ipv4.tcp_syncookies              | 启用TCP SYN cookies以防止SYN洪水攻击。               |\r\n| net.ipv4.tcp_fin_timeout             | 在FIN-WAIT-2状态下强制关闭TCP连接之前等待的时间。    |\r\n| net.ipv4.tcp_max_tw_buckets          | 内核可以维护的TIME_WAIT套接字的最大数量。            |\r\n| net.ipv4.tcp_timestamps              | 启用或禁用TCP时间戳以防止某些攻击。                  |\r\n| net.core.rmem_default                | 所有网络连接的接收缓冲区的默认大小。                 |\r\n| net.core.wmem_default                | 所有网络连接的发送缓冲区的默认大小。                 |\r\n| net.core.rmem_max                    | 所有网络连接的接收缓冲区的最大大小。                 |\r\n| net.core.wmem_max                    | 所有网络连接的发送缓冲区的最大大小。                 |\r\n| net.ipv4.tcp_rmem                    | TCP连接的接收缓冲区的最小、默认和最大大小。          |\r\n| net.ipv4.tcp_wmem                    | TCP连接的发送缓冲区的最小、默认和最大大小。          |\r\n| net.ipv4.tcp_tw_reuse                | 允许对新连接重用TIME_WAIT套接字。                    |\r\n| fs.inotify.max_user_instances        | 每个用户的最大inotify实例数。                        |\r\n| fs.inotify.max_user_watches          | 每个用户允许的最大监视数。                           |\r\n| fs.nr_open                           | 进程可以分配的最大文件描述符数量。                   |\r\n| kernel.pid_max                       | 进程ID号可以设置的最大值。                           |\r\n| net.bridge.bridge-nf-call-arptables  | 启用或禁用桥接流量的ARP表过滤。                      |\r\n| vm.swappiness                        | 控制在运行时内存和将应用程序数据缓存到内存中的平衡。 |\r\n| vm.overcommit_memory                 | 控制系统内存的过度承诺。                             |\r\n| vm.panic_on_oom                      | 内核在发生内存不足错误时的行为。                     |\r\n| vm.max_map_count                     | 进程可能具有的内存映射区域的最大数量。               |\r\n| net.ipv6.conf.all.disable_ipv6       | 禁用IPv6协议的配置参数。                             |\r\n| net.ipv6.conf.default.disable_ipv6   | 禁用默认的IPv6协议配置参数。                         |\r\n| net.ipv6.conf.lo.disable_ipv6        | 禁用本地回环接口的IPv6协议配置参数。                 |\r\n| net.ipv4.neigh.default.gc_stale_time | 邻居条目被认为过时的时间。                           |\r\n| net.ipv4.conf.all.rp_filter          | 启用或禁用反向路径过滤。                             |\r\n| net.ipv4.conf.default.rp_filter      | 启用或禁用默认反向路径过滤。                         |\r\n| net.ipv4.conf.default.arp_announce   | 发送ARP请求时使用的源地址类型。                      |\r\n| net.ipv4.conf.lo.arp_announce        | 发送ARP请求时使用的源地址类型。                      |\r\n| net.ipv4.conf.all.arp_announce       | 发送ARP请求时使用的源地址类型。                      |\r\n| net.ipv4.ip_forward                  | 启用或禁用IP转发。                                   |\r\n| net.ipv4.ip_local_port_range         | 可用于传出连接的本地端口范围。                       |\r\n| net.bridge.bridge-nf-call-iptables   | 启用或禁用桥接流量的iptables过滤。                   |\r\n| net.netfilter.nf_conntrack_max       | 连接跟踪表的最大条目数。                             |\r\n| net.ipv6.neigh.default.gc_thresh1    | 邻居缓存的最小条目数量。                             |\r\n| net.ipv6.neigh.default.gc_thresh2    | 邻居缓存的良好条目数量。                             |\r\n| net.ipv6.neigh.default.gc_thresh3    | 邻居缓存的最大条目数量。                             |\r\n| net.core.netdev_max_backlog          | 内核可以为每个网络设备内部排队的最大数据包数量。     |\r\n| net.core.rmem_max                    | 所有网络连接的接收缓冲区的最大大小。                 |\r\n| net.core.wmem_max                    | 所有网络连接的发送缓冲区的最大大小。                 |\r\n| net.ipv4.tcp_max_syn_backlog         | 等待被接受的不完整连接的最大数量。                   |\r\n| net.core.somaxconn                   | 可以在监听套接字排队中排队的最大连接数。             |\r\n| fs.inotify.max_user_instances        | 每个用户的最大inotify实例数。                        |\r\n| fs.inotify.max_user_watches          | 每个用户允许的最大监视数。                           |\r\n| fs.file-max                          | 系统可以分配的最大文件句柄（或打开文件）数量。       |\r\n| fs.nr_open                           | 进程可以分配的最大文件描述符数量。                   |\r\n| kernel.pid_max                       | 进程ID号可以设置的最大值。                           |\r\n| net.bridge.bridge-nf-call-arptables  | 启用或禁用桥接流量的ARP表过滤。                      |\r\n| vm.swappiness                        | 控制在运行时内存和将应用程序数据缓存到内存中的平衡。 |\r\n| vm.overcommit_memory                 | 控制系统内存的过度承诺。                             |\r\n| vm.panic_on_oom                      | 内核在发生内存不足错误时的行为。                     |\r\n| vm.max_map_count                     | 进程可能具有的内存映射区域的最大数量。               |\r\n\r\n​\t\r\n\r\n## 运行时安装\r\n\r\n目前Kubernetes官方推荐使用的是Containerd, 当然Docker也可以使用, 只需要添加一个shim垫片(与Kubernetes集群连接使用)。之间的原因: 懂得都懂。\r\n\r\n本文选择的是Containerd (每台节点都需要操作)\r\n\r\n1. 安装指定版本的Containerd.tar.gz\r\n\r\n   ```shell\r\n   # 因版本过多, 得到一个稳定, 推崇的, 并不太容易。\r\n   # 借鉴于AliYun, 因其云计算的场景, 方案也成熟。\r\n   # 本次使用的是1.6.33\r\n   # 下载地址: \r\n   https://github.com/containerd/containerd/releases/download/v1.6.33/containerd-1.6.33-linux-amd64.tar.gz\r\n   \r\n   # 每台节点执行\r\n   $ tar Cxzvf /usr/local containerd-1.6.33-linux-amd64.tar.gz \r\n   bin/\r\n   bin/containerd-shim\r\n   bin/containerd-stress\r\n   bin/ctr\r\n   bin/containerd-shim-runc-v2\r\n   bin/containerd-shim-runc-v1\r\n   bin/containerd\r\n   \r\n   # 使用官方提供的service文件：https://raw.githubusercontent.com/containerd/containerd/main/containerd.service\r\n   \r\n   # 必须是/usr/local/lib/systemd/system路径, 否则会找不到\r\n   mkdir -p /usr/local/lib/systemd/system\r\n   vim /usr/local/lib/systemd/system/containerd.service\r\n   # Copyright The containerd Authors.\r\n   #\r\n   # Licensed under the Apache License, Version 2.0 (the \"License\");\r\n   # you may not use this file except in compliance with the License.\r\n   # You may obtain a copy of the License at\r\n   #\r\n   #     http://www.apache.org/licenses/LICENSE-2.0\r\n   #\r\n   # Unless required by applicable law or agreed to in writing, software\r\n   # distributed under the License is distributed on an \"AS IS\" BASIS,\r\n   # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n   # See the License for the specific language governing permissions and\r\n   # limitations under the License.\r\n   \r\n   [Unit]\r\n   Description=containerd container runtime\r\n   Documentation=https://containerd.io\r\n   After=network.target local-fs.target\r\n   \r\n   [Service]\r\n   ExecStartPre=-/sbin/modprobe overlay\r\n   ExecStart=/usr/local/bin/containerd\r\n   \r\n   Type=notify\r\n   Delegate=yes\r\n   KillMode=process\r\n   Restart=always\r\n   RestartSec=5\r\n   \r\n   # Having non-zero Limit*s causes performance problems due to accounting overhead\r\n   # in the kernel. We recommend using cgroups to do container-local accounting.\r\n   LimitNPROC=infinity\r\n   LimitCORE=infinity\r\n   \r\n   # Comment TasksMax if your systemd version does not supports it.\r\n   # Only systemd 226 and above support this version.\r\n   TasksMax=infinity\r\n   OOMScoreAdjust=-999\r\n   \r\n   [Install]\r\n   WantedBy=multi-user.target\r\n   \r\n   systemctl daemon-reload \r\n   systemctl start containerd\r\n   systemctl enable containerd\r\n   ```\r\n\r\n2. 安装指定版本的Runc\r\n\r\n   Runc: 是真正创建, 运行容器的程序, Containerd服务去创建容器时, 本身是去调用Runc程序来进行容器的创建。\r\n\r\n   ```shell\r\n   # 本次下载的版本是: Runc 1.1.12\r\n   # 下载地址\r\n   https://github.com/opencontainers/runc/releases/download/v1.1.12/runc.amd64\r\n   \r\n   # 安装即可\r\n   install -m 755 runc.amd64 /usr/local/sbin/runc\r\n   ```\r\n\r\n3. 安装指定版本的网络插件CNI (这一步可以不做)\r\n\r\n   CNI: Container Network Interface: 容器网络接口, 实现容器间的访问通信的, 比如Ping。\r\n\r\n   这一步骤可以不做，因为containerd的cni插件解决的是容器间的访问通信，但是在安装kubernetes的同时，也会安装kubernetes所需要的cni插件：flannel 或者 calico。作用都是一致。\r\n\r\n   ```shell\r\n   # 下载地址\r\n   https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz\r\n   \r\n   # 安装\r\n   $ mkdir -p /opt/cni/bin\r\n   $ tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.1.1.tgz\r\n   ```\r\n\r\n4. Containerd配置文件的生成与修改\r\n\r\n   默认的Containerd配置文件中定义了使用了Cgroup Driver的类型, 镜像拉取的地址等, 需要进行修改成为正确的, 适合的。\r\n\r\n   ```shell\r\n   # 生成Containerd配置文件\r\n   mkdir /etc/containerd\r\n   containerd config default > /etc/containerd/config.toml\r\n   \r\n   # 修改Containerd使用的cgroup为systemd cgroup driver\r\n   $ vim /etc/containerd/config.toml\r\n   [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\r\n     ...\r\n     [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\r\n       SystemdCgroup = true\r\n   \r\n   # 修改Containerd使用的sandbox_image\r\n   [root@localhost ~]# vim /etc/containerd/config.toml\r\n   sandbox_image = \"registry.aliyuncs.com/google_containers/pause:3.9\"\r\n   \r\n   # 配置Containerd镜像加速【默认文件定义的镜像下载地址都是国外，访问不了，需要修改成国内代理】\r\n   # containerd官方推荐的方式如下：\r\n   $ vim /etc/containerd/config.toml\r\n   146     [plugins.\"io.containerd.grpc.v1.cri\".registry]\r\n   147       config_path = \"/etc/containerd/certs.d\"\r\n   148 \r\n   149       [plugins.\"io.containerd.grpc.v1.cri\".registry.auths]\r\n   150 \r\n   151       [plugins.\"io.containerd.grpc.v1.cri\".registry.configs]\r\n   152 \r\n   153       [plugins.\"io.containerd.grpc.v1.cri\".registry.headers]\r\n   154 \r\n   155       [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors]\r\n   \r\n   mkdir /etc/containerd/certs.d/docker.io -p\r\n   cat > /etc/containerd/certs.d/docker.io/hosts.toml << EOF\r\n   server = \"https://docker.io\"\r\n   [host.\"https://i3h3dbjr.mirror.aliyuncs.com\"]\r\n     capabilities = [\"pull\", \"resolve\"]\r\n   EOF\r\n   \r\n   systemctl restart containerd\r\n   ```\r\n\r\n5. BuildKit安装\r\n\r\n   若想要进行镜像构建, 则需要借助BuildKit, 否则构建会报错\r\n\r\n   ```shell\r\n   # 下载安装包\r\n   wget https://github.com/moby/buildkit/releases/download/v0.11.6/buildkit-v0.11.6.linux-amd64.tar.gz\r\n   \r\n   # 解压\r\n   tar -zxvf buildkit-v0.11.6.linux-amd64.tar.gz\r\n   \r\n   # 安装\r\n   cp -a bin/* /usr/local/bin/\r\n   > buildctl -version\r\n   buildctl github.com/moby/buildkit v0.11.6 2951a28cd7085eb18979b1f710678623d94ed578\r\n   \r\n   # 配置systemd管理\r\n   vi /usr/lib/systemd/system/buildkitd.service\r\n   [Unit]\r\n   Description=/usr/local/bin/buildkitd\r\n   ConditionPathExists=/usr/local/bin/buildkitd\r\n   After=containerd.service\r\n   \r\n   [Service]\r\n   Type=simple\r\n   ExecStart=/usr/local/bin/buildkitd\r\n   User=root\r\n   Restart=on-failure\r\n   RestartSec=1500ms\r\n   \r\n   [Install]\r\n   WantedBy=multi-user.target\r\n   \r\n   # 启动\r\n   systemctl daemon-reload && systemctl start buildkitd && systemctl enable buildkitd\r\n   ```\r\n\r\n6. 命令行工具安装\r\n\r\n   Containerd安装部署起来之后，可以进行pull，push镜像，start，stop容器等相关操作。\r\n\r\n   ​\tContainerd默认提供的命令行工具是ctr，但是这个命令使用起来确实不太方便，很多之前Docker的命令选项都没有。\r\n\r\n   ​\tnerdctl命令工具应运而生。是一个类似于Docker CLI的命令工具，用于管理和运行容器，它提供与Docker兼容的接口，并支持和Containerd集成。\r\n\r\n   ​\t也就是说, 之前使用Docker的操作, 例如docker run; docker build; docker load等, 只需将docker换成nerdctl即可。更多使用见: [1]\r\n\r\n   ```shell\r\n   # 下载安装包\r\n   wget https://github.com/containerd/nerdctl/releases/download/v1.4.0/nerdctl-1.4.0-linux-amd64.tar.gz\r\n   \r\n   # 解压\r\n   mkdir /root/nerdctl\r\n   tar -zxvf nerdctl-1.4.0-linux-amd64.tar.gz -C /root/nerdctl\r\n   cd /root/nerdctl && ls\r\n   containerd-rootless-setuptool.sh containerd-rootless.sh nerdctl\r\n   \r\n   # 安装\r\n   cp -a nerdctl /usr/bin/nerdctl\r\n   > nerdctl --version\r\n   nerdctl version 1.4.0\r\n   \r\n   # 创建配置文件\r\n   mkdir -p /etc/nerdctl\r\n   $ vi /etc/nerdctl/nerdctl.toml\r\n   namespace = \"k8s.io\"\r\n   debug = false\r\n   debug_full = false\r\n   insecure_registry = true\r\n   \r\n   # 上方配置的namespace是需要指定成为k8s.io, 默认为default\r\n   # 该namespace的作用是一种隔离机制，用于将系统资源（如进程、文件系统、网络）对不同实体进行隔离，使它们在各自的环境中运行，互不干扰。\r\n   # 若不指定为k8s.io, 则后续在安装kubernetes过程中, 都需要指定命名空间, 很不方便。\r\n   ```\r\n\r\n   \r\n\r\n## Kubeadm,Kubectl,Kubelet安装\r\n\r\n* kubeadm：kubernetes集群部署工具。\r\n* kubelet：运行在集群中的每一个节点上，用于启动 pod 和 容器。\r\n* kubectl：kubernetes命令行工具，用于与kubernetes集群进行交互，例如创建pod，查看集群状态等。\r\n\r\n所以需要在每一个控制面上安装kubelet, kubectl, 每一个数据面上安装kubelet。\r\n\r\nkubeadm只需要安装在一台控制面主机上即可。但是为了方便下载镜像, 本文在每个节点都安装。\r\n\r\n版本选择：\r\n\r\n​\tkubeadm的版本是1.29，则安装的的kubernetes集群版本（或者说kubernetes核心组件）肯定是1.29 或 1.28。\r\n\r\n​\tkubelet的版本必须小于kube-apiserver的版本，一般kubelet的版本选用小于等于3个kube-apiserver版本。（例如：kube-apiserver版本本次安装的是**1.29**，那么kubelet版本支持 **1.29**, **1.28**, **1.27**, 和 **1.26**。）\r\n\r\n​\tkubectl的版本只允许在1个小版本的偏差与kube-apiserver的版本。（例如：kube-apiserver版本本次安装的是1.29，那么kubectl版本支持 **1.30**, **1.29**, 和**1.28**。）\r\n\r\n更多的版本偏差，查阅官网：[2]\r\n\r\n安装步骤 ( 控制面安装Kubeadm, Kubectl, Kubelet; 数据面安装Kubelet ): \r\n\r\n```shell\r\n# Aliyun Kubernetes Repo源 配置. 因官网的仓库地址国内访问不到, 使用阿里云提供的仓库, 内容都是一样的. \r\n# 三台机器相同配置: \r\ncat <<EOF | tee /etc/yum.repos.d/kubernetes.repo\r\n[kubernetes]\r\nname=Kubernetes\r\nbaseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.29/rpm/\r\nenabled=1\r\ngpgcheck=1\r\ngpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.29/rpm/repodata/repomd.xml.key\r\nEOF\r\n\r\ndnf makecache\r\n\r\n# 列出kubeadm等版本信息\r\n[root@k8s-mn01 ~]# dnf --showduplicates list kubeadm | grep x86\r\nkubeadm.x86_64                    1.29.0-150500.1.1                   kubernetes\r\nkubeadm.x86_64                    1.29.1-150500.1.1                   kubernetes\r\nkubeadm.x86_64                    1.29.2-150500.1.1                   kubernetes\r\nkubeadm.x86_64                    1.29.3-150500.1.1                   kubernetes\r\nkubeadm.x86_64                    1.29.4-150500.2.1                   kubernetes\r\nkubeadm.x86_64                    1.29.5-150500.1.1                   kubernetes\r\nkubeadm.x86_64                    1.29.6-150500.1.1                   kubernetes\r\n\r\n#安装kubeadm, kubelet, kubectl 均为1.29.6-150500.1.1版本\r\ndnf -y install kubeadm-1.29.6 kubectl-1.29.6 kubelet-1.29.6\r\n\r\nsystemctl enable kubelet && systemctl start kubelet\r\n```\r\n\r\n\r\n\r\n## 高可用安装\r\n\r\nKeepalived + Haproxy是长久以来被人熟知, 经过验证的高可用方案。\r\n\r\n前提: 需要申请一个未被使用过的IP, 作为虚拟IP (VIP)。\r\n\r\n只需在控制面节点上操作: \r\n\r\n```shell\r\n# 每个节点安装keepalived, haproxy\r\ndnf -y install keepalived haproxy\r\n\r\n# 创建keepalived.conf\r\n$ /etc/keepalived/keepalived.conf\r\n! Configuration File for keepalived\r\nglobal_defs {\r\n    router_id LVS_DEVEL\r\n}\r\n\r\n# 指定检测脚本: \r\n# script: 脚本路径; interval: 脚本执行时间; weight: 权重; fall: 连续检测失败多少次之后认定节点不可用; rise: 连续检测成功多少次认为节点恢复正常。\r\nvrrp_script check_apiserver {\r\n  script \"/etc/keepalived/check_apiserver.sh\"\r\n  interval 3\r\n  weight -2\r\n  fall 10\r\n  rise 2\r\n}\r\n\r\n# state: 指定MASTER身份, 另外两台Keepalived设置成BACKUP\r\n# interface: 指定网卡; \r\n# virtual_router_id: VRRP虚拟路由id, 同一集群的Keepalived节点要相同, 用来识别彼此\r\n# priority: 优先级, 另外两台Keepalived分别设置成90 70\r\n# auth_type: VRRP组节点之间认证方式为PASS铭文\r\n# auth_pass: VRRP组节点之间用来认证通信的密码\r\n# virtual_ipaddress: VIP\r\n# track_script: 指定使用的检测脚本名称\r\n\r\nvrrp_instance VI_1 {\r\n    state MASTER\r\n    interface ens33\r\n    virtual_router_id 51\r\n    priority 100\r\n    authentication {\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n    virtual_ipaddress {\r\n        192.168.10.240\r\n    }\r\n    track_script {\r\n        check_apiserver\r\n    }\r\n}\r\n\r\n# 创建检测脚本\r\n# 该脚本的逻辑是: 检测本地的8443端口(haproxy服务)是否正常, 若不正常, 则停止本地的Keepalived服务, VIP飘逸到其它haproxy可用的节点, 继续提供服务。\r\n$ vi /etc/keepalived/check_apiserver.sh\r\n#!/bin/sh\r\n\r\ncurl -sfk --max-time 2 https://localhost:8443/healthz -o /dev/null \r\nif [ $? -nq 0]\r\nthen\r\n        echo \"*** Error GET https://localhost:8443/healthz\" 1>&2\r\n        systemctl stop keepalived\r\nfi\r\n# 给脚本文件执行权限\r\nchmod +x /etc/keepalived/check_apiserver.sh \r\n\r\n# 创建haproxy.cfg\r\n$ vi /etc/haproxy/haproxy.cfg\r\n#---------------------------------------------------------------------\r\n# Global settings\r\n#---------------------------------------------------------------------\r\nglobal\r\n    log stdout format raw local0\r\n    chroot      /var/lib/haproxy\r\n    pidfile     /var/run/haproxy.pid\r\n    maxconn     4000\r\n    user        haproxy\r\n    group       haproxy\r\n    daemon\r\n\r\n#---------------------------------------------------------------------\r\n# common defaults that all the 'listen' and 'backend' sections will\r\n# use if not designated in their block\r\n#---------------------------------------------------------------------\r\ndefaults\r\n    log                     global\r\n    option                  httplog\r\n    option                  dontlognull\r\n    option                  forwardfor    except 127.0.0.0/8\r\n    timeout connect         5s\r\n    timeout client          35s\r\n    timeout server          35s\r\n\r\n#---------------------------------------------------------------------\r\n# apiserver frontend which proxys to the control plane nodes\r\n#---------------------------------------------------------------------\r\n# 主要是这里的bind: 定义haproxy的代理端口为8443。也可以是其它。\r\nfrontend apiserver\r\n    bind *:8443\r\n    mode tcp\r\n    option tcplog\r\n    default_backend apiserverbackend\r\n\r\n#---------------------------------------------------------------------\r\n# round robin balancing for apiserver\r\n#---------------------------------------------------------------------\r\n# 以下是后端相关配置, 关键参数解释如下\r\n# mode tcp: 设置与后端服务通信的模式为TCP\r\n# balance roundrobin: 轮询方式\r\n# inter 10s: 检查间隔为10秒。\r\n# downinter 5s: 当服务被标记为不可用后，每5秒检查一次是否恢复。\r\n# rise 2: 在将服务器标记为上线之前，服务器必须连续2次成功响应检查。\r\n# fall 2: 在将服务器标记为下线之前，服务器必须连续2次失败响应检查。\r\n# slowstart 60s: 慢启动时间为60秒，用于控制新服务器上线后逐渐增加其权重。\r\n# maxconn 250: 每个服务器的最大并发连接数为250。\r\n# maxqueue 256: 后端队列的最大长度为256。\r\n# weight 100: 服务器的默认权重为100。\r\n# server 定义后端的服务器列表。\r\nbackend apiserverbackend\r\n    option tcplog\r\n    option tcp-check\r\n    mode tcp\r\n    balance roundrobin\r\n    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\r\n    server k8s-mn01 192.168.10.11:6443 check\r\n    server k8s-mn02 192.168.10.22:6443 check\r\n    server k8s-mn03 192.168.10.33:6443 check\r\n\r\n# 服务整体启动\r\nsystemctl enable --now keepalived\r\nsystemctl enable --now haproxy\r\n```\r\n\r\n\r\n\r\n## Etcd的高可用\r\n\r\n这里碍于资源, 并没有采用搭建外部单独的ETCD集群方案。\r\n\r\n当然还是很推荐使用外部的ETCD集群 (前提集群够大, 小集群使用默认提供的就好), 搭建方案目前也很成熟, 使用二进制搭建, 或者使用Docker容器的方式搭建, 再或者使用Kubernetes官方提供的方案: Kubeadm方式进行安装。也都是非常不错的选择。\r\n\r\n这里多说一句, 如果要搭建外部式ETCD, 则请不要把它同控制面安装部署在一起。我在调研Kubeadm安装ETCD高可用集群的时候, 心想尝试一下外部方式, 但是没有多余节点, 就部署到了控制面, 虽然安装上了, 但是kubeadm初始化失败, 报错为: k8s-mn01 not found。\r\n\r\n原因就是Kubernetes集群本身认为既然是外部的Etcd, 那么就不能与控制面安装在一起, 否则会报错。解决方法也挺奇葩, 有需要的可以参考: https://github.com/kubernetes/kubeadm/issues/1438#issuecomment-493004994\r\n\r\n\r\n\r\n## Kubeadm安装集群\r\n\r\n安装集群需要以下镜像: \r\n\r\n* kube-apiserver:v1.29.6\r\n* kube-controller-manager:v1.29.6\r\n* kube-scheduler:v1.29.6\r\n* kube-proxy:v1.29.6\r\n* etcd:3.5.12-0\r\n* pause:3.9\r\n* coredns:v1.11.1\r\n* calico-cni:\r\n\r\n以上的镜像可以自行下载, 也可以直接从Aliyun镜像仓库中下载: \r\n\r\n```shell\r\n# 所需镜像\r\nregistry.aliyuncs.com/google_containers/kube-apiserver:v1.29.6\r\nregistry.aliyuncs.com/google_containers/kube-controller-manager:v1.29.6\r\nregistry.aliyuncs.com/google_containers/kube-scheduler:v1.29.6\r\nregistry.aliyuncs.com/google_containers/kube-proxy:v1.29.6\r\nregistry.aliyuncs.com/google_containers/coredns:v1.11.1\r\nregistry.aliyuncs.com/google_containers/pause:3.9\r\nregistry.aliyuncs.com/google_containers/etcd:3.5.12-0\r\n\r\n# 或者直接一条命令, 依次全部下载\r\nkubeadm config images pull --image-repository registry.aliyuncs.com/google_containers\r\n```\r\n\r\n开始安装 (在第一台控制面节点操作):\r\n\r\n```shell\r\n# 生成默认的初始化配置文件\r\nkubeadm config print init-defaults >  kubeadm-config.yaml\r\n\r\n# 修改\r\n$ vi kubeadm-config.yaml\r\napiVersion: kubeadm.k8s.io/v1beta3\r\nbootstrapTokens:\r\n- groups:\r\n  - system:bootstrappers:kubeadm:default-node-token\r\n  token: abcdef.0123456789abcdef\r\n  ttl: 24h0m0s\r\n  usages:\r\n  - signing\r\n  - authentication\r\nkind: InitConfiguration\r\nlocalAPIEndpoint:\r\n  advertiseAddress: 192.168.10.11\r\n  bindPort: 6443\r\nnodeRegistration:\r\n  criSocket: unix:///var/run/containerd/containerd.sock\r\n  imagePullPolicy: IfNotPresent\r\n  name: k8s-mn01\r\n  taints: null\r\n---\r\napiServer:\r\n  timeoutForControlPlane: 4m0s\r\napiVersion: kubeadm.k8s.io/v1beta3\r\ncertificatesDir: /etc/kubernetes/pki\r\nclusterName: kubernetes\r\ncontrollerManager: {}\r\ndns: {}\r\ncontrolPlaneEndpoint: \"192.168.10.240:8443\"\r\netcd:\r\n  local:\r\n    dataDir: /var/lib/etcd\r\nimageRepository: registry.aliyuncs.com/google_containers\r\nkind: ClusterConfiguration\r\nkubernetesVersion: 1.29.6\r\nnetworking:\r\n  dnsDomain: cluster.local\r\n  serviceSubnet: 172.16.0.0/16\r\n  podSubnet: 10.96.0.0/8\r\nscheduler: {}\r\n```\r\n\r\n若是连接不是本地的, 而是外部的, 则需要将etcd那部分的配置, 修改为:\r\n\r\n```shell\r\nexternal:\r\n    endpoints:\r\n      - https://192.168.10.11:2379\r\n      - https://192.168.10.22:2379\r\n      - https://192.168.10.33:2379\r\n    caFile: /etc/kubernetes/pki/etcd/ca.crt\r\n    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt\r\n    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key\r\n```\r\n\r\n\r\n\r\n重要配置如下: \r\n\r\n| 配置项               | 描述                                                         |\r\n| -------------------- | ------------------------------------------------------------ |\r\n| advertiseAddress     | 指定本机地址                                                 |\r\n| bindPort             | 本机的Kube-Apiserver端口                                     |\r\n| criSocket            | 指定与容器运行时的通信文件                                   |\r\n| name                 | 指定本机的主机名                                             |\r\n| controlPlaneEndpoint | 指定控制面的通信地址, 这里写VIP地址                          |\r\n| imageRepository      | 指定下载Kubernetes组件的镜像仓库地址, 默认访问国外的仓库, 这里需要修改为国内的镜像仓库源 |\r\n| kubernetesVersion    | 指定安装的kubernetes版本                                     |\r\n| serviceSubnet        | 指定Kubernetes的Service资源分配的网段, 网段不能与真实机和Pod的网段冲突。 |\r\n| podSubnet            | 指定Kubernetes的Pod资源分配的网段, 网段不能与真实机和Service的网段冲突。 |\r\n\r\n\r\n\r\nKubeadm-config文件配置好之后, 执行下面的命令, 进行安装：\r\n\r\n```shell\r\nsudo kubeadm init --config kubeadm-config.yaml --upload-certs\r\n```\r\n\r\n安装结果如下:\r\n\r\n```shell\r\nYour Kubernetes control-plane has initialized successfully!\r\n\r\nTo start using your cluster, you need to run the following as a regular user:\r\n\r\n  mkdir -p $HOME/.kube\r\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\r\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\r\n\r\nAlternatively, if you are the root user, you can run:\r\n\r\n  export KUBECONFIG=/etc/kubernetes/admin.conf\r\n\r\nYou should now deploy a pod network to the cluster.\r\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\r\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\r\n\r\nYou can now join any number of the control-plane node running the following command on each as root:\r\n\r\n  kubeadm join 192.168.10.240:8443 --token abcdef.0123456789abcdef \\\r\n\t--discovery-token-ca-cert-hash sha256:ed4897f63cdf316b920d12913d0d45749788f8f85d34c3b81e9b4444dea9faab \\\r\n\t--control-plane --certificate-key ae20760aff597dc87e2ae67e2ab9d588f3eb9825b70ec1474855e44b849b78d3\r\n\r\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\r\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\r\n\"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward.\r\n\r\nThen you can join any number of worker nodes by running the following on each as root:\r\n\r\nkubeadm join 192.168.10.240:8443 --token abcdef.0123456789abcdef \\\r\n\t--discovery-token-ca-cert-hash sha256:ed4897f63cdf316b920d12913d0d45749788f8f85d34c3b81e9b4444dea9faab \r\n\r\n```\r\n\r\n根据上方提示:\r\n\r\n 1. 在执行kubeadm节点上执行:\r\n\r\n    ```shell\r\n      mkdir -p $HOME/.kube\r\n      sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\r\n      sudo chown $(id -u):$(id -g) $HOME/.kube/config\r\n    ```\r\n\r\n2. 在另外两台控制面节点上执行:\r\n\r\n   ```shell\r\n    kubeadm join 192.168.10.240:8443 --token abcdef.0123456789abcdef \\\r\n   \t--discovery-token-ca-cert-hash sha256:ed4897f63cdf316b920d12913d0d45749788f8f85d34c3b81e9b4444dea9faab \\\r\n   \t--control-plane --certificate-key ae20760aff597dc87e2ae67e2ab9d588f3eb9825b70ec1474855e44b849b78d3\r\n   ```\r\n\r\n3. 在数据面节点上执行加入集群的命令:\r\n\r\n   ```shell\r\n   kubeadm join 192.168.10.240:8443 --token abcdef.0123456789abcdef \\\r\n   \t--discovery-token-ca-cert-hash sha256:ed4897f63cdf316b920d12913d0d45749788f8f85d34c3b81e9b4444dea9faab \r\n   ```\r\n\r\n   若加入集群的令牌失效, 可以使用如下命令在控制面上重新生成: \r\n\r\n   ```shell\r\n   kubeadm token create --print-join-command\r\n   ```\r\n\r\n4. 检测安装是否正常: \r\n\r\n   ```shell\r\n   # 控制面节点\r\n   [root@k8s-mn01 ~]# kubectl get no\r\n   NAME       STATUS     ROLES           AGE   VERSION\r\n   k8s-mn01   NotReady   control-plane   24h   v1.29.6\r\n   k8s-mn02   NotReady   control-plane   24h   v1.29.6\r\n   k8s-mn03   NotReady   control-plane   24h   v1.29.6\r\n   k8s-wn01   NotReady   <none>          19m   v1.29.6\r\n   k8s-wn02   NotReady   <none>          9s    v1.29.6\r\n   ```\r\n\r\n\r\n\r\n安装网络插件Calico: \r\n\r\n​\tCalico截至目前为止, 最新的版本是3.28。该版本, 官方经过了与Kubernetes的充分测试, 支持: Kubernetesv1.27-1.30。\r\n\r\n​\tCalico3.27版本支持: Kubernetesv1.27-v1.29。\r\n\r\n​\t这里我们选用Calico:3.28。\r\n\r\n​\tCalico的安装方式目前有两种:\r\n\r\n​\t\t* 基于Operator方式安装, 能够管理Calico集群的安装, 升级, 生命周期管理等。\r\n\r\n​\t\t* 基于静态资源清单安装, 方便, 简单, 但无法像Opertaor一样能够自动管理Calico的生命周期。\r\n\r\n​\t\t  基于静态资源清单的部署常见的也分为两种: \r\n\r\n​\t\t\t\t*calico.yaml*: 当Calico使用Kubernetes API作为数据存储, 且集群节点少于50个。\r\n\r\n​\t\t\t\tcalico-typha.yaml: 当Calico使用Kubernetes API作为数据存储, 且集群节点大于50个。\r\n\r\n​\t这里我们选用基于清单的方式, 且使用calico-typha.yaml的方式部署, 对于一般的集群来说, 足够 (第一台控制面节点上操作)。\r\n\r\n```shell\r\nhttps://github.com/projectcalico/calico/blob/master/manifests/calico-typha.yaml\r\n\r\n# 重点配置:\r\nreplicas: 副本数 , 建议每200个节点1个副本, 生产的话建议3个副本。这里默认不变: 1个。\r\n# 以下配置: 用于给Pod分配IP的地址池范围, 因在kubeadm-config文件中定义过了Pod的IP地址池范围, 所以这里就不需要再配置了, Calico会根据kubeadm配置的来进行IP地址划分。\r\n- name: CALICO_IPV4POOL_CIDR  \r\n  value: \"192.168.0.0/16\"\r\n# Calico默认安装使用的IPIP模式。Always: 表示全网络覆盖; Cross-SubNet: 表示跨子网覆盖; Nerver: 表示不启用。\r\n- name: CALICO_IPV4POOL_IPIP\r\nvalue: \"Always\"\r\n\r\n# 镜像默认需要这些: \r\ndocker.io/calico/cni:master\r\ndocker.io/calico/node:master\r\ndocker.io/calico/kube-controllers:master\r\ndocker.io/calico/typha:master\r\n\r\n# 但由于镜像仓库在国外, 拉取不到, 可以换成我已经上传好的。\r\nsed -i \"s#docker.io/calico#registry.cn-hangzhou.aliyuncs.com/week-cnative#g\" calico-typha.yaml \r\nsed -i \"/week-cnative/ s/master/v3.28.0/g\" calico-typha.yaml\r\n\r\n# 安装Calico\r\n$ kubectl apply -f calico-typha.yaml \r\n```\r\n\r\n\r\n\r\n安装结束\r\n\r\n等待Calicao所有的Pod运行起来, 集群搭建成功。\r\n\r\n```shell\r\n[root@k8s-mn01 ~]# kubectl get no\r\nNAME       STATUS   ROLES           AGE   VERSION\r\nk8s-mn01   Ready    control-plane   25h   v1.29.6\r\nk8s-mn02   Ready    control-plane   25h   v1.29.6\r\nk8s-mn03   Ready    control-plane   25h   v1.29.6\r\nk8s-wn01   Ready    <none>          62m   v1.29.6\r\nk8s-wn02   Ready    <none>          43m   v1.29.6\r\n\r\n[root@k8s-mn01 ~]# kubectl get pod -n kube-system\r\nNAME                                       READY   STATUS    RESTARTS         AGE\r\ncalico-kube-controllers-67d65c9d9d-wqbv9   1/1     Running   2 (38m ago)      40m\r\ncalico-node-22r7n                          1/1     Running   7 (5m59s ago)    15m\r\ncalico-node-26kqv                          1/1     Running   7 (5m44s ago)    14m\r\ncalico-node-5w4lf                          1/1     Running   7 (5m57s ago)    14m\r\ncalico-node-nmqk9                          1/1     Running   0                26s\r\ncalico-node-qgxwr                          1/1     Running   13 (6m13s ago)   40m\r\ncalico-typha-75f8b94cd7-9ks9d              1/1     Running   0                5m17s\r\ncoredns-857d9ff4c9-t62xn                   1/1     Running   0                25h\r\ncoredns-857d9ff4c9-vxs8h                   1/1     Running   0                25h\r\netcd-k8s-mn01                              1/1     Running   2 (20h ago)      25h\r\netcd-k8s-mn02                              1/1     Running   1 (20h ago)      25h\r\netcd-k8s-mn03                              1/1     Running   1 (20h ago)      25h\r\nkube-apiserver-k8s-mn01                    1/1     Running   3 (75m ago)      25h\r\nkube-apiserver-k8s-mn02                    1/1     Running   1 (20h ago)      25h\r\nkube-apiserver-k8s-mn03                    1/1     Running   1 (20h ago)      25h\r\nkube-controller-manager-k8s-mn01           1/1     Running   4 (20h ago)      25h\r\nkube-controller-manager-k8s-mn02           1/1     Running   2 (26m ago)      25h\r\nkube-controller-manager-k8s-mn03           1/1     Running   1 (20h ago)      25h\r\nkube-proxy-h6s74                           1/1     Running   0                42m\r\nkube-proxy-jkphx                           1/1     Running   1 (20h ago)      25h\r\nkube-proxy-rn48p                           1/1     Running   1 (20h ago)      25h\r\nkube-proxy-wdj8w                           1/1     Running   0                61m\r\nkube-proxy-wv8jh                           1/1     Running   1 (20h ago)      25h\r\nkube-scheduler-k8s-mn01                    1/1     Running   3 (20h ago)      25h\r\nkube-scheduler-k8s-mn02                    1/1     Running   3 (25m ago)      25h\r\nkube-scheduler-k8s-mn03                    1/1     Running   1 (20h ago)      25h\r\n\r\n\r\n[root@k8s-mn01 ~]# nerdctl run --rm -it --net host -v /etc/kubernetes:/etc/kubernetes registry.aliyuncs.com/google_containers/etcd:3.5.12-0 etcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key --cacert /etc/kubernetes/pki/etcd/ca.crt --endpoints https://192.168.10.11:2379,https://192.168.10.22:2379,https://192.168.10.33:2379 endpoint health --write-out=table\r\n+----------------------------+--------+-------------+-------+\r\n|          ENDPOINT          | HEALTH |    TOOK     | ERROR |\r\n+----------------------------+--------+-------------+-------+\r\n| https://192.168.10.11:2379 |   true | 39.385214ms |       |\r\n| https://192.168.10.22:2379 |   true | 68.702829ms |       |\r\n| https://192.168.10.33:2379 |   true | 72.560861ms |       |\r\n+----------------------------+--------+-------------+-------+\r\n\r\n[root@k8s-mn01 ~]# nerdctl run --rm -it --net host -v /etc/kubernetes:/etc/kubernetes registry.aliyuncs.com/google_containers/etcd:3.5.12-0 etcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key --cacert /etc/kubernetes/pki/etcd/ca.crt --endpoints https://192.168.10.11:2379,https://192.168.10.22:2379,https://192.168.10.33:2379 endpoint status --write-out=table\r\n+----------------------------+------------------+---------+---------+-----------+------------+-----------+---------\r\n|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\r\n+----------------------------+------------------+---------+---------+-----------+------------+-----------+--------+\r\n| https://192.168.10.11:2379 | 6571fb7574e87dba |  3.5.12 |  5.6 MB |     false |      false |        17 |      60236 |              60236 |        |\r\n| https://192.168.10.22:2379 | a2fae84dac15fbd1 |  3.5.12 |  5.6 MB |      true |      false |        17 |      60236 |              60236 |        |\r\n| https://192.168.10.33:2379 | 514d51979dd338dc |  3.5.12 |  5.6 MB |     false |      false |        17 |      60236 |              60236 |        |\r\n+----------------------------+------------------+---------+---------+-----------+------------+-----------+--------+\r\n\r\n```\r\n\r\n安装成功之后, Kubernetes会生成一系列的目录, 重要的目录, 解释如下: \r\n\r\n```shell\r\n[root@k8s-mn01 ~]# tree /etc/kubernetes/\r\n/etc/kubernetes/\t\t\t\t\t\t\t\r\n├── admin.conf\t\t\t\t\t\t\t\t\r\n├── controller-manager.conf\t\t\t\t\t\r\n├── kubelet.conf\r\n├── manifests\r\n│   ├── etcd.yaml\r\n│   ├── kube-apiserver.yaml\r\n│   ├── kube-controller-manager.yaml\r\n│   └── kube-scheduler.yaml\r\n├── pki\r\n│   ├── apiserver-etcd-client.crt\r\n│   ├── apiserver-etcd-client.key\r\n│   ├── apiserver-kubelet-client.crt\r\n│   ├── apiserver-kubelet-client.key\r\n│   ├── apiserver.crt\r\n│   ├── apiserver.key\r\n│   ├── ca.crt\r\n│   ├── ca.key\r\n│   ├── etcd\r\n│   │   ├── ca.crt\r\n│   │   ├── ca.key\r\n│   │   ├── healthcheck-client.crt\r\n│   │   ├── healthcheck-client.key\r\n│   │   ├── peer.crt\r\n│   │   ├── peer.key\r\n│   │   ├── server.crt\r\n│   │   └── server.key\r\n│   ├── front-proxy-ca.crt\r\n│   ├── front-proxy-ca.key\r\n│   ├── front-proxy-client.crt\r\n│   ├── front-proxy-client.key\r\n│   ├── sa.key\r\n│   └── sa.pub\r\n├── scheduler.conf\r\n└── super-admin.conf\r\n```\r\n\r\n解释如下: \r\n\r\n1. admin.conf, controller-manager.conf, kubelet.conf, scheduler.conf, super-admin.conf：\r\n\r\n   这些是不同角色的Kubernetes配置文件，每个文件包含了对应角色的认证信息、访问控制配置和集群连接信息。通常由kubeconfig工具生成。\r\n\r\n2. manifests: 这个目录包含了Kubernetes集群中各个核心组件的静态配置清单（YAML文件），用于指定各个组件如何启动和配置。\r\n\r\n3. pki: 这个目录中包含了Kubernetes集群的公钥和私钥文件，以及CA证书，用于保证集群内部通信的安全性。具体文件包括:\r\n\r\n   **apiserver-etcd-client.crt, apiserver-etcd-client.key**: 用于API服务器与etcd客户端通信的证书。\r\n\r\n   **apiserver-kubelet-client.crt, apiserver-kubelet-client.key**: 用于API服务器与kubelet客户端通信的证书。\r\n\r\n   **apiserver.crt, apiserver.key**: API服务器的证书和私钥。\r\n\r\n   **ca.crt, ca.key**: 集群的根CA证书和私钥，用于签发其他证书。\r\n\r\n   **etcd/**: etcd存储相关的证书和私钥。\r\n\r\n   **front-proxy-ca.crt, front-proxy-ca.key**: 用于前置代理的CA证书和私钥。\r\n\r\n   **front-proxy-client.crt, front-proxy-client.key**: 用于前置代理客户端的证书和私钥。\r\n\r\n   **sa.key, sa.pub**: Kubernetes中的Service Account的私钥和公钥。\r\n\r\n\r\n\r\n## 结语\r\n\r\nKubernetes的安装方式有很多, kubeadm只是其中的一种, 其它比如Rancher的RKE (个人也比较喜欢), Kubesphere的KubeKey, Sealos等等, 都是安装很便捷的工具, 但各自的高可用方案, 底层的实现原理, 都如本文类似, 大差不差。以后有时间再尝试一下其它的安装方案吧, 到时候可以做个类比文章, 了解每一种方案的优缺点, 不同场景下, 选择最适用的方案。\r\n\r\n---\r\n\r\n[1] https://mp.weixin.qq.com/s/Hl2seS_Xn9dQsynpbS6Jiw\r\n\r\n[2] https://v1-29.docs.kubernetes.io/releases/version-skew-policy\r\n\r\n[3] https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm\r\n\r\n[4] https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability\r\n\r\n[5] https://kubernetes.io/docs/setup/production-environment/container-runtimes\r\n\r\n[6] https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#bootstrap-the-cluster"
        },
        {
          "id": "Rocky Linux 9.3 系统安装",
          "metadata": {
            "permalink": "/Rocky Linux 9.3 系统安装",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/04-Rocky Linux 9.3 系统安装.md",
            "source": "@site/blog/04-Rocky Linux 9.3 系统安装.md",
            "title": "Rocky Linux 9.3 系统安装",
            "description": "rocky",
            "date": "2024-06-05T21:35:00.000Z",
            "formattedDate": "2024年6月5日",
            "tags": [
              {
                "label": "rocky",
                "permalink": "/tags/rocky"
              },
              {
                "label": "linux",
                "permalink": "/tags/linux"
              }
            ],
            "readingTime": 4.01,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "Rocky Linux 9.3 系统安装",
              "title": "Rocky Linux 9.3 系统安装",
              "date": "2024-06-05 21:35",
              "tags": [
                "rocky",
                "linux"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
              "permalink": "/一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜"
            },
            "nextItem": {
              "title": "Netstat命令运用之，深入理解网络连接",
              "permalink": "/Netstat命令运用之，深入理解网络连接"
            }
          },
          "content": "<!-- ![logo](/assets/images/rockylinux.jpg) -->\r\n![rocky](/assets/images/rockylinux.jpg)\r\n### 写在前面\r\n\r\n​\t\tCentos最为Linux开源发行版中最受人欢迎的系列，即将迎来它的黄昏。其所带来的价值是无限的，地位是不可替代的。我也是其中受益的一份子。\r\n\r\n​\t\t正因如此，需要找到能够平替的新系统：差异化小，稳定健壮，提供长支持能力。\r\n\r\n​\t\tUbuntu，Fedora，SUSE，RedHat，Rocky......都是能够考虑的。\r\n\r\n​\t\t今天推荐的是Rocky Linux，与Centos几乎无异，命令通用，安装简单，开源，对标RH而进行的代码重构，且作者是前Centos项目的创始人，提供长期支持，提供Centos迁移方案等，是我认为该系统是下一个Centos。\r\n\r\n\r\n<!-- truncate -->\r\n\r\n\r\n### 安装步骤：\r\n\r\n1. 官网下载Rocky Linux 9.3镜像\r\n\r\n   注意：在本文编写时间，Rocky最新版本9.4发布，官网的下载页面默认均为9.4，下载地址为：\r\n\r\n   ```shell\r\n   https://download.rockylinux.org/pub/rocky/9/isos/x86_64/Rocky-9.4-x86_64-minimal.iso\r\n   ```\r\n\r\n   ​\t\t\t如果想要下载历史版本的镜像，地址与上面并不相同。目前Rocky 9.3下载地址为：\r\n\r\n   ```shell\r\n   https://dl.rockylinux.org/vault/rocky/9.3/isos/x86_64/Rocky-9.3-x86_64-minimal.iso\r\n   ```\r\n\r\n   \r\n\r\n2. Rocky Linux 9.3 系统安装\r\n\r\n   * Vmware 17版本演示.\r\n\r\n   * 新建虚拟机步骤不再赘述. \r\n\r\n     * 磁盘容量：50G\r\n\r\n     * 内存：1G；CPU：1核\r\n\r\n     * 选用 步骤1 中现在的镜像.\r\n\r\n     * 需要注意的是，在`新建虚拟机的导向过程`中，客户机操作系统的选择，版本这里选用的是：其它 Linux 5.x 内核 64 位. \r\n\r\n     * 启动虚拟机，出现如下界面. （ 了解Centos7系统的，相信并不陌生. ）默认第一步即可. \r\n\r\n       ![image-20240604143043854](/images/04-rocky_linux/image-20240604143043854.png)\r\n\r\n     * 进入安装界面，语言选择 中文简体 ；继续，进入该界面: \r\n\r\n       ​\t![image-20240604143507063](/images/04-rocky_linux/image-20240604143507063.png)\r\n\r\n       * 进入：软件选择，勾选标准安装. （ 附带上系统环境软件 ）.\r\n       * 进入：安装目的地，自定义划分磁盘：/boot分区：512M-1024M；swap分区：内存的2倍（若真实服务器内存>16GB，则swap分区大小为内存的1倍）；/分区：剩余全部 .\r\n\r\n       ![image-20240604144215882](/images/04-rocky_linux/image-20240604144215882.png)\r\n\r\n       * 进入：Root密码，配置root密码，并勾选下方的`允许使用root用户进行ssh远程登录.` .\r\n\r\n       * 开始安装 . \r\n\r\n       * 安装结束，进入登录界面. \r\n\r\n         * 提示信息：activate the web console with:  systemctl enable --now cockpit.socket\r\n\r\n           该含义是：使用命令：\"systemctl enable --now cockpit.socket\" 激活web管理界面。即该系统安装了cockpit, 激活cockpit, 能够实现图形化界面管理Rocky Linux系统。\r\n\r\n     3. IP地址配置，SSH远程连接\r\n\r\n        不同于Centos7，Rocky Linux 9.3中配置IP地址的方式，有较大的差别。配置方式有三种：\r\n\r\n        1. nmtui：通过图形化界面配置网络\r\n        2. nmcli：通过命令/交互方式配置网络（官方推荐）\r\n        3. 网卡配置文件方式配置网络（官方不推荐）\r\n\r\n        但由于本人是Centos7系统的重度感染者，依旧习惯于编辑网卡配置文件的方式。\r\n\r\n        ```shell\r\n        # 编辑网络配置文件\r\n        vi /etc/NetworkManager/system-connections/ens33.nmconnection\r\n        \r\n        [connection]\r\n        id=ens33\r\n        uuid=a5d63f95-a602-3897-943a-f48238886e99\r\n        type=ethernet\r\n        autoconnect-priority=-999\r\n        interface-name=ens33\r\n        \r\n        [ethernet]\r\n        \r\n        [ipv4]\r\n        method=manual\r\n        address=192.168.10.166/24\r\n        gateway=192.168.10.2\r\n        dns=114.114.114.114;8.8.8.8\r\n        \r\n        [ipv6]\r\n        addr-gen-mode=eui64\r\n        method=auto\r\n        \r\n        [proxy]\r\n        \r\n        # 重启生效\r\n        nmcli connection  load /etc/NetworkManager/system-connections/ens33.nmconnection\r\n        nmcli connection  up /etc/NetworkManager/system-connections/ens33.nmconnection \r\n        \r\n        # 查看生效\r\n        ifconfig ens33 或者 ip addr\r\n        ```\r\n\r\n        上述网卡文件内容解释 ( 内容来自官网：[RL9 - network manager - Documentation (rockylinux.org)](https://docs.rockylinux.org/zh/gemstones/network/RL9_network_manager/?h=networkmanager) )：\r\n\r\n        | connection     |                                                              |\r\n        | -------------- | ------------------------------------------------------------ |\r\n        | 键名称         | 描述                                                         |\r\n        | id             | con-name 的别名，其值为字符串。                              |\r\n        | uuid           | 设备唯一表示。                                               |\r\n        | type           | 连接的类型，其值可以是 ethernet、bluetooth、vpn、vlan 等等。 您可以使用 `man nmcli` 查看所有支持的类型。 |\r\n        | interface-name | 此连接绑定到的网络接口的名称，其值为字符串。                 |\r\n        | timestamp      | Unix 时间戳，以秒为单位。 此处的值是自1970年1月1日以来的秒数。 |\r\n        | autoconnect    | 是否随系统开机自启动。 值为布尔型。                          |\r\n        |                |                                                              |\r\n        | **ethernet**   |                                                              |\r\n        | 键名称         | 描述                                                         |\r\n        | mac-address    | MAC 物理地址。                                               |\r\n        | mtu            | 最大传输单位。                                               |\r\n        | auto-negotiate | 是否自动协商。 值为布尔型。                                  |\r\n        | duplex         | 值可以是 half （半双工）、full（全双工）                     |\r\n        | speed          | 指定网卡的传输速率。 100 即 100Mbit/s。 如果**auto-negotiate=false**，则必须设置 **speed** 键和 **duplex** 键；如果 **auto-negotiate=true**，则使用的速率为协商速率，此处的写入不生效（仅适用于BASE-T 802.3规范）；当非零时，**duplex** 键必须有值。 |\r\n        |                |                                                              |\r\n        | **ipv4**       |                                                              |\r\n        | 键名称         | 描述                                                         |\r\n        | address        | 分配的IP地址。                                               |\r\n        | gateway        | 分配的网关地址。                                             |\r\n        | dns            | 分配的dns地址，多个dns地址之间使用；分隔。                   |\r\n        | method         | IP获取的方法。 值是字符串类型。 值可以是：auto、disabled、link-local、manual、shared【auto：自动获取；manual：静态获取】 |\r\n\r\n     \r\n\r\n     **至此，可以通过远程连接工具，连接使用。**\r\n### 写在最后\r\n\r\n​\t\t\t对于大多数企业来讲，更换一个操作系统，风险，精力是巨大的，可能更多的是选择后来者的业务部署在新的系统，逐渐的转变。\r\n\r\n​\t\t\tRocky Linux希望能够成为各位的选择.......\r\n\r\n​"
        },
        {
          "id": "Netstat命令运用之，深入理解网络连接",
          "metadata": {
            "permalink": "/Netstat命令运用之，深入理解网络连接",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/03-Netstat命令运用之，深入理解网络连接.md",
            "source": "@site/blog/03-Netstat命令运用之，深入理解网络连接.md",
            "title": "Netstat命令运用之，深入理解网络连接",
            "description": "假设你要给朋友打一个电话，输入电话号码，之后朋友接通了电话，那么，通话过程就算是建立成功。这个接通的状态我们称之为ESTABLISHED。",
            "date": "2024-03-10T21:50:00.000Z",
            "formattedDate": "2024年3月10日",
            "tags": [
              {
                "label": "tcp",
                "permalink": "/tags/tcp"
              },
              {
                "label": "netstat",
                "permalink": "/tags/netstat"
              }
            ],
            "readingTime": 7.1,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "Netstat命令运用之，深入理解网络连接",
              "title": "Netstat命令运用之，深入理解网络连接",
              "date": "2024-03-10 21:50",
              "tags": [
                "tcp",
                "netstat"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "Rocky Linux 9.3 系统安装",
              "permalink": "/Rocky Linux 9.3 系统安装"
            },
            "nextItem": {
              "title": "SSL证书详解",
              "permalink": "/ssl证书详解"
            }
          },
          "content": "<!-- ![logo](/assets/images/avatar300.png) -->\r\n\r\n\r\n\r\n假设你要给朋友打一个电话，输入电话号码，之后朋友接通了电话，那么，通话过程就算是建立成功。这个接通的状态我们称之为`ESTABLISHED`。\r\n\r\n不管是在Windows系统，还是Linux系统，功能的提供都依靠背后的进程。因此，读懂进程语言是重要的。\r\n\r\n在上面的例子中，如果建立通话过程中遇到了故障了，又会是哪些状态呢？\r\n\r\n<!-- truncate -->\r\n\r\n## 状态分类\r\n\r\n1. **LISTEN**：表示进程正在监听指定端口，等待其他进程发起连接请求。好比你的朋友家电话在等待其他人拨打过来。\r\n2. **ESTABLISHED**：表示连接已经建立并且数据可以传输。\r\n3. **TIME_WAIT**：表示进程在等待，以确保远程端接收到最后的确认。好比你在等待你的朋友接听。\r\n4. **CLOSE_WAIT**：表示连接已经关闭，但你还在等待朋友对电话做出回应。\r\n5. **FIN_WAIT1 和 FIN_WAIT2**：表示连接即将关闭或正在等待远程端的关闭信号，好比你正准备要挂断电话一样。\r\n6. **CLOSING**：表示连接关闭过程中可能出现问题，类似于拨打过程中出现异常情况。\r\n7. **LAST_ACK**：表示你拨打了电话，但还在等待朋友的确认。\r\n8. **SYN_SENT 和 SYN_RECV**：表示正在建立连接的过程中，好比你通话过程的建立中。\r\n9. **UNKNOWN**：表示状态未知，可能是系统出现异常，好比是信号塔出现了故障，导致电话信号无法发送，或者无法拨打。\r\n\r\n\r\n\r\n## 查看状态\r\n\r\n以Linux系统为例，使用`netstat`或者`ss`命令进行查看进程连接状态\r\n\r\n```shell\r\n# netstat 命令的基本用法和常用选项：\r\n-a（all）：显示所有连接和监听端口，包括那些处于等待连接的状态。\r\n-t（tcp）：仅显示 TCP 协议相关的连接信息。\r\n-u（udp）：仅显示 UDP 协议相关的连接信息。\r\n-n（numeric）：以数字形式显示地址和端口号，而不进行反向域名解析。\r\n-p（program）：显示与连接相关的进程信息。\r\n-e（extended）：显示额外的详细信息，如用户 ID 和 inode 等。\r\n-r（route）：显示路由表信息。\r\n-c（continuous）：持续显示网络状态信息，而非一次性输出。\r\n\r\n# 查看tcp协议的进程\r\n[root@localhost ~]# netstat -naplt\r\nActive Internet connections (servers and established)\r\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \r\ntcp        0      0 127.0.0.1:9000          0.0.0.0:*               LISTEN      8955/php-fpm: maste \r\ntcp        0      0 0.0.0.0:3306            0.0.0.0:*               LISTEN      8159/mysqld         \r\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      6768/sshd           \r\ntcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      7065/master         \r\ntcp        0      0 0.0.0.0:10050           0.0.0.0:*               LISTEN      9603/zabbix_agentd  \r\ntcp        0      0 0.0.0.0:10051           0.0.0.0:*               LISTEN      9722/zabbix_server  \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51314         TIME_WAIT   -                   \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51300         TIME_WAIT   -                 \r\ntcp        0      0 192.168.10.110:22       192.168.10.1:33819      ESTABLISHED 21143/sshd: root@pt \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51278         TIME_WAIT   -                   \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51296         TIME_WAIT   -                   \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51346         TIME_WAIT   -                   \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51318         TIME_WAIT   -                   \r\ntcp        0      0 192.168.10.110:22       192.168.10.1:1834       ESTABLISHED 7250/sshd: root@pts \r\n\r\n# 输出信息说明：\r\nProto：协议类型，如tcp或udp。\r\nRecv-Q：接收队列中的数据量（以字节为单位）。\r\nSend-Q：发送队列中的数据量（以字节为单位）。\r\nLocal Address：本地IP地址和端口号。\r\nForeign Address：远程IP地址和端口号。\r\nState：连接状态。\r\nPID/Program name：与连接关联的进程ID和程序名称\r\n\r\n============================================================================================================\r\n\r\n# ss 命令的基本用法和常用选项：\r\n-t：显示 TCP 套接字信息。\r\n-u：显示 UDP 套接字信息。\r\n-l：仅显示监听状态的套接字。\r\n-a：显示所有套接字（包括监听和非监听状态）。\r\n-p：显示与套接字关联的进程信息。\r\n-n：以数字形式显示地址和端口号，而不进行反向域名解析。\r\n-s：按照协议统计套接字数量。\r\n-o：显示计时器信息。\r\n-i：显示套接字的详细信息。\r\n\r\n# 查看tcp协议的进程\r\n[root@localhost ~]# ss -naptn\r\nState     Recv-Q  Send-Q      Local Address:Port    Peer Address:Port              \r\nLISTEN      0      50               *:3306      \t\t*:*   \t\t   users:((\"mysqld\",pid=7740,fd=14))\r\n\r\n# 输出信息说明：\r\ntate：连接状态，这里是LISTEN，表示该端口正在监听来自其他计算机的连接请求。\r\nRecv-Q：接收队列中的数据量（以字节为单位），这里是0。\r\nSend-Q：发送队列中的数据量（以字节为单位），这里是50。\r\nLocal Address:Port：本地IP地址和端口号，*:3306表示所有网络接口上的3306端口。\r\nPeer Address:Port：对等方的IP地址和端口号，*:*表示任意远程地址和端口。\r\nusers：进程相关信息，包括进程名称（mysqld）、进程ID（pid=7740）和文件描述符（fd=14）。\r\n```\r\n\r\n\r\n\r\n## 底层原理\r\n\r\n进程之间的连接，都是依靠套接字来进行的，在linux系统中一般以.sock结尾的文件，称之为套接字文件。\r\n\r\n套接字：是进程与网络之间的接口，通过网络中不同主机上的一端，到另一端的通信，数据交换的机制。就好比是上方例子的电话本身就是套接字的抽象。在套接字文件中，会包含通信的基本信息：IP，端口等，就好比是打电话你要输入对方的电话号码。\r\n\r\n在建立过程当中，有TCP会话建立，UDP会话建立，这种建立过程，一般分为三步，熟称为`三次握手`；建立解除，一般分为四步，熟称为`四次挥手`。\r\n\r\n\r\n\r\n一图胜千言：\r\n\r\n![netstat](https://cdn.jsdelivr.net/gh/week2311/Images@main/netstat.png)结合上图，\r\n\r\n三次握手流程：\r\n\r\n1. 第一次握手。客户端向服务器发送一个SYN标志位置为1的包，并且包含一个随机生成的序列号（seq number），发送完毕后，客户端进入SYN_SENT状态，等待服务器的确认。\r\n2. 第二次握手。服务器收到客户端的SYN包后，会发送一个SYN和ACK标志位都置为1的包，其中ACK number设置为客户端的seq number加1，确定是回复的来自客户端的包，同时服务器也会生成一个随机数作为初始发送序号（initial sequence number）。发送完毕后，服务器进入SYN_RCVD状态。\r\n3. 第三次握手。客户端收到服务器的SYN+ACK包后，会发送一个ACK标志位置为1的包，其中ACK number设置为服务器的seq number加1，表示客户端确认收到了服务器的数据。发送完毕后，客户端和服务器进入ESTABLISHED状态，完成三次握手。\r\n\r\n四次挥手流程：\r\n\r\n1. 第一次挥手（FIN_WAIT_1）：客户端发送一个FIN报文，并指定一个序列号。这标志着客户端准备关闭从服务器到客户端的数据传输。客户端进入FIN_WAIT_1状态，此时客户端不再发送数据，但仍可接收数据。12345\r\n\r\n2. 第二次挥手（CLOSE_WAIT）：服务器收到客户端的FIN后，会发送一个ACK报文，确认号设置为收到的序列号加1。服务器进入CLOSE_WAIT状态，这意味着服务器已经收到客户端的关闭请求，但服务器可能还有数据需要发送给客户端。\r\n\r\n1. 第三次挥手（LAST_ACK）：服务器发送一个FIN报文，并指定一个序列号，表示服务器准备关闭从客户端到服务器的数据传输。服务器进入LAST_ACK状态。\r\n2. 第四次挥手（TIME_WAIT）：客户端收到服务器的FIN后，会发送一个ACK报文，确认号设置为收到的序列号加1。客户端进入TIME_WAIT状态，并等待一段时间（2MSL，即Maximum Segment Lifetime，报文段最大寿命），以确保服务器收到ACK报文。这段时间之后，客户端和服务器都进入CLOSED状态，完成四次挥手。\r\n\r\n注：在这里说明一下，网络查询的时候，会发现握手，或者挥手的过程中，有的连接会多seq序列的字段，比如四次挥手的服务器的第一次发送请求包，其实这里我的理解是seq作用是标记自己，回包的时候ack确认回复会使用到。但有的时候会发送两次连接，也就是两个seq发送，每一次的值不同，那么回包的时候也就以最后一次为准，所以有时图中未标记出，表示并未很大作用，没有展示出来。\r\n\r\n\r\n\r\n## 总结\r\n\r\n通过阅读本文，希望能够帮助你能更进一步的了解进程之间的网络连接，并且能够根据状态信息，在日常运维或者排查错误时，可以带来更多的思路，想法等等。"
        },
        {
          "id": "ssl证书详解",
          "metadata": {
            "permalink": "/ssl证书详解",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/02-SSL证书详解.md",
            "source": "@site/blog/02-SSL证书详解.md",
            "title": "SSL证书详解",
            "description": "在访问众多的网站中，相信大家都有遇到过下面的场景：",
            "date": "2024-02-25T12:20:00.000Z",
            "formattedDate": "2024年2月25日",
            "tags": [
              {
                "label": "ssl",
                "permalink": "/tags/ssl"
              },
              {
                "label": "ca",
                "permalink": "/tags/ca"
              }
            ],
            "readingTime": 5.523333333333333,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "ssl证书详解",
              "title": "SSL证书详解",
              "date": "2024-02-25 12:20",
              "tags": [
                "ssl",
                "ca"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "Netstat命令运用之，深入理解网络连接",
              "permalink": "/Netstat命令运用之，深入理解网络连接"
            },
            "nextItem": {
              "title": "Docusaurus博客搭建",
              "permalink": "/first-blog"
            }
          },
          "content": "<!-- ![logo](/assets/images/avatar300.png) -->\r\n\r\n在访问众多的网站中，相信大家都有遇到过下面的场景：\r\n\r\n![ssl](https://cdn.jsdelivr.net/gh/week2311/Images@main/ssl.png)\r\n\r\n一般遇到这种情况，大多数都是点击高级继续访问，类似那种`无视风险继续安装`的意思。但其实这是浏览器在向你发出的一种警示信号，告诉你：访问这个IP地址的网站是不安全的。那么浏览器是如何得知的呢，答案就是`证书`。\r\n\r\n\r\n<!-- truncate -->\r\n## SSL协议\r\n\r\n首先我们先了解一下ssl协议：\r\n\r\n> SSL（Secure Sockets Layer）协议是一种用于在计算机网络上安全传输数据的协议。它使用加密技术来确保在客户端和服务器之间传输的数据是安全的，不会被未经授权的人员窃取或篡改。通常用于保护网站上的敏感信息，例如登录凭据、信用卡信息等。\r\n\r\n也就是说，这是一个信息传输过程中的安全加密协议，防止黑客的攻击，信息的泄露等。上图中的提示信息是如此，也是http+ssl协议实现了这样的效果。\r\n\r\n工作原理是通过在客户端和服务器之间建立安全的连接，然后使用加密算法对数据进行加密和解密。这样可以确保在数据传输过程中，即使被截获，也无法被破解。\r\n\r\n那么此时就有一个问题，如何去让一个服务，或者服务器得到信任呢？\r\n\r\n\r\n\r\n## CA证书\r\n\r\nCA证书用来标识服务器的一个合法身份，可以理解为的居民身份证。\r\n\r\n如何判断一个人是否是合法的中国公民 ----- 身份证\r\n\r\n如何判断一台服务器是否是合法的 ----- CA证书。\r\n\r\n现实生活中，身份证是由公安机颁发，那么一个人要拿到身份证，一般要经过以下流程：\r\n\r\n![person_card](https://cdn.jsdelivr.net/gh/week2311/Images@main/person_card.png)\r\n\r\n服务器领域中，CA证书是由CA机构（Certificate Authority，凭证管理中心）颁发的，那么要拿到CA机构颁发下来的证书，与上方的流程类型：\r\n\r\n![ca_card](https://cdn.jsdelivr.net/gh/week2311/Images@main/ca_card.png)\r\n\r\n### 申请证书\r\n\r\n根据上图，申请证书步骤如下：\r\n\r\n注：以下都是模拟证书的生成过程，真实情况一般不需要这么复杂，掏钱就行。\r\n\r\n1. 我们需要生成私钥，即非对称加密技术（下文会讲解）。\r\n\r\n```shell\r\nopenssl genrsa -out cert.key 1024\r\n```\r\n\r\n2. 创建证书申请文件.csr\r\n\r\n```shell\r\nopenssl req -new -key cert.key -out cert.csr\r\n# 在这一步骤中， 需要我们输入申请信息：国家，地区，组织，email等。\r\n```\r\n\r\n![csr](https://cdn.jsdelivr.net/gh/week2311/Images@main/csr.png)\r\n\r\n3. CA签发证书\r\n\r\n```shell\r\nopenssl x509 -req -days 365 -sha256 -in cert.csr -signkey cert.key -out cert.pem\r\n# 注解：\r\n# -days 365：证书有限期365天\r\n# -sha256：使用 SHA-256 哈希算法来对证书进行签名\r\n# -in cert.csr：指定申请文件路径\r\n# -sign cert.key：指定用于签名 CSR 的私钥文件的路径和名称\r\n# -out cert.pem：指定生成的证书文件的路径和名称\r\n```\r\n\r\n至此，在你的目录下，会多出来以下文件：\r\n\r\n```shell\r\n# cert.key：服务器私钥文件。\r\n# cert.csr：证书申请文件。\r\n# cert.pem：pem格式的证书文件，其中包含私钥，证书等秘密数据。\r\n```\r\n\r\n\r\n\r\n### 证书后缀\r\n\r\n有的时候，你会遇到这两种后缀的证书：.pem；.crt。二者都是证书文件扩展名，主要区别如下：\r\n\r\n1. PEM格式是一种基于Base64编码的ASCII文本格式，可以包含证书、私钥等多种格式的加密信息。\r\n2. PEM格式的证书常常用在Unix/Linux系统中，在Apache、Nginx等服务器软件中也广泛使用。\r\n\r\n3. CRT格式是一种二进制编码格式，主要用于Windows操作系统中的程序中，并且通常只包含证书信息。\r\n\r\n所以当看到这两种后缀的证书时，不要懵，其实都是证书文件。\r\n\r\n\r\n\r\n## 非对称加密\r\n\r\n`对称`：左右两边的图案，符号，数据等是一致的，称之为互相对称。那么在这里的`左`和`右`，可以理解为`客户端`和`服务器`，需要一致的对象是密钥，可以理解为同一把钥匙，才能解开数据。这种方式我们称之为`对称加密`。\r\n\r\n`非对称`：左右两边的图案，符号，数据等是不一致的，称之为非对称。那么在这里的`左`和`右`，可以理解为`客户端`和`服务器`，需要的对象是不同的密钥，才能解开数据。这种方式我们称之为`非对称加密`。\r\n\r\n在非对称加密中，密钥需要两个：公钥，私钥。\r\n\r\n* 公钥：顾名思义，就是公开的密钥，大家都可以得到。公钥负责加密数据\r\n* 私钥：不对外公开，特殊加密过的密钥。私钥负责解密数据\r\n\r\n只要通过私钥解开了公钥加密过的数据，那么就表示是被信任的，数据安全隧道才会建立。\r\n\r\n![encryption](https://cdn.jsdelivr.net/gh/week2311/Images@main/encryption.png)\r\n\r\n## HTTPS工作原理\r\n\r\n至此，我们知道SSL证书是为了解决数据明文传输不安全的风险：窃听风险，篡改风险，冒充风险。\r\n\r\n以HTTP为例，在未通过SSL加密时，数据传输模式：\r\n\r\n![image-20240220093754487](https://cdn.jsdelivr.net/gh/week2311/Images@main/risk.png)\r\n\r\n为了解决上述的问题，使用SSL进行数据加密传输，一开始是采用的是对称加密，但是这一方式还是存在安全风险。\r\n\r\n因为对称加密使用一对相同的密钥进行加密，解密。且密钥的传输是放在报文当中，也会使得中间人拦截，并篡改报文中的真密钥，返回假密钥给到客户端。\r\n\r\n那么如何保证数据的加，解密是唯一性的，且到达客户端的公钥是正确的呢？答案就是上文说到的`非对称加密`和`证书`。\r\n\r\n一图胜千言：\r\n\r\n![procedure](https://cdn.jsdelivr.net/gh/week2311/Images@main/procedure.png)\r\n\r\n根据图中描述可知，文章开头出现的情况，属于Client（浏览器）得到的证书是未被验证通过的，未知的证书。\r\n\r\n## 结语\r\n\r\n随着时间的转变，技术的迭代更新，SSL也出现了安全问题。后续出现了它的继任者--TLS协议，该协议在SSL的基础之上发展而来，得到了进一步的改进和优化。\r\n\r\n总之，SSL/TLS协议可以实现加强数据在传输过程的安全，不被窃取，监听等。在对安全性要求高的场景下，还会用到专业的安全硬件设备。毕竟数据是所有。\r\n\r\n\r\n\r\n参考链接：\r\n\r\n* https://baijiahao.baidu.com/s?id=1685474345600994715&wfr=spider&for=pc\r\n\r\n* https://blog.csdn.net/qq_60243891/article/details/132530818\r\n\r\n* https://blog.csdn.net/keeppractice/article/details/126975243\r\n\r\n## 📝License\r\n\r\n[MIT](./LICENSE) © Week 100%"
        },
        {
          "id": "first-blog",
          "metadata": {
            "permalink": "/first-blog",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/01-first-blog.md",
            "source": "@site/blog/01-first-blog.md",
            "title": "Docusaurus博客搭建",
            "description": "网站由来",
            "date": "2024-02-15T19:20:00.000Z",
            "formattedDate": "2024年2月15日",
            "tags": [
              {
                "label": "docusaurus-theme-zen",
                "permalink": "/tags/docusaurus-theme-zen"
              },
              {
                "label": "lifestyle",
                "permalink": "/tags/lifestyle"
              }
            ],
            "readingTime": 2.84,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "first-blog",
              "title": "Docusaurus博客搭建",
              "date": "2024-02-15 19:20",
              "tags": [
                "docusaurus-theme-zen",
                "lifestyle"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "SSL证书详解",
              "permalink": "/ssl证书详解"
            }
          },
          "content": "<!-- ![logo](/assets/images/avatar300.png) -->\n## 网站由来\n对于碎片化的信息，知识，一直都没有很好的整理。有做过文档笔记，但是查找起来很麻烦，有的时候还不如直接百度来的快，因此也出现了重复性的搜索，于是为了方便自己，提高效率，萌生了搭建博客的想法。\ndocusaurus博客框架符合我的美感，之前也看过其他的博客框架，例如Hero，Wiki，Wordpress，Hexo等，但都不太喜欢。\n于是开始了解docusaurus，但自身并不懂前端语言，于是就在看到了河山的博客，感觉很不错，接下来的工作就是进行二开了，经过了三，四天的修改，便有了今天的样貌。\n\n## 项目目录\n\n基于docusaurus搭建的主题，结合了简单易用与其他开源页面设计方案、支持MDX和React、可扩展和定制等优点，以及加上多设计美观、博客与文档一体的主题，为你提供了一个优秀的个人页面解决方案。该主题使用🦖 <a href=\"https://docusaurus.io/\">Docusaurus</a>搭建，遵循[MIT](./LICENSE)协议。\n\n<!-- truncate -->\n\n> This is a theme built with docusaurus, which combines the simplicity and ease of use of docusaurus with other open source page design solutions, supports MDX and React, is extensible and customizable, and also has a beautiful design, a blog and documentation integrated theme, providing you with an excellent personal page solution.\n\n\n## 项目目录\n\n```bash\n├── blog                           # 博客\n│   ├── first-blog.md\n│   └── authors.yml                # 作者信息(可以多个作者)\n├── docs                           # 文档/笔记\n│   └── stack\n│         └──introduction.md       # 笔记介绍\n├── data                           # 项目/导航/友链数据\n│   ├── friend.ts                  # 友链\n│   ├── project.ts                 # 项目\n│   └── resource.ts                # 资源导航\n├── i18n                           # 国际化\n├── src\n│   ├── components                 # 组件\n│   ├── css                        # 自定义CSS\n│   ├── pages                      # 自定义页面\n│   ├── plugin                     # 自定义插件\n│   └── theme                      # 自定义主题组件\n├── static                         # 静态资源文件\n│   └── assets                     # 静态文件\n├── docusaurus.config.js           # 站点的配置信息\n├── sidebars.js                    # 文档的侧边栏\n├── package.json\n└── yarn.lock                      # 建议使用yarn保留\n```\n\n## 安装\n\n克隆仓库并安装依赖\n```bash\ngit clone https://github.com/week2311/blog.git ./blog\ncd blog\nyarn\nyarn start\n```\n\n国内仓库备份\n```bash\ngit clone https://github.com/week2311/blog.git ./blog\ncd blog\nyarn\n```\n\n生成静态网页代码(./build)\n\n```bash\nyarn run build\n```\n\n启动服务\n```bash\nyarn run serve\n```\n\n## Netlify托管\n对于个人而言，购买一台服务器来运行项目无疑是一项不菲的支出，虽然有云服务器。\nNetlify很好的解决了这个问题，每个月有 免费的 100G 流量、300分钟的构建，还有全球的CDN节点，对于我来说已经很够用了。\n在Netlify官网，点击 Deploy to Netlify，根据官方步骤走了就行了，很方便快捷，但前提是你需要将项目放到github仓库，\n且需要有一个域名。\n\n## 域名访问\n虽然有白嫖的域名，但建议去买一个属于自己的域名，一是国内访问快速，稳定，二是不需要很多繁琐的申请，快速。\n而且一年也就一杯咖啡的钱，有的新用户也就1块钱，像我这个就是阿里云上购买申请的，还是很不错的。\nSSL证书的话，可以通过腾讯云上去申请免费1年的，获取证书文件，然后将文件内容替换netlify上的，即可实现https访问。\n\n## 📝License\n\n[MIT](./LICENSE) © Week 100%"
        }
      ],
      "blogListPaginated": [
        {
          "items": [
            "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
            "Rocky Linux 9.3 系统安装",
            "Netstat命令运用之，深入理解网络连接",
            "ssl证书详解",
            "first-blog"
          ],
          "metadata": {
            "permalink": "/",
            "page": 1,
            "postsPerPage": 10,
            "totalPages": 1,
            "totalCount": 5,
            "blogDescription": "docusaurus-theme-zen",
            "blogTitle": "Blog"
          }
        }
      ],
      "blogTags": {
        "/tags/kubernetes": {
          "label": "kubernetes",
          "items": [
            "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜"
          ],
          "permalink": "/tags/kubernetes",
          "pages": [
            {
              "items": [
                "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜"
              ],
              "metadata": {
                "permalink": "/tags/kubernetes",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/centos": {
          "label": "centos",
          "items": [
            "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜"
          ],
          "permalink": "/tags/centos",
          "pages": [
            {
              "items": [
                "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜"
              ],
              "metadata": {
                "permalink": "/tags/centos",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/rocky": {
          "label": "rocky",
          "items": [
            "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
            "Rocky Linux 9.3 系统安装"
          ],
          "permalink": "/tags/rocky",
          "pages": [
            {
              "items": [
                "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
                "Rocky Linux 9.3 系统安装"
              ],
              "metadata": {
                "permalink": "/tags/rocky",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 2,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/linux": {
          "label": "linux",
          "items": [
            "Rocky Linux 9.3 系统安装"
          ],
          "permalink": "/tags/linux",
          "pages": [
            {
              "items": [
                "Rocky Linux 9.3 系统安装"
              ],
              "metadata": {
                "permalink": "/tags/linux",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/tcp": {
          "label": "tcp",
          "items": [
            "Netstat命令运用之，深入理解网络连接"
          ],
          "permalink": "/tags/tcp",
          "pages": [
            {
              "items": [
                "Netstat命令运用之，深入理解网络连接"
              ],
              "metadata": {
                "permalink": "/tags/tcp",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/netstat": {
          "label": "netstat",
          "items": [
            "Netstat命令运用之，深入理解网络连接"
          ],
          "permalink": "/tags/netstat",
          "pages": [
            {
              "items": [
                "Netstat命令运用之，深入理解网络连接"
              ],
              "metadata": {
                "permalink": "/tags/netstat",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/ssl": {
          "label": "ssl",
          "items": [
            "ssl证书详解"
          ],
          "permalink": "/tags/ssl",
          "pages": [
            {
              "items": [
                "ssl证书详解"
              ],
              "metadata": {
                "permalink": "/tags/ssl",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/ca": {
          "label": "ca",
          "items": [
            "ssl证书详解"
          ],
          "permalink": "/tags/ca",
          "pages": [
            {
              "items": [
                "ssl证书详解"
              ],
              "metadata": {
                "permalink": "/tags/ca",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/docusaurus-theme-zen": {
          "label": "docusaurus-theme-zen",
          "items": [
            "first-blog"
          ],
          "permalink": "/tags/docusaurus-theme-zen",
          "pages": [
            {
              "items": [
                "first-blog"
              ],
              "metadata": {
                "permalink": "/tags/docusaurus-theme-zen",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/lifestyle": {
          "label": "lifestyle",
          "items": [
            "first-blog"
          ],
          "permalink": "/tags/lifestyle",
          "pages": [
            {
              "items": [
                "first-blog"
              ],
              "metadata": {
                "permalink": "/tags/lifestyle",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        }
      },
      "blogTagsListPath": "/tags"
    }
  },
  "docusaurus-plugin-ideal-image": {},
  "docusaurus-plugin-pwa": {},
  "@easyops-cn/docusaurus-search-local": {},
  "docusaurus-bootstrap-plugin": {},
  "docusaurus-mdx-fallback-plugin": {}
}