{
  "docusaurus-plugin-content-docs": {
    "default": {
      "path": "/docs",
      "versions": [
        {
          "name": "current",
          "label": "Next",
          "isLast": true,
          "path": "/docs",
          "mainDocId": "stack/introduction",
          "docs": [
            {
              "id": "stack/free",
              "path": "/docs/free",
              "sidebar": "stack"
            },
            {
              "id": "stack/introduction",
              "path": "/docs/Stack",
              "sidebar": "stack"
            },
            {
              "id": "stack/kernel-update",
              "path": "/docs/Centos7系统---内核升级",
              "sidebar": "stack"
            },
            {
              "id": "stack/lscpu",
              "path": "/docs/lscpu",
              "sidebar": "stack"
            },
            {
              "id": "stack/time-sync",
              "path": "/docs/时间同步服务",
              "sidebar": "stack"
            },
            {
              "id": "stack/top",
              "path": "/docs/top",
              "sidebar": "stack"
            }
          ],
          "draftIds": [],
          "sidebars": {
            "stack": {
              "link": {
                "path": "/docs/Stack",
                "label": "stack/introduction"
              }
            }
          }
        }
      ],
      "breadcrumbs": true
    }
  },
  "docusaurus-plugin-content-blog": {
    "default": {
      "blogs": [
        {
          "id": "kwok",
          "metadata": {
            "permalink": "/kwok",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/09-K8S集群构建而中道崩殂，这TM！还怎么运维.md",
            "source": "@site/blog/09-K8S集群构建而中道崩殂，这TM！还怎么运维.md",
            "title": "K8S集群构建而中道崩殂，这TM！还怎么运维",
            "description": "在之前的KubeCon上，了解到Kwok技术，印象比较深刻，是因为它能够秒级去创建出上千个Node，Pod，当然这些是假的。",
            "date": "2024-08-26T10:45:00.000Z",
            "formattedDate": "2024年8月26日",
            "tags": [
              {
                "label": "kubernetes",
                "permalink": "/tags/kubernetes"
              },
              {
                "label": "kwok",
                "permalink": "/tags/kwok"
              }
            ],
            "readingTime": 3.1,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "kwok",
              "title": "K8S集群构建而中道崩殂，这TM！还怎么运维",
              "date": "2024-08-26 10:45",
              "tags": [
                "kubernetes",
                "kwok"
              ],
              "authors": "Week"
            },
            "nextItem": {
              "title": "K8S快问快答（一）",
              "permalink": "/K8s-quicktalk"
            }
          },
          "content": "在之前的KubeCon上，了解到Kwok技术，印象比较深刻，是因为它能够秒级去创建出上千个Node，Pod，当然这些是假的。\r\n\r\n但是它对于学习Kubernetes的人，资源不充足，或者模拟测试的场景，都非常友好。我就有一个体会，比如：我想对调度策略中的亲和性，进行一个实践操作，验证效果，但是前提是我得有一个Kubernetes集群。如果我有环境，那还好；如果没有，那浪费在搭建上的时间会很多。\r\n\r\n如果使用Kwok，我可以几秒钟完成搭建功能，并专注于实践，甚至我不需要虚拟机，直接安装在Windows/Mac中。\r\n\r\n<!-- truncate -->\r\n\r\n## 先看效果：\r\n\r\n我使用一台Rocky9.3的虚拟机，模拟20台虚机，和一套Pig的服务。\r\n\r\n![image-20240821164906397](https://bexp.135editor.com/files/users/1331/13319639/202408/9PRffXnS_IGSL.png?auth_key=1725206399-0-0-1306b2a73e91544dea7317eb8824a376)\r\n\r\n在这里，我可以自己通过yaml文件去创建资源，给节点打标签，污点等，去控制pod的一些行为等等，几乎与真实的k8s集群一样。Kwok模拟的是Apiserver的行为。\r\n\r\n\r\n\r\n## 安装,配置\r\n\r\nLinux上的安装方式：\r\n\r\n1. 容器运行时安装，见文章[1]\r\n\r\n   注意：安装版本要求：containerd >= 1.7；\r\n\r\n   ​\t  需要安装cni插件。\r\n\r\n2. 二进制安装kwok，见官网[2] 的`Binary Releases`安装步骤。\r\n\r\n3. 创建集群：\r\n\r\n   ```shell\r\n   kwokctl create cluster --kube-apiserver-image=registry.aliyuncs.com/google_containers/kube-apiserver:v1.29.6 --kube-controller-manager-image=registry.aliyuncs.com/google_containers/kube-controller-manager:v1.29.6 --kube-scheduler-image=registry.aliyuncs.com/google_containers/kube-scheduler:v1.29.6 --etcd-image=registry.aliyuncs.com/google_containers/etcd:3.5.11-0\r\n   ```\r\n\r\n   需要使用到控制面组件的镜像，用于模拟。指定镜像下载地址，默认是国外镜像仓库地址。\r\n\r\n   注：如果控制面的组件，都是用二进制安装的，则不需要指定镜像地址，默认优先使用二进制的。\r\n\r\n4. 创建node：\r\n\r\n   ```shell\r\n   kwokctl scale node --replicas 20 --config ~/.kwok/clusters/pig/kwok.yaml --name pig\r\n   ```\r\n\r\n   -c：指定配置文件位置。\r\n\r\n   --name：指定集群名称\r\n\r\n5. 创建Pod：\r\n\r\n   可以通过命令创建：\r\n\r\n   ```shell\r\n   kubectl create deployment pod --image=pod --replicas=5\r\n   ```\r\n\r\n   也可以通过编写好的yaml文件创建：\r\n\r\n   ```shell\r\n   kubectl apply -f pig-all.yaml\r\n   ```\r\n\r\n6. 结果：\r\n\r\n   ```shell\r\n   [root@localhost ~]# kubectl get no\r\n   NAME          STATUS   ROLES    AGE    VERSION\r\n   node-000000   Ready    master   92m    kwok-v0.6.0\r\n   node-000001   Ready    master   100m   kwok-v0.6.0\r\n   node-000002   Ready    master   98m    kwok-v0.6.0\r\n   node-000003   Ready    agent    98m    kwok-v0.6.0\r\n   node-000004   Ready    agent    98m    kwok-v0.6.0\r\n   node-000005   Ready    agent    98m    kwok-v0.6.0\r\n   node-000006   Ready    agent    98m    kwok-v0.6.0\r\n   node-000007   Ready    agent    98m    kwok-v0.6.0\r\n   node-000008   Ready    agent    98m    kwok-v0.6.0\r\n   node-000009   Ready    agent    98m    kwok-v0.6.0\r\n   node-000010   Ready    agent    98m    kwok-v0.6.0\r\n   node-000011   Ready    agent    98m    kwok-v0.6.0\r\n   node-000012   Ready    agent    98m    kwok-v0.6.0\r\n   node-000013   Ready    agent    98m    kwok-v0.6.0\r\n   node-000014   Ready    agent    98m    kwok-v0.6.0\r\n   node-000015   Ready    agent    98m    kwok-v0.6.0\r\n   node-000016   Ready    agent    98m    kwok-v0.6.0\r\n   node-000017   Ready    agent    98m    kwok-v0.6.0\r\n   node-000018   Ready    agent    98m    kwok-v0.6.0\r\n   node-000019   Ready    agent    98m    kwok-v0.6.0\r\n   \r\n   [root@localhost ~]# kubectl get pod -o wide -n pig \r\n   NAME                                      READY   STATUS    RESTARTS   AGE   IP          NODE          NOMINATED NODE   READINESS GATES\r\n   minio-854ddc8b4-7k4cq                     1/1     Running   0          71m   10.0.5.1    node-000005   <none>           <none>\r\n   pig-auth-7985cddf5-8t4x4                  1/1     Running   0          76m   10.0.13.1   node-000013   <none>           <none>\r\n   pig-codegen-6759fd865b-gx9pd              1/1     Running   0          72m   10.0.8.1    node-000008   <none>           <none>\r\n   pig-gateway-66b687d645-k4h6h              1/1     Running   0          77m   10.0.3.2    node-000003   <none>           <none>\r\n   pig-monitor-5777b76db9-2hfw8              1/1     Running   0          74m   10.0.11.2   node-000011   <none>           <none>\r\n   pig-mysql-df66c67f-wq8kp                  1/1     Running   0          84m   10.0.11.1   node-000011   <none>           <none>\r\n   pig-redis-6f58b56fd-gl5hw                 1/1     Running   0          84m   10.0.4.1    node-000004   <none>           <none>\r\n   pig-register-555859b5fb-2vk5r             1/1     Running   0          79m   10.0.3.1    node-000003   <none>           <none>\r\n   pig-sentinel-dashboard-678d6c454d-6qjfc   1/1     Running   0          73m   10.0.14.2   node-000014   <none>           <none>\r\n   pig-upms-biz-6f96fc46d-p2gm2              1/1     Running   0          75m   10.0.14.1   node-000014   <none>           <none>\r\n   pig-xxl-job-admin-69fc56b8cf-ltcxp        1/1     Running   0          71m   10.0.2.1    node-000002   <none>           <none>\r\n   ```\r\n   \r\n   \r\n\r\n## 脚本安装\r\n\r\n**安装过程太麻烦不想做？没关系，尝鲜的脚本已经编写好了。**\r\n\r\n**Kwok_Shell链接: https://pan.baidu.com/s/1N3XTNn52v3fm4tmCdjD5fw?pwd=LK79 \r\n提取码: LK79**\r\n\r\n![image-20240823165739143](https://bexp.135editor.com/files/users/1331/13319639/202408/pKejQuBN_AeeL.png?auth_key=1725206399-0-0-4d2dc97bf4e4abff2c52f15e67cee928)\r\n\r\n注意：当执行 \"kwokctl create\" 那个命令时，报错：\r\n\r\n```shell\r\nERROR Failed to setup config err=\"cmd wait: nerdctl create --name=kwok-kwok-etcd --pull=never --entrypoint=etcd --network=kwok-kwok --restart=unless-stopped --label=com.docker.compose.project=kwok-kwok --publish=32765:2379/tcp registry.aliyuncs.com/google_containers/etcd:3.5.11-0 --name=node0 --auto-compaction-retention=1 --quota-backend-bytes=8589934592 --data-dir=/etcd-data --initial-advertise-peer-urls=http://0.0.0.0:2380 --listen-peer-urls=http://0.0.0.0:2380 --advertise-client-urls=http://0.0.0.0:2379 --listen-client-urls=http://0.0.0.0:2379 --initial-cluster=node0=http://0.0.0.0:2380: exit status 1\\ntime=\\\"2024-08-23T16:53:23+08:00\\\" level=fatal msg=\\\"failed to verify networking settings: failed to create default network: subnet 10.4.0.0/24 overlaps with other one on this address space\\\"\\n\" cluster=kwok\r\n```\r\n\r\n此时手动执行一下报错信息中的nerdctl命令即可。这大概率是nerdctl命令本身的Bug，有时间可以提个issue。\r\n\r\n\r\n\r\n## 结语\r\n\r\n通过这个工具，我们能够以最小的资源模拟最大的效果，不管是k8s的初学者，模拟架构的资源使用，压测等场景都是一个方便的工具。\r\n\r\n\r\n\r\n---\r\n\r\n参考链接\r\n\r\n[1].https://mp.weixin.qq.com/s/ehKm7rvvxWCGgUm1etePOA\r\n\r\n[2].https://kwok.sigs.k8s.io/docs/user/installation/"
        },
        {
          "id": "K8s-quicktalk",
          "metadata": {
            "permalink": "/K8s-quicktalk",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/08-K8S快问快答（一）.md",
            "source": "@site/blog/08-K8S快问快答（一）.md",
            "title": "K8S快问快答（一）",
            "description": "一. k8s中所说的对象，资源是什么意思？",
            "date": "2024-08-01T10:45:00.000Z",
            "formattedDate": "2024年8月1日",
            "tags": [
              {
                "label": "kubernetes",
                "permalink": "/tags/kubernetes"
              }
            ],
            "readingTime": 10.88,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "K8s-quicktalk",
              "title": "K8S快问快答（一）",
              "date": "2024-08-01 10:45",
              "tags": [
                "kubernetes"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "K8S集群构建而中道崩殂，这TM！还怎么运维",
              "permalink": "/kwok"
            },
            "nextItem": {
              "title": "细梳Service：一文练就融会贯通！",
              "permalink": "/Service"
            }
          },
          "content": "### 一. k8s中所说的对象，资源是什么意思？\r\n\r\n> 答：\r\n>\r\n> 对象：是你在k8s集群中能看到，能管理的实物，称之为对象。例如：Pod，Deployment，Service等。\r\n>\r\n> 资源：是k8s集群中定义的一系列，可以被创建，被使用，被查询等的事物，称之为资源。例如：上述的对象，无形的规则和配置等。\r\n>\r\n> 如果说k8s中的资源有哪些，你可以说：有Pod，Depolyment，Configmap等，因为这些是能够被使用的；\r\n>\r\n> 如何说k8s集群中Pod对象有哪些，可能是通过kubectl命令创建出来多个副本的Pod，每一个Pod都是Pod资源的对象，或者是持久化的实例。\r\n<!-- truncate -->\r\n\r\n\r\n### 二. Pod每次被创建，都会伴随一个Pause的容器产生，该容器的作用是什么？[1]\r\n\r\n> 答：Pause容器：又称Infra容器，Pause表面意思为暂停，即该容器是一个处于暂停状态的容器。\r\n>\r\n> ​\t作用1：\r\n>\r\n> ​\t\tPod是一个容器组，其中至少包含一个容器，Pod才能够被创建运行出来，若容器数为0，则Pod被当成销毁。Pause容器保证了Pod的存活，在意外情况下，即使Pod中的业务容器都被删除，因Pause一直是暂停容器，Pod也不会被当成销毁。除非手动删除Pod。\r\n>\r\n> ​\t作用2：\r\n>\r\n> ​\t\tPod是一个容器组，其中的容器都共享网络空间，而网络空间的创建，是由Pause去做的，其它容器只需要加入到Pause所在的网络空间中即可。\r\n>\r\n> ​\t作用3：\r\n>\r\n> ​\t\t同网络空间一样，Pod中的容器需要共用进程空间，即PID名称空间，也是由Pause创建，并作为Pod中，其它容器的第一个进程，即init进程。接管，回收各个容器所产生的僵尸进程。\r\n\r\n\r\n\r\n### 三. Pod健康检查的三种探针策略，以及各自的区别是什么？\r\n\r\n> 答：Liveness Probe：存活探针。\r\n>\r\n> ​      目的：确定Pod中的容器是否正在运行。\r\n>\r\n> ​      行为：根据探测的条件，如果探针探测失败，kubelet会杀死该容器，但是会受到容器的restartPolicy的影响。\r\n>\r\n> Readiness Probe：就绪探针。\r\n>\r\n> ​      目的：确定Pod中的容器是否准备好接受流量，或者说容器中的服务是否准备就绪。\r\n>\r\n> ​      行为：根据探测的条件，如果探针探测失败，endpoint控制器会将从与Pod匹配的所有Service的endpoint中删除该Pod的IP地址。\r\n>\r\n> ​            通过 kubectl get pod 查看，Pod的状态为Ready列：0/1，不就绪的状态。\r\n>\r\n> Startup Porbe：启动探针。\r\n>\r\n> ​      目的：确定容器中的服务是否已经启动。\r\n>\r\n> ​      行为：如果配置了启动探针，则会禁用掉其它所有的探针，直到它探测成功为止，如果探测失败，kubelet会杀死该容器，\r\n>\r\n> ​            但同样会受到容器的restartPolicy的影响。\r\n\r\nYaml示例：\r\n\r\n```shell\r\nspec:\r\n  containers:\r\n  - name: my-container\r\n    image: my-image\r\n    livenessProbe:\r\n      httpGet:\r\n        path: /healthz\r\n        port: 8080\r\n      initialDelaySeconds: 3\r\n      periodSeconds: 3\r\n    readinessProbe:\r\n      httpGet:\r\n        path: /ready\r\n        port: 8080\r\n      periodSeconds: 5\r\n    startupProbe:\r\n      httpGet:\r\n        path: /healthz\r\n        port: 8080\r\n      failureThreshold: 30\r\n      periodSeconds: 10\r\n```\r\n\r\n\r\n\r\n### 四. Pod中定义容器的restartPolicy三种重启策略，各自的区别，以及与探针的互相影响有哪些？\r\n\r\n> 答：三种重启策略：Always，OnFailure，Never\r\n>\r\n> ​\tAlways：论容器是因为什么原因退出，都会自动重启。\r\n>\r\n> ​\tOnFailure：只有当容器以非0状态码退出时，发生错误时，才会重启。\r\n>\r\n> ​\tNever：容器退出后，无论是什么原因，都不会重启。\r\n>\r\n> ​\t对探针的影响：\r\n>\r\n> ​\t如果是Liveness Probe探针失败导致容器需要重启：\r\n>\r\n> ​\t\t如果RestartPolicy是 Always 或 OnFailure，容器会被重启；\r\n>\r\n> ​\t\t如果是Never，容器不会重启，Pod将会保持在失败状态（但是由于受控制器的影响，保持期待，会再创建新的Pod来运行）。\r\n\r\nYaml示例：\r\n\r\n```shell\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: my-pod\r\nspec:\r\n  restartPolicy: OnFailure\r\n  containers:\r\n  - name: my-container\r\n    image: my-image\r\n    livenessProbe:\r\n      httpGet:\r\n        path: /healthz\r\n        port: 8080\r\n      initialDelaySeconds: 3\r\n      periodSeconds: 3\r\n```\r\n\r\n\r\n\r\n### 五. Qos服务质量是什么？[2]\r\n\r\n> 答：Qos服务质量：是根据是否对Pod设置了资源的请求(request)和限制(limit)，来去给Pod进行分类的机制。\r\n>\r\n> ​\t说人话就是：给Pod进行分类，这些Pod是一类，那些Pod是另一类，分类的标准：是否限制了Pod使用多少的内存/CPU。有限制的Pod是一类，没有限制的是另一类。根据分类，会去他们区别对待，有限制的好比是VIP客户，没有限制的是普通客户，当节点上资源不够时，要对他们进行驱离，则会先对最低类型的Pod动手。\r\n>\r\n> ​\tQos的类型有：\r\n>\r\n> ​\t\tGuaranteed（SVIP客户）：最高类型，在资源压力下，该类型的Pod是最后考虑驱逐的。\r\n>\r\n> ​\t\t\t划分标准：Pod定义了资源限制：即request和limit，且二者的值必须相等，这意味着该Pod分配了一个固定的资源值。\r\n>\r\n> ​\t\t\t\t\t  只有在Pod自身超过了限制的资源，或者没有更低优先级的Pod，才会考虑进行驱逐。\r\n>\r\n> ​\t\tBurstable（VIP客户）：一般类型，在资源压力下，该类型的Pod会在BestEffort类型Pod之后驱逐。\r\n>\r\n> ​\t\t\t划分标准：Pod至少定义了request资源请求，但没有设置limit限制。或者不符合Guaranteed和BestEffort划分标准的。\r\n>\r\n> ​\t\t\t\t\t  这样类型的Pod，由于没有limit限制，可以无限制使用节点资源，故也有隐患，在驱逐时，考虑倒数第二驱逐。\r\n>\r\n> ​\t\tBestEffort（普通客户）：最低级别，在资源压力下，该类型的Pod会首先被驱逐。\r\n>\r\n> ​\t\t\t划分标准：Pod没有设置 requests 和 limits。\r\n>\r\n> 要查看Pod属于哪一类型，可以使用`kubectl describe pod <pod名> -n 命名空间`查看：\t\r\n\r\n```shell\r\n$ kubectl describe pod test-7955cf7657-qh4wq -n test\r\n......\r\nQoS Class:                   BestEffort\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\r\nEvents:\r\n......\r\n```\r\n\r\n\r\n\r\n### 六. Pod驱逐行为产生的原因，行为，影响都有哪些？[3]\r\n\r\n> 答：驱逐 字面意思是赶走；\r\n>\r\n> ​\tPod驱逐：将Pod从所在的节点上赶走。\r\n>\r\n> ​\t产生的原因：一般是由于节点压力导致的驱逐，即节点上的`内存`，`存储空间`，`进程可用进程数`的不足，为了保证核心程序，Pod，系统正常运行，不至于暴死，kubelet做的Pod销毁，释放资源空间的行为。\r\n>\r\n> ​\t行为：kubelet根据节点资源的使用情况，通过cgroup获取值，并与驱逐条件进行对比，超过阈值则产生驱逐行为。\r\n>\r\n> ​\t条件分为硬驱逐 和 软驱逐。\r\n>\r\n> * 硬驱逐：达到限制，kubelet会立即杀死Pod，且不会正常终止Pod，回收资源。类似kill -9\r\n> * 软驱逐：达到限制，kubelet不会立即杀死Pod，会给要“枪毙”的Pod一段时间，去做自己的善后事情，即正常终止Pod，回收资源。\r\n>\r\n> ​\tPod的驱逐顺序：\r\n>\r\n> ​\t\t1. 识别资源压力，确定需要释放的资源量\r\n>\r\n> ​\t\t2. 考虑资源使用超过请求的 Pods：\r\n>\r\n> ​\t\t\t* 所有 BestEffort Pods\r\n>\r\n> ​\t\t\t* 使用超过请求的 Burstable Pods，在这个组内，按 Pod 优先级排序，低优先级先驱逐\r\n>\r\n> ​\t\t3. 如果仍需要更多资源，考虑资源使用未超过请求的 Pods：\r\n>\r\n> ​\t\t\t* 使用未超过请求的 Burstable Pods，在这个组内，按 Pod 优先级排序，低优先级先驱逐\r\n>\r\n> ​\t\t\t* Guaranteed Pods（只有在极端情况下） 同样在这个组内，按 Pod 优先级排序，低优先级先驱逐\r\n>\r\n> ​\t\t4. 在整个过程中，kubelet 会持续评估是否已释放足够的资源\r\n\r\n\r\n\r\n### 七. Pod终止的相关时间有哪些？[4]\r\n\r\n> 答：Pod终止受到：体面终止周期时间的影响。\r\n>\r\n> ​\t体面终止周期：即给Pod中的所有容器在被杀死之前，做好自己的善后工作，比如请求的处理/连接的关闭，数据的清理等，的时间。默认是30s。\r\n>\r\n> ​\t如果终止周期/宽限期设置为0，则会立即终止，发生的行为会是ApiServer上会删除Pod的相关信息，但是Pod实现在在节点上运行的容器可能还在一直的运行着。\r\n>\r\n> ​\t且如果要强制终止/删除Pod，可以这样操作：\r\n\r\n```shell\r\nkubectl delete pod <pod-name> --grace-period=0 --force\r\n# --grace-period：设置宽限时间\r\n```\r\n\r\n\r\n\r\n### 八. Yaml文件中的**Annotations**和**Label**是什么，二者的区别。\r\n\r\n> 答：Annotations（注解）：是一种声明被创建的`资源`（例如Pod）元数据的机制。\r\n>\r\n> ​\t通俗的讲，就是你创建了一个Pod，那这个Pod的是谁创建的，什么时间创建的，使用的仓库地址是多少等一系列的信息，可以通过注解的方式去记录。这样做的好处，是能够让其它人一眼了解这个Pod的属性信息。\r\n>\r\n> 通过注解可以实现这样的目的。例如：\r\n\r\n```shell\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: my-website\r\n  annotations:\r\n    kubernetes.io/change-cause: \"Initial deployment of my website\"\r\n    owner: \"marketing-team@company.com\"\r\n    git-commit: \"abc123\"\r\n    deployment-date: \"2023-07-29\"\r\nspec:\r\n  replicas: 3\r\n  selector:\r\n    matchLabels:\r\n      app: my-website\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: my-website\r\n    spec:\r\n      containers:\r\n      - name: my-website\r\n        image: my-website:1.0\r\n        ports:\r\n        - containerPort: 80\r\n```\r\n\r\n> 类似添加描述信息的作用，功能（以上的owner等名称都可以自定义）。但这只是注解功能的其一。\r\n>\r\n> 另一个重要的功能是：配置管理/注入。\r\n>\r\n> 思考一下，如果我们在传统方式下，想要将nginx能接收的最大请求体大小为50M，这种情况，我们直接修改nginx.conf配置文件即可，但是在Pod中如何去做呢？\r\n>\r\n> 通过注解的方式，例如：\r\n\r\n```shell\r\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: example-ingress\r\n  annotations:\r\n    nginx.ingress.kubernetes.io/proxy-body-size: \"50m\"\r\n```\r\n\r\n> 注解的组成：前缀/名称:值。\r\n>\r\n> ​\t\t\t前缀规则：可选，如果使用，格式要符合有效的DNS域名格式（不会解析，只是要求格式要与DNS格式一致，结尾要有一个/）。\r\n>\r\n> ​\t\t    名称：必须存在，必须字母/数字开头和结尾，最多63个字符。\r\n>\r\n> \r\n>\r\n> ​\tLabel：主要用来对创建的集群`资源`进行一个筛选的功能。\r\n>\r\n> ​\t例如：你创建了10个pod，5个Pod是nginx，5个Pod是http。这个时候你如何只过滤去nginx的呢：可以给nginx Pod打标签，这样进行查询的使用，可以通过`kubectl get pod -n <名称空间名称> -l 标签名`去进行过滤查询。如下所示：\r\n\r\n```shell\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: nginx-pod-1\r\n  labels:\r\n    app: nginx\r\n    environment: production\r\n    \r\nkuebctl get pod -l app=nginx\r\n```\r\n\r\n​\t主要区别：\r\n\r\n| Annotations                    | Labels                         |\r\n| ------------------------------ | ------------------------------ |\r\n| 不用于选择和查询资源           | 用于选择和查询资源             |\r\n| 用于存储辅助信息，和配置管理   | 用于组织和分类资源             |\r\n| 命名规则相对宽松               | 有严格的命名规范：key: value   |\r\n| 相对静态，通常在资源创建时设置 | 可以发生改变，用于反应资源状态 |\r\n| API基本上忽略内容，只存储      | 被API服务器积极处理和验证      |\r\n\r\n\r\n\r\n### 九. kubectl create 和 apply 二者操作的区别是什么？\r\n\r\n答：\r\n\r\n| create                                        | apply                                                      |\r\n| --------------------------------------------- | ---------------------------------------------------------- |\r\n| 用于首次创建资源，如果资源存在，则报错        | 用于创建和更新资源，如果资源不存在则创建；如果已存在则更新 |\r\n| 命令式操作，直接告诉K8S创建一个资源，简单暴力 | 声明式操作，描述系统的期望状态，让系统决定如何达到这个状态 |\r\n| 适用于快速创建                                | 适用于管理复杂的应用部署，更新，版本控制                   |\r\n| 适用一次性操作或调试                          | 适用于生产环境                                             |\r\n\r\n\r\n\r\n### 十. 如何将一个不可用的节点设置成为不可调度，排空，并下线？\r\n\r\n答：流程如下:\r\n\r\n* 第一步：将节点设置为不可调度。防止新的Pod调度到该节点上。\r\n\r\n  ```shell\r\n  kubectl cordon <节点名>\r\n  ```\r\n\r\n* 第二步：排空节点上的Pod。\r\n\r\n  ```shell\r\n  kubectl drain <节点名> --ignore-daemonsets --delete-emptydir-data\r\n  \r\n  # --ignore-daemonsets：允许跳过DaemonSet管理的Pod\r\n  # --delete-emptydir-data：允许删除使用emptyDir卷的Pod\r\n  ```\r\n\r\n  注：你也完全可以直接执行kubectl drain操作，而省略cordon步骤。原因是：drain会先cordon之后，再尝试优雅地终止该节点上地Pod（排空）。\r\n\r\n* 第三步：确定节点已排空。\r\n\r\n  ```shell\r\n  # 检查节点状态\r\n  kubectl describe node <节点名>\r\n  # 重点关注部分：\r\n  # Unschedulabel：应该显示为true\r\n  # Taints：包含node.kubernetes.io/unschedulable:NoSchedule这个污点。\r\n  # Non-terminated Pods：这里应该只列出DaemonSet管理的Pod（如果有的话），其它类型都已驱逐。\r\n  \r\n  # 检查Pod都已排空，处理DaemonSet类型的Pod\r\n  Kubectl get pod -A -o wide | grep <节点IP>\r\n  ```\r\n\r\n* 第四步：从集群中删除节点。\r\n\r\n  ```shell\r\n  kubectl delete node <节点名>\r\n  ```\r\n\r\n* 第五步：后续该节点的DaemonSet类型的Pod，会由于该Pod的控制器监控到节点不存在，会强制终止该类型的Pod，垃圾回收。\r\n\r\n\r\n\r\n---\r\n\r\n## \t\t\t\t\t\t\t\t参考链接\r\n\r\n[1].https://www.51cto.com/article/767383.html\r\n\r\n[2].https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/pod-qos/\r\n\r\n[3].https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/#eviction-signals-and-thresholds\r\n\r\n[4].https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/"
        },
        {
          "id": "Service",
          "metadata": {
            "permalink": "/Service",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/07-细梳Service：一文练就融会贯通.md",
            "source": "@site/blog/07-细梳Service：一文练就融会贯通.md",
            "title": "细梳Service：一文练就融会贯通！",
            "description": "Pod是Kubernetes集群中的最小调度单元。举例来说，Kubernetes是整个公司，定义了一系列的规章制度(功能)，而公司是由人构成的，去进行工作，也就是说，人是公司中的最小调度单元，Pod亦是如此：人的工作是写文档，做表格等；Pod的工作是管理运行应用程序（容器化）。",
            "date": "2024-07-21T20:45:00.000Z",
            "formattedDate": "2024年7月21日",
            "tags": [
              {
                "label": "kubernetes",
                "permalink": "/tags/kubernetes"
              },
              {
                "label": "service",
                "permalink": "/tags/service"
              }
            ],
            "readingTime": 18.043333333333333,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "Service",
              "title": "细梳Service：一文练就融会贯通！",
              "date": "2024-07-21 20:45",
              "tags": [
                "kubernetes",
                "service"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "K8S快问快答（一）",
              "permalink": "/K8s-quicktalk"
            },
            "nextItem": {
              "title": "CentOS7失宠，谁又会成为下一个甄嬛!",
              "permalink": "/CentOS7失宠，谁又会成为下一个甄嬛!"
            }
          },
          "content": "> Pod是Kubernetes集群中的最小调度单元。举例来说，Kubernetes是整个公司，定义了一系列的规章制度(功能)，而公司是由人构成的，去进行工作，也就是说，人是公司中的最小调度单元，Pod亦是如此：人的工作是写文档，做表格等；Pod的工作是管理运行应用程序（容器化）。\r\n>\r\n> Pod是一个或多个容器的集合。好比：Pod是一个快递盒子，应用就被打包放在了这个盒子里，在其中它们共享网络和存储资源。\r\n\r\n<!-- truncate -->\r\n### 写在前面\r\n\r\n继上一篇的《一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜》，能够构建起一个单Master/多Master的Kubernetes集群，也了解了集群的构成等基础知识。本着后面写一些基础的Kubernetes概念性文章，例如控制器等，但官网已经解释很明白了，有比自己做的更好的，自己就不去再做了，写一些自己比较摸棱两可的，梳理一番。本文将探讨：Service，Endpoints，EndpointSlices，无头Service，有/无状态服务。\r\n\r\n\r\n\r\n### Pod访问问题\r\n\r\n想象一下，你有一个博客项目：\r\n\r\n​\t1. 在传统模式下：\r\n\r\n​\t   需要将Blog博客项目部署到一台安装了Apache服务的服务器上，用户通过在浏览器上，输入该服务器的IP地址/域名:端口（如：   \t192.168.10.100:8080）来访问博客页面。\r\n\r\n​\t2. 在Kubernetes下：\r\n\r\n​\t   Blog博客项目被打包到 `Pod`中运行。你的博客现在被运行在Pod中，而不是直接运行在服务器上。\r\n\r\n那么此时访问Pod中的Blog项目，再像传统模式一样，通过Pod的IP:Port，是不行的。原因如下：\r\n\r\n* **隔离性**：Pod使用容器技术，与宿主机（运行Pod的实际服务器）网络是隔离的。\r\n\r\n* **不稳定性**：Pod可能会被销毁和重新创建，每次IP地址都可能改变。\r\n* **端口冲突**：在传统服务器上，不同应用必须使用不同的端口。但在Kubernetes中，不同Pod可以使用相同的端口，因为它们是隔离的。如果多个Blog的Pod，端口都一样，怎么知道访问就是自己的呢。\r\n\r\n为了应对这一问题，在Kubernetes容器管理平台上，单独有了一个角色，就是为了将运行在一个或一组 Pod 中的应用程序公开到网络的方法：即Service。\r\n\r\n也就是说，Service是流量访问的入口/网关。\r\n\r\n\r\n\r\n### Endpoints与EndpointSlices\r\n\r\n再介绍Service之前，先来说一下Endpoints，以及它的改进版：EndpointSlices。\r\n\r\nEndponints，翻译过来叫端点，同Service创建而自动创建，与Service同名。它的作用是：\r\n\r\n*\t\t\t记录Service关联的后端Pod的IP地址和端口的集合。\r\n\r\n* 动态更新，当Service关联的后端Pod，删除，增加，更新了，Endpoints的地址记录也会被更新。\r\n\r\n借此，我们不需要关心Pod若被删除，或者IP地址更改而带来的寻址不到的问题。\r\n\r\n借用官网的例子: [1]\r\n\r\n我们可以在创建一个Service，并将其关联到后端的Pod之后，查看Endpoints的信息：\r\n\r\n```shell\r\n# kubectl get endpoints nginx-service\r\nNAME            ENDPOINTS                                      AGE\r\nnginx-service   10.244.0.5:80,10.244.0.6:80,10.244.0.7:80      1m\r\n```\r\n\r\nEndpointSlices，是Endpoints的改进版本，同Service创建而自动创建，与Service同名。它的作用是：\r\n\r\n* 提供与`Endpoints`相同的基本功能。\r\n* 支持更高效的`Endpoints`信息的更新和扩展。\r\n\r\n我们可以再创建一个Service，并将其关联到后端的Pod之后，查看EndpointSlices的信息：\r\n\r\n```shell\r\n# kubectl get endpointslices -l kubernetes.io/service-name=nginx-service\r\nNAME                  ADDRESSTYPE   PORTS   ENDPOINTS                 AGE\r\nnginx-service-abc12   IPv4          80      10.244.0.5,10.244.0.6     2m\r\nnginx-service-def34   IPv4          80      10.244.0.7                2m\r\n```\r\n\r\n对比`Endpoints` 和 `EndpointSlices`的输出信息，我们可以发现：\r\n\r\nEndpointsSlices将 ENDPOINTS列的信息 由之前的一个，切割划分成为了多个，这样做的优势是：\r\n\r\n1. 在我们的例子中，只有 3 个 Pod，端点信息的存储值也就3个IP，信息量不大。enpointSlices与endpoint的差异不大。\r\n2. 但如果是100个或1000个Pod的情况，如果使用Endpoints，端口信息的存储值会成百上千，如下面所示：\r\n\r\n```shell\r\n# kubectl get endpoints nginx-service\r\nNAME            ENDPOINTS                                      AGE\r\nnginx-service   Pod-1的IP:Port, ... , Pod-1000的IP:Port         1m\r\n```\r\n\r\n​\t这样管理起来会很臃肿，麻烦，低效，也消耗集群的资源：假如其中的一个Pod信息发生了改变，则需要更新整个Endpoints；\r\n\r\n​\t但如果使用EndpointSlices，因为它把一个端点信息划分成多个，可以更高效地更新和管理这些信息。同样是更新一个Pod信息，就不需要更新整个Endpoints，只需要更新Pod所在的其中一个EndpointSlices即可。\r\n\r\n总结Endpoints:\r\n\r\n* 单个大对象包含所有端点信息。\r\n* 通常需要整体更新。\r\n* 在大规模部署中可能面临性能瓶颈。\r\n* 较早引入，广泛支持。\r\n* 最多包含1000个端点。\r\n\r\n总结EndpointSlices：\r\n\r\n* 分片存储：端点信息被分割成多个较小的 EndpointSlice 对象。\r\n\r\n* 高效更新：可以只更新变化的部分，而不是整个列表。\r\n\r\n* 增强的元数据：包含额外信息，如拓扑数据，有助于更智能的路由决策。\r\n\r\n* 更好的性能：设计用于处理大规模集群和服务。\r\n* 一个 Service 可以链接到多个 EndpointSlice 之上\r\n* 默认情况下，一旦现有 EndpointSlice 都包含至少 100 个端点，Kubernetes 就会创建一个新的 EndpointSlice。\r\n\r\nKubernetes官方推荐使用 EndpointSlice API（Kubernetes v1.21 [stable]） 替换 Endpoints。\r\n\r\n\r\n\r\n### Service概述\r\n\r\n创建一个Service，并与后端的Pod关联起来并不难。先看实践：\r\n\r\n```shell\r\n# 创建名为my-nginx的Service，并关联集群中带有run:my-nginx的pod。\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: my-nginx\r\n  labels:\r\n    run: my-nginx\r\nspec:\r\n  ports:\r\n  - port: 80\r\n    protocol: TCP\r\n  selector:\r\n    run: my-nginx\r\n\r\n$ kubectl get svc my-nginx\r\nNAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\r\nmy-nginx   ClusterIP   10.0.162.149   <none>        80/TCP    21s\r\n\r\n$ kubectl describe svc my-nginx\r\nName:                my-nginx\r\nNamespace:           default\r\nLabels:              run=my-nginx\r\nAnnotations:         <none>\r\nSelector:            run=my-nginx\r\nType:                ClusterIP\r\nIP Family Policy:    SingleStack\r\nIP Families:         IPv4\r\nIP:                  10.0.162.149\r\nIPs:                 10.0.162.149\r\nPort:                <unset> 80/TCP\r\nTargetPort:          80/TCP\r\nEndpoints:           10.244.2.5:80,10.244.3.4:80\r\nSession Affinity:    None\r\nEvents:              <none>\r\n\r\n$ kubectl get endpointslices -l kubernetes.io/service-name=my-nginx\r\nNAME             ADDRESSTYPE   PORTS   ENDPOINTS               AGE\r\nmy-nginx-7vzhx   IPv4          80      10.244.2.5,10.244.3.4   21s\r\n```\r\n\r\nService:`my-nginx`被创建，查看`endpointslices`信息也都正确。此时在节点上访问IP：10.0.162.149，能够访问到Nginx的首页。\r\n\r\n实践操作很简单，映射到底层的样貌，如下图所示；\r\n\r\n![service](/assets/images/service.png)\r\n1. Master节点上，管理员通过`kubectl`命令等方式，创建Service，同时，自动创建同名的Endpoints/EndpointSlices，存储后端的Pod信息。\r\n2. 至此Service和EndpointSlices的使命已经结束，只是创建出一些数据而已。\r\n3. 每个节点上都由一个Kube-Proxy的组件，它的职责：定期监视`Service`和`EndpointSlices`的数据信息是否更新，删除，创建等，并自动创建/更新每个节点上的Iptables/Ipvs规则。\r\n4. 之后，Pod间的通信，或者外部的通信访问，都是通过节点的Iptables规则/Ipvs配置来将数据包进行转发，传输。若是跨节点的Pod通信，则还需要借助网络插件来实现，例如Flannel/Calico等。\r\n\r\n由上述，我们得知Service本事只是创建一组元数据罢了，实际做流量转发的是节点上的规则。那么规则的创建进而可以通过创建Service的类型不同，而不同。\r\n\r\n\r\n\r\n### Service类型\r\n\r\n要控制kube-proxy所创建的iptables规则/ipvs配置，可以通过修改Service的类型去控制。\r\n\r\n默认Service有四种类型，分别是：\r\n\r\n1. ClusterIP（默认类型）\r\n\r\n- 特点：分配一个集群内部的 IP 地址，仅在集群内部可访问。例如Pod和Pod之间的通信，如果是Kubernetes集群外的请求访问是不行的。\r\n\r\n2. NodePort\r\n\r\n- 特点：宿主机上开启一个特定端口（默认范围 30000-32767），直接与Pod的端口相连，实现访问宿主机IP：端口能够直接访问到Pod。用途：允许外部访问，但通常用于开发或测试。\r\n\r\n  适用场景：简单的外部访问需求，如演示或临时访问。\r\n\r\n3. LoadBalancer\r\n\r\n- 特点：使用云提供商的负载均衡器暴露服务，创建外部的负载均衡器，将流量分发到后端Pod上，即该类型的Service本身的负载均衡是与外部的负载均衡服务关联，实现。创建该类型的Service的原因也是创建一堆数据，进而外部的负载均衡器根据这些数据，创建负载调度规则，再结合kube-proxy实现流量接入：外部客户端 -> 云负载均衡器 -> Kubernetes 节点 -> kube-proxy -> Pod。\r\n\r\n  适用场景：生产环境中需要高可用和负载均衡的外部服务。\r\n\r\n4. ExternalName\r\n\r\n- 特点：将外部服务映射到集群内部，并以DNS的形式用于Pod中服务对外部服务访问。\r\n\r\n  适用场景：集成外部服务，如数据库或 API端点。\r\n\r\n具体创建方式，可以查询官网: [1]\r\n\r\n\r\n\r\n### Service的负载均衡\r\n\r\nService实现负载均衡的方式有两种：iptables 和 ipvs。\r\n\r\n1. Iptables模式：kube-proxy 在 Linux 上使用 iptables 配置数据包转发规则的一种模式。\r\n\r\n   因此，该模式下流量的负载均衡，是由iptables规则来实现的。\r\n\r\n   验证：\r\n\r\n```shell\r\n# 使用yaml部署测试Pod，Service\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: nginx\r\n  labels:\r\n    app: nginx\r\n  namespace: week\r\nspec:\r\n  replicas: 3\r\n  selector:\r\n    matchLabels:\r\n      app: nginx\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: nginx\r\n    spec:\r\n      containers:\r\n      - name: nginx\r\n        image: nginx:latest\r\n        imagePullPolicy: IfNotPresent\r\n        ports:\r\n          - containerPort: 80\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: nginx-svc\r\n  namespace: week\r\nspec:\r\n  selector:\r\n    app: nginx\r\n  ports:\r\n    - name: nginx\r\n      protocol: TCP\r\n      port: 80\r\n      targetPort: 80\r\n      \r\n# 查看\r\n[root@k8s-mn01 ~]# kubectl get pod -n week\r\nNAME                     READY   STATUS    RESTARTS   AGE\r\nnginx-54b6f7ddf9-4jt5r   1/1     Running   0          8m5s\r\nnginx-54b6f7ddf9-6stvj   1/1     Running   0          8m5s\r\nnginx-54b6f7ddf9-kr2vn   1/1     Running   0          8m5s\r\n\r\n[root@k8s-mn01 ~]# kubectl get svc -n week\r\nNAME        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\r\nnginx-svc   ClusterIP   172.16.73.164   <none>        80/TCP    9m27s\r\n\r\n```\r\n\r\n```shell\r\n# 去到任意一个工作节点上，过滤出相对应的iptables条目\r\n[root@k8s-wn01 ~]# iptables-save | grep 172.16.73.164\r\n-A KUBE-SERVICES -d 172.16.73.164/32 -p tcp -m comment --comment \"week/nginx-svc:nginx cluster IP\" -m tcp --dport 80 -j KUBE-SVC-4LRRYOSSZE5QDVLA\r\n-A KUBE-SVC-4LRRYOSSZE5QDVLA ! -s 10.0.0.0/8 -d 172.16.73.164/32 -p tcp -m comment --comment \"week/nginx-svc:nginx cluster IP\" -m tcp --dport 80 -j KUBE-MARK-MASQ\r\n\r\n$ 解释说明：\r\n$ 第一条规则表示：将发往 nginx 服务集群 IP (172.16.73.164) 的 80 端口 TCP 流量转发到特定的服务处理链 KUBE-SVC-4LRRYOSSZE5QDVLA。\r\n$ 第二条规则表示：对于不是来自集群内部(非 10.0.0.0/8)但目标是 nginx 服务的流量,标记它们以进行后续的地址伪装(MASQUERADE)处理【即地址转换处理】。\r\n\r\n# 也就是说，所有的规则都指向了 KUBE-SVC-4LRRYOSSZE5QDVLA，再次过滤\r\n[root@k8s-wn01 ~]# iptables-save | grep KUBE-SVC-4LRRYOSSZE5QDVLA\r\n-A KUBE-SVC-4LRRYOSSZE5QDVLA -m comment --comment \"week/nginx-svc:nginx -> 10.208.67.129:80\" -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-S2NHQD4JFZO6ZT3O\r\n-A KUBE-SVC-4LRRYOSSZE5QDVLA -m comment --comment \"week/nginx-svc:nginx -> 10.208.67.130:80\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-WCKBX4BRVWPAVTTD\r\n-A KUBE-SVC-4LRRYOSSZE5QDVLA -m comment --comment \"week/nginx-svc:nginx -> 10.42.234.135:80\" -j KUBE-SEP-2ELJFQOAS5FHEUHZ\r\n\r\n$ 第一条规则：以约33.33%的概率随机将流量转发到 IP 为 10.208.67.129、端口为 80 的 nginx 服务端点。\r\n$ 第二条规则：在剩余流量中，以 50% 的概率随机将流量转发到 IP 为 10.208.67.130、端口为 80 的 nginx 服务端点。\r\n$ 第三条规则：将剩余的所有流量（约33.33%）转发到 IP 为 10.42.234.135、端口为 80 的 nginx 服务端点。\r\n$ 这三条规则一起实现了对 nginx 服务的负载均衡，将流量均匀地分配到三个不同的后端服务。\r\n```\r\n\r\n总结：Service iptables模式下的负载均衡，是靠不同的iptables规则，通过百分比的形式去进行流量分摊的。\r\n\r\n优点：\r\n\r\n* 性能高效：直接在内核空间处理流量，开销较小。\r\n* 可靠性强：作为 Linux 内核的一部分，iptables 非常稳定。\r\n* 无需额外进程：不需要单独的代理进程，减少了系统资源消耗。\r\n* 配置灵活：可以实现复杂的网络规则和策略。\r\n\r\n缺点：\r\n\r\n* 规则复杂度：随着服务数量增加，iptables 规则会变得非常复杂，影响可维护性。\r\n* 更新开销大：每次服务变更都需要刷新所有 iptables 规则，在大规模集群中可能造成明显延迟。\r\n* 规则数量限制：大规模集群中可能达到 iptables 规则数量的上限。\r\n* 排障困难：由于规则复杂，故障排查和问题定位较为困难。\r\n* 缺乏高级负载均衡特性：相比其他方案，支持的负载均衡算法较少。\r\n\r\n\r\n\r\n2. Ipvs模式：kube-proxy 监视 Kubernetes Service 和 EndpointSlice， 然后调用 `netlink` 接口创建 IPVS 规则， 并定期与 Kubernetes Service 和 EndpointSlice 同步 IPVS 规则。\r\n\r\n   Ipvs本身是Linux内核中的实现负载均衡的一种模块，机制，使用`netfilter`函数实现。相信了解LVS负载均衡器的并不陌生。\r\n\r\n   注意：若本身未检测到 IPVS 内核模块，则 kube-proxy 会退回到 iptables 代理模式运行。\r\n\r\n   修改iptables模式 --> ipvs模式：\r\n\r\n   ```shell\r\n   # 编辑kube-proxy的配置文件，修改mode的模式为ipvs\r\n   kubectl edit configmap kube-proxy -n kube-system\r\n   mode: \"ipvs\"  \r\n   \r\n   # 重启所有的kube-proxy的Pod\r\n   kubectl delete pod -l k8s-app=kube-proxy -n kube-system\r\n   ```\r\n\r\n   验证：\r\n\r\n   ```shell\r\n   # 还是接着上面的例子\r\n   [root@k8s-mn01 ~]# ipvsadm -ln \r\n   IP Virtual Server version 1.2.1 (size=4096)\r\n   Prot LocalAddress:Port Scheduler Flags\r\n     -> RemoteAddress:Port           Forward Weight ActiveConn InActConn  \r\n   TCP  172.16.73.164:80 rr\r\n     -> 10.42.234.135:80             Masq    1      0          1         \r\n     -> 10.208.67.129:80             Masq    1      0          1         \r\n     -> 10.208.67.130:80             Masq    1      0          1         \r\n   \r\n   # 截取nginx-svc的负载均衡规则，可以发现流量通过service的IP进入，并以rr轮询的方式转发到后端的pod IP上。\r\n   ```\r\n\r\n总结：Service ipvs模式下的负载均衡，是通过负载调度规则实现，通过不同的轮询算法去进行流量分摊的。\r\n\r\n优点：\r\n\r\n性能更高：相比 iptables，IPVS 在大规模集群中有更好的性能表现，尤其是在服务数量很多时。\r\n\r\n1. 更多的负载均衡算法：支持多种调度算法，如轮询、加权轮询、最少连接等。\r\n2. 更好的可扩展性：使用哈希表作为数据结构，规则查找的时间复杂度为O(1)，不受规则数量影响。\r\n3. 连接保持能力：支持 FULLNAT 模式和连接保持，有利于应用层会话保持。\r\n4. 更新效率高：服务更新时只需要更新相关的 IPVS 规则，不需要刷新所有规则。\r\n\r\n缺点：\r\n\r\n1. 配置复杂度增加：相比 iptables，IPVS 的配置和管理可能更复杂。\r\n2. 额外的内核依赖：需要确保 Linux 内核支持 IPVS 模块，可能需要额外的配置或升级。\r\n3. 故障排查难度增加：由于其工作在更底层，排查问题可能需要更专业的网络知识。\r\n4. 可能需要额外的内存：在某些情况下，IPVS 可能比 iptables 消耗更多的内存。\r\n5. 兼容性问题：一些旧版本的 Linux 发行版可能对 IPVS 的支持不够完善\r\n\r\n\r\n\r\n### 有、无头Service\r\n\r\n先说结论：\r\n\r\n​\t会被分配一个集群IP，且具备负载均衡功能的称为有头Service，也就是我们上方所说的标准Service。\r\n\r\n​\t不会被分配一个集群IP，且不具备负载均衡功能的称为无头Service，是Kubernetes中的一种特殊Service类型。\r\n\r\n二者对比：\r\n\r\n| 有头Service（标准Service） |         无头Service          |\r\n| :------------------------: | :--------------------------: |\r\n|     会被分配一个集群IP     |       不会被分配集群IP       |\r\n|      具备负载均衡功能      |      不具备负载均衡功能      |\r\n|      由kube-proxy处理      |       kube-proxy不处理       |\r\n| 平台提供负载均衡和路由支持 | 平台不提供负载均衡或路由支持 |\r\n\r\n##### 创建无头Service：\r\n\r\n1. 将`.spec.type`设置为`ClusterIP`（这是默认值）\r\n2. 将`.spec.clusterIP`设置为`\"None\"`\r\n\r\n示例YAML：\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: my-headless-service\r\nspec:\r\n  clusterIP: None  # 这使其成为无头Service\r\n  selector:\r\n    app: my-app\r\n  ports:\r\n    - port: 80\r\n      targetPort: 8080\r\n```\r\n\r\n##### 无头Service工作原理：\r\n\r\n​\t无头Service不使用虚拟IP地址和代理来配置路由和数据包转发。相反，它通过内部的DNS服务报告各个Pod的端点IP地址。这些DNS记录由集群的DNS服务（Coredns）提供。\r\n\r\nDNS的配置方式取决于Service是否定义了选择器（即上述YAML的selector字段）：\r\n\r\n1. **带选择算符的Service**：\r\n   - Kubernetes控制平面创建EndpointSlice对象，自动找到匹配的Pod\r\n   - DNS查询会直接返回所有匹配Pod的IP地址\r\n2. **无选择算符的Service**：\r\n   - 控制平面不创建EndpointSlice对象，即不自动查到Pod\r\n   - 如果是指向外部服务（`ExternalName`类型），DNS返回那个类型的Service的名称\r\n   - 如果是手动配置的内部端点，DNS返回这些端点的IP地址\r\n   - 定义无选择算符的无头 Service 时，`port` 必须与 `targetPort` 匹配\r\n\r\n简单来说，带选择算符的Service自动管理Pod，而无选择算符的服务则更灵活，可以指向任何你想要的地方，无论是集群内还是集群外。\r\n\r\n也因此，DNS的配置方式受此影响，若是带选择算符，则DNS的配置就是那一组标签的Pod IP；若不带，则DNS的配置会根据Service的具体配置而变化。\r\n\r\n##### 适用场景：\r\n\r\n无头Service特别适用于以下场景：\r\n\r\n1. 需要直接访问Pod IP的场景\r\n2. 客户端需要知道所有后端Pod的情况\r\n3. 分布式系统，如数据库集群\r\n4. 自定义负载均衡\r\n5. 服务发现\r\n\r\n##### 处理Pod IP变化的问题\r\n\r\n一个常见的疑问是：既然Pod的IP会变化，为什么还要直接访问Pod的IP呢？\r\n\r\n在分布式系统中，这种直接访问仍然是必要的。以分布式数据库系统为例：\r\n\r\n1. 新数据库节点（Pod）加入集群时，通过无头Service的DNS查询获取所有现有节点的IP。\r\n2. 新节点连接到这些IP，加入集群。\r\n3. 其他节点感知新节点的加入，更新成员列表。\r\n4. 如果某个节点的IP发生变化，集群通过内部机制检测并更新这个变化。\r\n\r\n这个过程由数据库系统的内部服务发现机制处理，而无头Service提供了必要的DNS支持。\r\n\r\n\r\n\r\n### 有/无状态服务\r\n\r\nService本质是暴露服务到网络上，提供访问入口的。那么针对不同的服务，可能所需要创建的Service类型（有/无头）也会不同。\r\n\r\n服务分为：有状态服务；无状态服务。\r\n\r\n我们以：是否需要记住连接/交互之前的信息的服务，判断是有/无状态的服务。\r\n\r\n需要记住，则是有状态服务；不需要记住，则是无状态服务。\r\n\r\n举个栗子：\r\n\r\n想象你去一家咖啡店:\r\n\r\n* 无状态服务: 每次你点单,服务员都会问你要什么,即使你刚刚才点过。他们不记得你之前的选择。\r\n* 有状态服务: 服务员记得你之前点的内容。当你再来时,他们可能会说:\"还是老样子吗?\"\r\n\r\n再举个栗子：\r\n\r\n* 有状态服务会保存用户的信息和之前的交互历史。比如,在线购物网站记住你的购物车内容,即使你关闭了浏览器。\r\n\r\n* 无状态服务断开连接，关闭浏览器，则用户刚刚所记录/处理的信息，都会随之清空。\r\n\r\n有状态的服务：\r\n\r\n​\t数据库：例如Mysql的数据会持久化到磁盘，那么每次Mysql重启，他都能够找到上次数据保存的位置，这本身也是状态的维护。\r\n\r\n​\t网页的Session：也是有状态的一种体现，它能够根据Session，自动得知用户上次访问/停留的页面是什么。\r\n\r\n无状态的服务：\r\n\r\n​\t静态web服务器: 每次请求都是独立的,不需要记住之前的交互。且显示的内容都是一样的，也不需要记录。\r\n\r\n​\tDNS(域名系统): 每次查询都是独立的,不依赖于之前的查询。\r\n\r\n二者对比；\r\n\r\n|                 无状态服务                 |                     有状态服务                     |\r\n| :----------------------------------------: | :------------------------------------------------: |\r\n| 更容易水平扩展，因为可以简单地添加更多实例 |          扩展需要考虑状态同步和一致性问题          |\r\n|   无状态服务的失败通常影响较小，容易恢复   | 有状态服务需要额外的机制来确保状态的可靠性和一致性 |\r\n|            不需要维护复杂的状态            |         利用缓存的状态信息，提供更快的响应         |\r\n|         设计更简单，易于理解和维护         |      可能更复杂，需要处理状态管理、同步等问题      |\r\n\r\n在实际应用中，许多系统会综合二者来使用，以平衡各自的优势和劣势。例如：某东App可能使用无状态服务，来提供商品列表，同时使用有状态服务，来构建购物车或用户服务。\r\n\r\n**这样看来，无状态服务可以通过使用`有头Service`来提供，反向代理和负载均衡功能，因为毕竟不需要考虑连接状态；有状态服务可以通过使用`无头Service`，因为不需要，也不能使用反向代理和负载均衡，而是需要直接通过集群中DNS解析，来直接连接到每一个Pod中，保持状态的连接。**\r\n\r\n\r\n\r\n### 写在最后\r\n\r\n​\t本文整体梳理了Service相关概念，由于Service是网络层面的，还需要对容器底层，以及Pod底层访问通信的原理，可能对本文的理解会更容易些。不知不觉又是5000字了，下一篇打算再梳理一篇可观测性的文章吧，目前也是云原生的关键服务，观测性服务市面上也有许多，例如：Skywalking，Jaeger，Zipkin，OpenTelemetry等等，主说OpenTelemetry吧。\r\n\r\n\r\n\r\n### 参考链接\r\n\r\n---\r\n\r\n[1] https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/#defining-a-service\r\n\r\n[2] https://kubernetes.io/zh-cn/docs/reference/networking/virtual-ips"
        },
        {
          "id": "CentOS7失宠，谁又会成为下一个甄嬛!",
          "metadata": {
            "permalink": "/CentOS7失宠，谁又会成为下一个甄嬛!",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/06-CentOS7失宠，谁又会成为下一个甄嬛！.md",
            "source": "@site/blog/06-CentOS7失宠，谁又会成为下一个甄嬛！.md",
            "title": "CentOS7失宠，谁又会成为下一个甄嬛!",
            "description": "CentOS 7.9即将于2024年6月30日到达其生命周期终点（EOL）。这位侍寝公司多年的宠妃也已风光不再。如今，各位技术界的“小主”们，需要赶紧为公司寻一位新的“得力宠妃”来接替她的位置，以保这后宫安稳。",
            "date": "2024-07-01T11:45:00.000Z",
            "formattedDate": "2024年7月1日",
            "tags": [
              {
                "label": "rocky",
                "permalink": "/tags/rocky"
              },
              {
                "label": "linux",
                "permalink": "/tags/linux"
              }
            ],
            "readingTime": 3.8833333333333333,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "CentOS7失宠，谁又会成为下一个甄嬛!",
              "title": "CentOS7失宠，谁又会成为下一个甄嬛!",
              "date": "2024-07-01 11:45",
              "tags": [
                "rocky",
                "linux"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "细梳Service：一文练就融会贯通！",
              "permalink": "/Service"
            },
            "nextItem": {
              "title": "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
              "permalink": "/一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜"
            }
          },
          "content": "<!-- ![logo](/assets/images/rockylinux.jpg) -->\r\n\r\nCentOS 7.9即将于2024年6月30日到达其生命周期终点（EOL）。这位侍寝公司多年的宠妃也已风光不再。如今，各位技术界的“小主”们，需要赶紧为公司寻一位新的“得力宠妃”来接替她的位置，以保这后宫安稳。\r\n\r\n终止的原因：\r\n\r\n1. Red Hat政策变更： 2020年12月，Red Hat宣布将终止CentOS Linux的开发，转而专注于CentOS Stream。这一决定直接影响了CentOS 7的长期支持计划。\r\n<!-- truncate -->\r\n2. 资源重新分配： Red Hat选择将资源集中在CentOS Stream上，这是一个滚动发行版，位于Fedora和RHEL之间，作为RHEL的上游版本(即游戏领域的体验服)。\r\n3. 商业策略调整： 此举旨在鼓励更多用户转向付费的RHEL订阅，或参与到CentOS Stream的开发中来。\r\n\r\n总结就是: RH掀桌子了, 想用就付费！\r\n\r\n\r\n\r\n#### 挑选新系统:\r\n\r\n结合每一个系统的特点, 兼容性, 是否具备迁移方式, 进行筛选, 以下是不错的几个选择: \r\n\r\n| 系统名称                     | 背景                                     | 特点                                                         | 适用场景                                           | 迁移方式                                   |\r\n| ---------------------------- | ---------------------------------------- | ------------------------------------------------------------ | -------------------------------------------------- | ------------------------------------------ |\r\n| Rocky Linux                  | CentOS的精神继承者，由原CentOS创始人领导  | • 100% RHEL兼容• 社区驱动• 多架构支持• 提供容器镜像 | 希望平滑过渡的CentOS用户，企业服务器和数据中心     | migrate2rocky工具，支持就地升级            |\r\n| AlmaLinux                    | 由CloudLinux公司赞助的CentOS替代品       | • RHEL兼容• 提供免费和付费支持• 实时内核版本• 安全增强特性 | 需要企业级支持，对安全性和实时性能有要求的用户     | ELevate项目支持从CentOS 7升级到AlmaLinux 8 |\r\n| Oracle Linux                 | 由Oracle支持和维护                       | • RHEL兼容• Ksplice零停机更新• UEK优化内核• 与Oracle Cloud集成 | 运行Oracle数据库和应用的环境，需要高性能的大型企业 | centos2ol脚本支持就地转换                  |\r\n| Ubuntu LTS                   | 由Canonical开发的通用型Linux发行版       | • 用户友好• 广泛的软件支持• Livepatch内核更新• 强大的云原生支持 | 开发环境、云计算场景、桌面到服务器的各种应用       | 需要重新安装，无直接迁移工具               |\r\n| RHEL                         | Red Hat的旗舰产品，企业级Linux标准       | • 顶级稳定性和支持• Red Hat Insights分析• 全面的安全特性• 与Red Hat生态系统集成 | 大型企业、关键业务系统、需要顶级支持的环境         | Convert2RHEL工具支持直接转换               |\r\n| SUSE Linux Enterprise Server | 面向企业的Linux解决方案                  | • 优秀的SAP支持• YaST配置工具• 长达13年的支持• 实时补丁功能 | 运行SAP的企业环境、需要长期支持的关键业务系统      | 需要重新安装，提供迁移指南和咨询服务       |\r\n| Debian                       | 完全自由的社区驱动发行版                 | • 高度稳定性• 庞大的软件仓库• 强大的包管理系统• 支持多种架构• 严格的自由软件准则 | 服务器环境、嵌入式系统、追求稳定性和自由软件的用户 | 需要重新安装，无直接从CentOS迁移的官方工具 |\r\n\r\n选择新系统就像选择一位长期伙伴,需要考虑多方面因素:\r\n\r\n1. 如果你是前 CentOS 用户，追求平稳过渡：Rocky Linux 或 AlmaLinux 是不错的选择, 兼容最好的还是Rocky。\r\n2. 对 Oracle 生态系统情有独钟：Oracle Linux 可能是你的最佳选择，但兼容性会差一点。\r\n3. 喜欢图形化，易用性和广泛的社区支持：Ubuntu LTS 值得考虑。\r\n4. 预算充足，需要顶级企业支持：RHEL 是公认的行业标准。\r\n5. 运行 SAP 或大型机环境：SUSE Linux Enterprise Server 可能更适合你。\r\n6. 追求自由和灵活，注重稳定性：Debian 会是一个理想的伙伴。它拥有强大的软件包管理系统和活跃的社区，为用户提供了高度的自定义和配置空间。\r\n\r\n\r\n\r\n#### 结语\r\n\r\n国产操作系统这里就不做推荐了, 整体的体验都是换汤不换药, 再多的就不说了。另外还有一点，国内操作系统，每一家都有自己的技术规范，且体验不一定友好，其它的又是其它的规范，无法得到一个统一。不出问题还好，出了问题也只能依赖着系统厂家，在皮肤上面缝缝补补。"
        },
        {
          "id": "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
          "metadata": {
            "permalink": "/一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/05-Kubernetes1.29版本高可用安装.md",
            "source": "@site/blog/05-Kubernetes1.29版本高可用安装.md",
            "title": "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
            "description": "rocky",
            "date": "2024-06-24T19:40:00.000Z",
            "formattedDate": "2024年6月24日",
            "tags": [
              {
                "label": "kubernetes",
                "permalink": "/tags/kubernetes"
              },
              {
                "label": "centos",
                "permalink": "/tags/centos"
              },
              {
                "label": "rocky",
                "permalink": "/tags/rocky"
              }
            ],
            "readingTime": 28.673333333333332,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
              "title": "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
              "date": "2024-06-24 19:40",
              "tags": [
                "kubernetes",
                "centos",
                "rocky"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "CentOS7失宠，谁又会成为下一个甄嬛!",
              "permalink": "/CentOS7失宠，谁又会成为下一个甄嬛!"
            },
            "nextItem": {
              "title": "Rocky Linux 9.3 系统安装",
              "permalink": "/Rocky Linux 9.3 系统安装"
            }
          },
          "content": "<!-- ![logo](/assets/images/docker.png) -->\r\n![rocky](/assets/images/docker.png)\r\n\r\n### 写在前面\r\n\r\n​\t一直想沉淀一篇Kubernetes高可用安装的文章，之前都是参考网上的博客，虽然安装的大致逻辑是有，可具体的细节没有梳理，还是模糊。\r\n\r\n​\t正好前段时间 Kubernetes 10周年际，借此时间，钻研一番。\r\n\r\n​\t本文使用的安装方式是: Kubeadm\r\n\r\n<!-- truncate -->\r\n\r\n## 预备知识\r\n\r\n​\tKubernetes是容器编排工具，即统一管理，控制，调度容器。\r\n\r\n​\tKubernetes整体，由控制面（Control Plan）和数据面（Data Plan）组成。\r\n\r\n​\tKubernetes控制面，是整个集群的核心，大脑，控制面出现问题，瘫痪，Kubernetes会无法正常使用。\r\n\r\n​\tKubernetes控制面，是由多个组件共同协作，完成相对应的工作。组件包括：\r\n\r\n  * Kube-Apiserver: 整个集群的通信入口, 大脑。\r\n\r\n  * Kube-Scheduler：将资源调度分配到数据面。\r\n  * Kube-Controller-Manager：集群资源的控制器。\r\n  * Etcd：整个集群的数据库。\r\n\r\nKubernetes数据面，即每一个node, 是工作节点。每个工作节点上需要部署多个组件, 完成与控制面通信等工作。组件包括：\r\n\r\n  * Kubelet: 用于与控制面通信, 可以理解为是agent代理程序。 \r\n  * Kubelet-Proxy: 用于代理和负载均衡每一个节点上的Pod（应用程序）。\r\n  * Container_Runtime: 容器运行时, 即能够提供Container（容器）运行环境的程序, 或者服务，称之为Container_Runtime, 例如Docker, Containerd。\r\n\r\nCgroup Driver: Ggroup 驱动：Linux内核提供的一种功能，用于限制，控制，隔离进程的资源（内存，cpu等）使用。容器领域的概念中, 需要对每一个容器进行资源的限制, 就需要借助这个驱动实现。而在Kubernetes集群中由Kubelet进行调用Container_Runtime, 再由Container_Runtime去创建容器, 因此：\r\n\r\n  ​\t需要在创建容器运行时服务时, 指定使用的Cgroup Driver类型。（需要和kubelet一致）\r\n\r\n  ​\t需要在创建Kubelet的时候, 指定使用的Cgroup Driver类型。（需要和容器运行时一致）\r\n\r\n  ​\t默认Cgroup Driver的类型有: cgroupfs , systemd。\r\n\r\n！注意：如果Linux系统的init进程是systemd, 那么不推荐使用cgroupfs作为kubelet或者容器运行时的Cgroup Driver, 原因是systemd进程会创建一个cgroupfs drivers, 即systemd, 如果使用cgroupfs, 则系统当中会存在两个cgroup driver, 进而导致systemd cgroup driver的不稳定。\r\n\r\n  ​\t\t在Kubernetes1.22版本, 使用kubeadm安装方式, kubelet 默认使用 systemd 。\r\n\r\n  ​\t\t在Kubernetes1.28版本, kubelet会自动检测匹配容器运行时的cgroup驱动程序。\r\n\r\n\r\n\r\n​那么, 实现Kubernetes的高可用, 本质上是控制面组件的高可用。\r\n\r\n\r\n\r\n## Kubernetes高可用\r\n\r\n​\t高可用, 其实就是当控制面故障时, 整个集群依然能够正常使用的效果。\r\n\r\n​\t因此, 可以对控制面进行多副本创建, 之后再结合VIP, 反向代理, 负载均衡服务, 提供统一的访问入口, 将请求代理到后端的每一个控制面上。实现高可用。\r\n\r\n​\t在这其中, 有一个特殊的组件: Etcd。\r\n\r\n​\tEtcd作为整个Kubernetes集群的数据库, 它的重要性是更不可说的, 但其本身并不是Kubernetes的原生组件, 而是一个单独的开源项目。既然是独立的, 那么就可以进行分布式。\r\n\r\n​\t目前对Etcd的高可用实现方案, 官方提供了两种:\r\n\r\n 1. 叠加式(不推荐): 与Kubernetes控制面叠加部署到一起, 官方架构图如下: \r\n\r\n    ![Stacked etcd topology](https://kubernetes.io/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg)\r\n\r\n    可以看到, 每一个控制面节点上都部署了一个Etcd的实例, 它们之间由共同构建成Etcd的集群。\r\n\r\n    但是这种方式也有一个弊端, 若一个控制面挂掉之后, Etcd也随之不可用。进而提高了故障的代价。因此也是不推荐的。（适用于资源有限的情况）\r\n\r\n 2. 外部式: 外部搭建独立的Etcd集群, Kubernetes直接远程连接使用。官方架构图如下: \r\n\r\n    ![External etcd topology](https://kubernetes.io/images/kubeadm/kubeadm-ha-topology-external-etcd.svg)\r\n\r\n​\t这种情况下, 若控制面挂掉, 也不会影响其访问的Etcd, 提高了容错, 降低了故障成本。维护起来也方便。\r\n\r\n\r\n\r\n## 反向代理, 负载均衡\r\n\r\n​\t反向代理, 即将客户端发送过来的请求转发给后端的服务器, 起到保护后端服务器的目的。\r\n\r\n​\t负载均衡, 即将客户端发送过来的请求根据相应的规则, 算法, 应该怎样给到后端的服务器, 起到减轻单台服务器压力的目的。\r\n\r\n​\t目前市面上实现这两种功能的服务有很多, 比如nginx, keepalived, haproxy, lvs，load balancer等等。\r\n\r\n​\t本文使用keepalived + haproxy的方式进行实现。\r\n\r\n​\t碍于资源, 本文的架构使用叠加式的方式部署。但安装方法, 步骤都是通用的。\r\n\r\n\r\n\r\n## 安装准备\r\n\r\n架构设计: \r\n\r\n| 节点名   | IP地址         | 描述   |\r\n| -------- | -------------- | ------ |\r\n| k8s-mn01 | 192.168.10.11  | 控制面 |\r\n| k8s-mn02 | 192.168.10.22  | 控制面 |\r\n| k8s-mn03 | 192.168.10.33  | 控制面 |\r\n| k8s-wn01 | 192.168.10.100 | 数据面 |\r\n| k8s-wn02 | 192.168.10.200 | 数据面 |\r\n\r\n为了方便Kubernetes集群的后续使用, 减少问题出现的频率, 需要在安装之前做准备。\r\n\r\n需要考虑的因素 (每台节点都需要操作)：\r\n\r\n 1. 操作系统的选用: **本文安装选用Rocky9.3**。当然也可以选择开源明星: Centos系列。\r\n\r\n    **不管是Rocky, 还是Centos, 本文的安装方法都是通用的。**\r\n\r\n 2. 每台机器的资源分配：内存，CPU，磁盘\r\n\r\n    * 内存: 根据业务需求，分配每个节点的内存。64G，128G，512G等。**测试学习不低于2G**。\r\n\r\n    * CPU：根据业务需求，分配每个节点的CPU核数。8核，16核，32核，64核，128核等。\r\n\r\n      ​\t **测试学习不低于2核**。\r\n\r\n    * 磁盘：根据业务需求，分配每个节点的磁盘容量。1T，2T，nT等, **测试学习不低于50G**。\r\n\r\n      在磁盘划分时, 需要注意以下路径, 最好能给一块单独的空间, 且是LVM卷(方便扩容)。\r\n\r\n      | 路径                               | 容量       | 描述                                                         | 备注        |\r\n      | ---------------------------------- | ---------- | ------------------------------------------------------------ | ----------- |\r\n      | /var/lib/docker 或 /var/lib/docker | 200G--不限 | 容器存储路径, 根据需求自定义。                               | 建议LVM卷   |\r\n      | /var/lib/etcd                      | 50G--不限  | Etcd存储路径，根据需求自定义, 使用固态磁盘，受限于磁盘速度, 故速度越快越好。 | 必须SSD磁盘 |\r\n\r\n​\t\t当然, 你也完全可以给/分配足够大的磁盘空间。\r\n\r\n2. 内核版本: 5.*, 需要将操作系统内核更新到最新的稳定版。(如果内核版本过低, 则Kubernetes需要调用, 使用内核提供的模块, 参数没有, 出现问题)\r\n\r\n   ```shell\r\n   # 查看内核版本\r\n   uname -r\r\n   5.14.0-362.8.1.el9_3.x86_64\r\n   ```\r\n\r\n3. 节点之间的网络是需要流畅的, 统一的。\r\n\r\n4. 节点之间的主机名是唯一的。\r\n\r\n   ```shell\r\n   # 配置节点的主机名 (自定义)\r\n   # 节点1执行: \r\n   hostnamectl set-hostname k8s-mn01\r\n   # 节点2执行: \r\n   hostnamectl set-hostname k8s-mn02\r\n   # 节点3执行\r\n   hostnamectl set-hostname k8s-mn03\r\n   # 节点4执行\r\n   hostnamectl set-hostname k8s-wn01\r\n   # 节点5执行\r\n   hostnamectl set-hostname k8s-wn02\r\n   \r\n   # 添加解析记录, 使节点直接也可以使用主机名进行访问通信\r\n   # 每台节点执行\r\n   cat << EOF >> /etc/hosts\r\n   192.168.10.11 k8s-mn01\r\n   192.168.10.22 k8s-mn02\r\n   192.168.10.33 k8s-mn03\r\n   192.168.10.100 k8s-wn01\r\n   192.168.10.200 k8s-wn02\r\n   EOF\r\n   ```\r\n\r\n5. 节点之间能够互相免密登录\r\n\r\n   ```shell\r\n   # 每个节点执行\r\n   # 生成密钥对文件, 传输公钥到目标节点\r\n   ssh-keygen -t rsa -b 2048 \r\n   一路回车\r\n   \r\n   ssh-copy-id root@目标节点IP\r\n   ```\r\n\r\n6. 节点的软件仓库源.repo, 是可用的。\r\n\r\n   ```shell\r\n   # 每个节点执行\r\n   # 替换成为阿里源\r\n   sed -e 's|^mirrorlist=|#mirrorlist=|g' \\\r\n       -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\\r\n       -i.bak \\\r\n       /etc/yum.repos.d/[Rr]ocky*.repo\r\n       \r\n   # 安装所需软件包\r\n   dnf -y install ipvsadm\r\n   ```\r\n\r\n7. Firewalld, Selinux等安全机制关闭\r\n\r\n   ```shell\r\n   # 每台节点执行\r\n   systemctl stop firewalld\r\n   systemctl disable firewalld\r\n   \r\n   setenforce 0\r\n   sed -i \"/SELINUX/ s/enforcing/disabled/g\" /etc/selinux/config\r\n   ```\r\n\r\n8. 保证端口: 6443；2379-2380；10250；10259；10257；30000-32767  未被占用, 可被监听。\r\n\r\n9. 禁用Swap分区\r\n\r\n   ```shell\r\n   # 每台节点执行\r\n   vim /etc/fstab\r\n   #/dev/mapper/rl-swap     none                    swap    defaults        0 0\r\n   \r\n   swapoff -a\r\n   ```\r\n\r\n10. 节点之间的时间是统一的\r\n\r\n    ```shell\r\n    # 批注：由于RHEL8之后，官方就弃用了ntpdate软件包命令，而是建议使用chrony服务，而Rocky对标的是RHEL9，也是同样。(Centos系列完全可以使用该配置, 以下的dnf等同于yum, 所以以下的dnf命令均可以替换为yum)\r\n    \r\n    # 安装，配置chrony服务\r\n    $ dnf -y install chrony\r\n    $ vim /etc/chrony.conf\r\n      1 # Use public servers from the pool.ntp.org project.\r\n      2 # Please consider joining the pool (https://www.pool.ntp.org/join.html).\r\n      3 #pool 2.rocky.pool.ntp.org iburst\r\n      4 pool ntp.aliyun.com iburst\r\n    \r\n     26 # Allow NTP client access from local network.\r\n     27 allow 192.168.10.0/24\r\n     \r\n    $ systemctl start chronyd\r\n    $ systemctl enable chronyd\r\n    $ chronyc sources\r\n    MS Name/IP address         Stratum Poll Reach LastRx Last sample               \r\n    ===============================================================================\r\n    ^? 203.107.6.88                  2   6     3     2    -65ms[  -65ms] +/-   38ms\r\n    $ date\r\n    Mon Jun 10 21:21:37 CST 2024\r\n    \r\n    # 批注：上方修改chrony.conf中两处的含义是；\r\n    # pool ntp.aliyun.com iburst: 指定使用的上游时间服务器地址\r\n    # allow 192.168.10.0/24: 允许192.168.10.0/24网段的所有机器连接自己来同步时间\r\n    \r\n    # timedatectl查看时区是否为Asia/Shanghai\r\n    $ timedatectl\r\n                   Local time: Mon 2024-06-10 21:35:43 CST\r\n               Universal time: Mon 2024-06-10 13:35:43 UTC\r\n                     RTC time: Mon 2024-06-10 13:35:43\r\n                    Time zone: Asia/Shanghai (CST, +0800)\r\n    System clock synchronized: yes\r\n                  NTP service: active\r\n              RTC in local TZ: no\r\n    # 若时区不是Asia/Shanghai，则使用该命令修改\r\n    $ timedatectl set-timezone Asia/Shanghai\r\n    \r\n    # 其它节点只需要与该时间服务器同步即可, 即修改/etc/chrony.conf文件, 指定时间服务器地址: pool 时间服务器地址 iburst\r\n    ```\r\n\r\n11. 内核进行优化\r\n\r\n​\t注:通过配置sysctl.conf文件，对内核优化，优化方面有：系统方面，用户方面，容器方面。\r\n\r\n​\t优化的目的：提高系统的性能，服务运行需要。若不对其进行优化，在前期运行可能没有问    \t\t\t\t题，但在后期会因为一些内核参数的值限制，导致系统，服务的运行不稳定。\r\n\r\n​\t若不优化, 你在后面维护Kubernetes过程中, 大概率会遇到 Too many open files 等报错。\r\n\r\n​\t以下参数中: net.bridge.bridge-nf-call-iptables  = 1\r\n​                net.bridge.bridge-nf-call-ip6tables = 1\r\n​                net.ipv4.ip_forward                 = 1\r\n\r\n​\t是Kubernetes集群安装, 运行所需要的。\r\n\r\n```shell\r\n$ vi /etc/sysctl.conf \r\nfs.file-max = 202808\r\nnet.core.netdev_max_backlog = 262144\r\nnet.core.somaxconn = 262144\r\nnet.ipv4.tcp_max_orphans = 262144\r\nnet.ipv4.tcp_max_syn_backlog = 262144\r\nnet.ipv4.tcp_synack_retries = 1\r\nnet.ipv4.tcp_syn_retries = 1\r\nnet.ipv4.ip_local_port_range = 15000 65000\r\nnet.ipv4.tcp_keepalive_intvl = 60\r\nnet.ipv4.tcp_keepalive_probes = 3\r\nnet.ipv4.tcp_keepalive_time = 1500\r\nnet.ipv4.tcp_syncookies = 1\r\nnet.ipv4.tcp_fin_timeout = 30\r\nnet.ipv4.tcp_max_tw_buckets = 6000\r\nnet.ipv4.tcp_timestamps = 0\r\nnet.ipv4.tcp_timestamps = 0\r\nnet.ipv4.tcp_tw_reuse = 1\r\nnet.ipv4.tcp_timestamps = 1\r\nnet.core.rmem_default = 6291456\r\nnet.core.wmem_default = 6291456\r\nnet.core.rmem_max = 12582912\r\nnet.core.wmem_max = 12582912\r\nnet.ipv4.tcp_rmem = 10240 87380 12582912\r\nnet.ipv4.tcp_wmem = 10240 87380 12582912\r\nnet.ipv4.tcp_keepalive_time=600\r\nnet.ipv4.tcp_keepalive_intvl=30 \r\nnet.ipv4.tcp_keepalive_probes=10  \r\nnet.ipv6.conf.all.disable_ipv6=1 \r\nnet.ipv6.conf.default.disable_ipv6=1 \r\nnet.ipv6.conf.lo.disable_ipv6=1 \r\nnet.ipv4.neigh.default.gc_stale_time=120 \r\nnet.ipv4.conf.all.rp_filter=0  \r\nnet.ipv4.conf.default.rp_filter=0\r\nnet.ipv4.conf.default.arp_announce=2\r\nnet.ipv4.conf.lo.arp_announce=2\r\nnet.ipv4.conf.all.arp_announce=2\r\nnet.ipv4.ip_local_port_range= 45001 65000\r\nnet.ipv4.ip_forward=1\r\nnet.ipv4.tcp_max_tw_buckets=6000\r\nnet.ipv4.tcp_syncookies=1\r\nnet.ipv4.tcp_synack_retries=2\r\nnet.bridge.bridge-nf-call-ip6tables=1\r\nnet.bridge.bridge-nf-call-iptables=1 \r\nnet.netfilter.nf_conntrack_max=2310720 \r\nnet.ipv6.neigh.default.gc_thresh1=8192\r\nnet.ipv6.neigh.default.gc_thresh2=32768\r\nnet.ipv6.neigh.default.gc_thresh3=65536\r\nnet.core.netdev_max_backlog=16384    \r\nnet.core.rmem_max=16777216         \r\nnet.core.wmem_max=16777216         \r\nnet.ipv4.tcp_max_syn_backlog=8096  \r\nnet.core.somaxconn = 32768           \r\nfs.inotify.max_user_instances=8192   \r\nfs.inotify.max_user_watches=524288   。\r\nfs.file-max=52706963                \r\nfs.nr_open=52706963                  \r\nkernel.pid_max=4194303             \r\nnet.bridge.bridge-nf-call-arptables=1 \r\nvm.swappiness=0                       \r\nvm.overcommit_memory=1                \r\nvm.panic_on_oom=0                     \r\nvm.max_map_count=262144\r\n\r\n# 加载上述内核参数生效所需要的模块，并加载生效\r\nsudo modprobe overlay\r\nsudo modprobe br_netfilter\r\nsysctl -p\r\n```\r\n\r\n\r\n\r\n内核参数说明 (以下顺序不分先后): \r\n\r\n| 内核参数                             | 含义                                                 |\r\n| ------------------------------------ | ---------------------------------------------------- |\r\n| fs.file-max                          | 系统可以分配的最大文件句柄（或打开文件）数量。       |\r\n| net.core.netdev_max_backlog          | 内核可以为每个网络设备内部排队的最大数据包数量。     |\r\n| net.core.somaxconn                   | 可以在监听套接字排队中排队的最大连接数。             |\r\n| net.ipv4.tcp_max_orphans             | 内核开始丢弃连接之前允许的最大孤立套接字数量。       |\r\n| net.ipv4.tcp_max_syn_backlog         | 等待被接受的不完整连接的最大数量。                   |\r\n| net.ipv4.tcp_synack_retries          | 在放弃由远程端点发起的TCP连接尝试之前的重试次数。    |\r\n| net.ipv4.tcp_syn_retries             | 在放弃本地发起的TCP连接尝试之前的重试次数。          |\r\n| net.ipv4.ip_local_port_range         | 可用于传出连接的本地端口范围。                       |\r\n| net.ipv4.tcp_keepalive_intvl         | 连续TCP保活探测之间的时间间隔。                      |\r\n| net.ipv4.tcp_keepalive_probes        | 在考虑连接已死亡之前发送的TCP保活探测数量。          |\r\n| net.ipv4.tcp_keepalive_time          | 最后发送的数据包与第一个TCP保活探测之间的时间间隔。  |\r\n| net.ipv4.tcp_syncookies              | 启用TCP SYN cookies以防止SYN洪水攻击。               |\r\n| net.ipv4.tcp_fin_timeout             | 在FIN-WAIT-2状态下强制关闭TCP连接之前等待的时间。    |\r\n| net.ipv4.tcp_max_tw_buckets          | 内核可以维护的TIME_WAIT套接字的最大数量。            |\r\n| net.ipv4.tcp_timestamps              | 启用或禁用TCP时间戳以防止某些攻击。                  |\r\n| net.core.rmem_default                | 所有网络连接的接收缓冲区的默认大小。                 |\r\n| net.core.wmem_default                | 所有网络连接的发送缓冲区的默认大小。                 |\r\n| net.core.rmem_max                    | 所有网络连接的接收缓冲区的最大大小。                 |\r\n| net.core.wmem_max                    | 所有网络连接的发送缓冲区的最大大小。                 |\r\n| net.ipv4.tcp_rmem                    | TCP连接的接收缓冲区的最小、默认和最大大小。          |\r\n| net.ipv4.tcp_wmem                    | TCP连接的发送缓冲区的最小、默认和最大大小。          |\r\n| net.ipv4.tcp_tw_reuse                | 允许对新连接重用TIME_WAIT套接字。                    |\r\n| fs.inotify.max_user_instances        | 每个用户的最大inotify实例数。                        |\r\n| fs.inotify.max_user_watches          | 每个用户允许的最大监视数。                           |\r\n| fs.nr_open                           | 进程可以分配的最大文件描述符数量。                   |\r\n| kernel.pid_max                       | 进程ID号可以设置的最大值。                           |\r\n| net.bridge.bridge-nf-call-arptables  | 启用或禁用桥接流量的ARP表过滤。                      |\r\n| vm.swappiness                        | 控制在运行时内存和将应用程序数据缓存到内存中的平衡。 |\r\n| vm.overcommit_memory                 | 控制系统内存的过度承诺。                             |\r\n| vm.panic_on_oom                      | 内核在发生内存不足错误时的行为。                     |\r\n| vm.max_map_count                     | 进程可能具有的内存映射区域的最大数量。               |\r\n| net.ipv6.conf.all.disable_ipv6       | 禁用IPv6协议的配置参数。                             |\r\n| net.ipv6.conf.default.disable_ipv6   | 禁用默认的IPv6协议配置参数。                         |\r\n| net.ipv6.conf.lo.disable_ipv6        | 禁用本地回环接口的IPv6协议配置参数。                 |\r\n| net.ipv4.neigh.default.gc_stale_time | 邻居条目被认为过时的时间。                           |\r\n| net.ipv4.conf.all.rp_filter          | 启用或禁用反向路径过滤。                             |\r\n| net.ipv4.conf.default.rp_filter      | 启用或禁用默认反向路径过滤。                         |\r\n| net.ipv4.conf.default.arp_announce   | 发送ARP请求时使用的源地址类型。                      |\r\n| net.ipv4.conf.lo.arp_announce        | 发送ARP请求时使用的源地址类型。                      |\r\n| net.ipv4.conf.all.arp_announce       | 发送ARP请求时使用的源地址类型。                      |\r\n| net.ipv4.ip_forward                  | 启用或禁用IP转发。                                   |\r\n| net.ipv4.ip_local_port_range         | 可用于传出连接的本地端口范围。                       |\r\n| net.bridge.bridge-nf-call-iptables   | 启用或禁用桥接流量的iptables过滤。                   |\r\n| net.netfilter.nf_conntrack_max       | 连接跟踪表的最大条目数。                             |\r\n| net.ipv6.neigh.default.gc_thresh1    | 邻居缓存的最小条目数量。                             |\r\n| net.ipv6.neigh.default.gc_thresh2    | 邻居缓存的良好条目数量。                             |\r\n| net.ipv6.neigh.default.gc_thresh3    | 邻居缓存的最大条目数量。                             |\r\n| net.core.netdev_max_backlog          | 内核可以为每个网络设备内部排队的最大数据包数量。     |\r\n| net.core.rmem_max                    | 所有网络连接的接收缓冲区的最大大小。                 |\r\n| net.core.wmem_max                    | 所有网络连接的发送缓冲区的最大大小。                 |\r\n| net.ipv4.tcp_max_syn_backlog         | 等待被接受的不完整连接的最大数量。                   |\r\n| net.core.somaxconn                   | 可以在监听套接字排队中排队的最大连接数。             |\r\n| fs.inotify.max_user_instances        | 每个用户的最大inotify实例数。                        |\r\n| fs.inotify.max_user_watches          | 每个用户允许的最大监视数。                           |\r\n| fs.file-max                          | 系统可以分配的最大文件句柄（或打开文件）数量。       |\r\n| fs.nr_open                           | 进程可以分配的最大文件描述符数量。                   |\r\n| kernel.pid_max                       | 进程ID号可以设置的最大值。                           |\r\n| net.bridge.bridge-nf-call-arptables  | 启用或禁用桥接流量的ARP表过滤。                      |\r\n| vm.swappiness                        | 控制在运行时内存和将应用程序数据缓存到内存中的平衡。 |\r\n| vm.overcommit_memory                 | 控制系统内存的过度承诺。                             |\r\n| vm.panic_on_oom                      | 内核在发生内存不足错误时的行为。                     |\r\n| vm.max_map_count                     | 进程可能具有的内存映射区域的最大数量。               |\r\n\r\n​\t\r\n\r\n## 运行时安装\r\n\r\n目前Kubernetes官方推荐使用的是Containerd, 当然Docker也可以使用, 只需要添加一个shim垫片(与Kubernetes集群连接使用)。之间的原因: 懂得都懂。\r\n\r\n本文选择的是Containerd (每台节点都需要操作)\r\n\r\n1. 安装指定版本的Containerd.tar.gz\r\n\r\n   ```shell\r\n   # 因版本过多, 得到一个稳定, 推崇的, 并不太容易。\r\n   # 借鉴于AliYun, 因其云计算的场景, 方案也成熟。\r\n   # 本次使用的是1.6.33\r\n   # 下载地址: \r\n   https://github.com/containerd/containerd/releases/download/v1.6.33/containerd-1.6.33-linux-amd64.tar.gz\r\n   \r\n   # 每台节点执行\r\n   $ tar Cxzvf /usr/local containerd-1.6.33-linux-amd64.tar.gz \r\n   bin/\r\n   bin/containerd-shim\r\n   bin/containerd-stress\r\n   bin/ctr\r\n   bin/containerd-shim-runc-v2\r\n   bin/containerd-shim-runc-v1\r\n   bin/containerd\r\n   \r\n   # 使用官方提供的service文件：https://raw.githubusercontent.com/containerd/containerd/main/containerd.service\r\n   \r\n   # 必须是/usr/local/lib/systemd/system路径, 否则会找不到\r\n   mkdir -p /usr/local/lib/systemd/system\r\n   vim /usr/local/lib/systemd/system/containerd.service\r\n   # Copyright The containerd Authors.\r\n   #\r\n   # Licensed under the Apache License, Version 2.0 (the \"License\");\r\n   # you may not use this file except in compliance with the License.\r\n   # You may obtain a copy of the License at\r\n   #\r\n   #     http://www.apache.org/licenses/LICENSE-2.0\r\n   #\r\n   # Unless required by applicable law or agreed to in writing, software\r\n   # distributed under the License is distributed on an \"AS IS\" BASIS,\r\n   # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n   # See the License for the specific language governing permissions and\r\n   # limitations under the License.\r\n   \r\n   [Unit]\r\n   Description=containerd container runtime\r\n   Documentation=https://containerd.io\r\n   After=network.target local-fs.target\r\n   \r\n   [Service]\r\n   ExecStartPre=-/sbin/modprobe overlay\r\n   ExecStart=/usr/local/bin/containerd\r\n   \r\n   Type=notify\r\n   Delegate=yes\r\n   KillMode=process\r\n   Restart=always\r\n   RestartSec=5\r\n   \r\n   # Having non-zero Limit*s causes performance problems due to accounting overhead\r\n   # in the kernel. We recommend using cgroups to do container-local accounting.\r\n   LimitNPROC=infinity\r\n   LimitCORE=infinity\r\n   \r\n   # Comment TasksMax if your systemd version does not supports it.\r\n   # Only systemd 226 and above support this version.\r\n   TasksMax=infinity\r\n   OOMScoreAdjust=-999\r\n   \r\n   [Install]\r\n   WantedBy=multi-user.target\r\n   \r\n   systemctl daemon-reload \r\n   systemctl start containerd\r\n   systemctl enable containerd\r\n   ```\r\n\r\n2. 安装指定版本的Runc\r\n\r\n   Runc: 是真正创建, 运行容器的程序, Containerd服务去创建容器时, 本身是去调用Runc程序来进行容器的创建。\r\n\r\n   ```shell\r\n   # 本次下载的版本是: Runc 1.1.12\r\n   # 下载地址\r\n   https://github.com/opencontainers/runc/releases/download/v1.1.12/runc.amd64\r\n   \r\n   # 安装即可\r\n   install -m 755 runc.amd64 /usr/local/sbin/runc\r\n   ```\r\n\r\n3. 安装指定版本的网络插件CNI (这一步可以不做)\r\n\r\n   CNI: Container Network Interface: 容器网络接口, 实现容器间的访问通信的, 比如Ping。\r\n\r\n   这一步骤可以不做，因为containerd的cni插件解决的是容器间的访问通信，但是在安装kubernetes的同时，也会安装kubernetes所需要的cni插件：flannel 或者 calico。作用都是一致。\r\n\r\n   ```shell\r\n   # 下载地址\r\n   https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz\r\n   \r\n   # 安装\r\n   $ mkdir -p /opt/cni/bin\r\n   $ tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.1.1.tgz\r\n   ```\r\n\r\n4. Containerd配置文件的生成与修改\r\n\r\n   默认的Containerd配置文件中定义了使用了Cgroup Driver的类型, 镜像拉取的地址等, 需要进行修改成为正确的, 适合的。\r\n\r\n   ```shell\r\n   # 生成Containerd配置文件\r\n   mkdir /etc/containerd\r\n   containerd config default > /etc/containerd/config.toml\r\n   \r\n   # 修改Containerd使用的cgroup为systemd cgroup driver\r\n   $ vim /etc/containerd/config.toml\r\n   [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\r\n     ...\r\n     [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\r\n       SystemdCgroup = true\r\n   \r\n   # 修改Containerd使用的sandbox_image\r\n   [root@localhost ~]# vim /etc/containerd/config.toml\r\n   sandbox_image = \"registry.aliyuncs.com/google_containers/pause:3.9\"\r\n   \r\n   # 配置Containerd镜像加速【默认文件定义的镜像下载地址都是国外，访问不了，需要修改成国内代理】\r\n   # containerd官方推荐的方式如下：\r\n   $ vim /etc/containerd/config.toml\r\n   146     [plugins.\"io.containerd.grpc.v1.cri\".registry]\r\n   147       config_path = \"/etc/containerd/certs.d\"\r\n   148 \r\n   149       [plugins.\"io.containerd.grpc.v1.cri\".registry.auths]\r\n   150 \r\n   151       [plugins.\"io.containerd.grpc.v1.cri\".registry.configs]\r\n   152 \r\n   153       [plugins.\"io.containerd.grpc.v1.cri\".registry.headers]\r\n   154 \r\n   155       [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors]\r\n   \r\n   mkdir /etc/containerd/certs.d/docker.io -p\r\n   cat > /etc/containerd/certs.d/docker.io/hosts.toml << EOF\r\n   server = \"https://docker.io\"\r\n   [host.\"https://i3h3dbjr.mirror.aliyuncs.com\"]\r\n     capabilities = [\"pull\", \"resolve\"]\r\n   EOF\r\n   \r\n   systemctl restart containerd\r\n   ```\r\n\r\n5. BuildKit安装\r\n\r\n   若想要进行镜像构建, 则需要借助BuildKit, 否则构建会报错\r\n\r\n   ```shell\r\n   # 下载安装包\r\n   wget https://github.com/moby/buildkit/releases/download/v0.11.6/buildkit-v0.11.6.linux-amd64.tar.gz\r\n   \r\n   # 解压\r\n   tar -zxvf buildkit-v0.11.6.linux-amd64.tar.gz\r\n   \r\n   # 安装\r\n   cp -a bin/* /usr/local/bin/\r\n   > buildctl -version\r\n   buildctl github.com/moby/buildkit v0.11.6 2951a28cd7085eb18979b1f710678623d94ed578\r\n   \r\n   # 配置systemd管理\r\n   vi /usr/lib/systemd/system/buildkitd.service\r\n   [Unit]\r\n   Description=/usr/local/bin/buildkitd\r\n   ConditionPathExists=/usr/local/bin/buildkitd\r\n   After=containerd.service\r\n   \r\n   [Service]\r\n   Type=simple\r\n   ExecStart=/usr/local/bin/buildkitd\r\n   User=root\r\n   Restart=on-failure\r\n   RestartSec=1500ms\r\n   \r\n   [Install]\r\n   WantedBy=multi-user.target\r\n   \r\n   # 启动\r\n   systemctl daemon-reload && systemctl start buildkitd && systemctl enable buildkitd\r\n   ```\r\n\r\n6. 命令行工具安装\r\n\r\n   Containerd安装部署起来之后，可以进行pull，push镜像，start，stop容器等相关操作。\r\n\r\n   ​\tContainerd默认提供的命令行工具是ctr，但是这个命令使用起来确实不太方便，很多之前Docker的命令选项都没有。\r\n\r\n   ​\tnerdctl命令工具应运而生。是一个类似于Docker CLI的命令工具，用于管理和运行容器，它提供与Docker兼容的接口，并支持和Containerd集成。\r\n\r\n   ​\t也就是说, 之前使用Docker的操作, 例如docker run; docker build; docker load等, 只需将docker换成nerdctl即可。更多使用见: [1]\r\n\r\n   ```shell\r\n   # 下载安装包\r\n   wget https://github.com/containerd/nerdctl/releases/download/v1.4.0/nerdctl-1.4.0-linux-amd64.tar.gz\r\n   \r\n   # 解压\r\n   mkdir /root/nerdctl\r\n   tar -zxvf nerdctl-1.4.0-linux-amd64.tar.gz -C /root/nerdctl\r\n   cd /root/nerdctl && ls\r\n   containerd-rootless-setuptool.sh containerd-rootless.sh nerdctl\r\n   \r\n   # 安装\r\n   cp -a nerdctl /usr/bin/nerdctl\r\n   > nerdctl --version\r\n   nerdctl version 1.4.0\r\n   \r\n   # 创建配置文件\r\n   mkdir -p /etc/nerdctl\r\n   $ vi /etc/nerdctl/nerdctl.toml\r\n   namespace = \"k8s.io\"\r\n   debug = false\r\n   debug_full = false\r\n   insecure_registry = true\r\n   \r\n   # 上方配置的namespace是需要指定成为k8s.io, 默认为default\r\n   # 该namespace的作用是一种隔离机制，用于将系统资源（如进程、文件系统、网络）对不同实体进行隔离，使它们在各自的环境中运行，互不干扰。\r\n   # 若不指定为k8s.io, 则后续在安装kubernetes过程中, 都需要指定命名空间, 很不方便。\r\n   ```\r\n\r\n   \r\n\r\n## Kubeadm,Kubectl,Kubelet安装\r\n\r\n* kubeadm：kubernetes集群部署工具。\r\n* kubelet：运行在集群中的每一个节点上，用于启动 pod 和 容器。\r\n* kubectl：kubernetes命令行工具，用于与kubernetes集群进行交互，例如创建pod，查看集群状态等。\r\n\r\n所以需要在每一个控制面上安装kubelet, kubectl, 每一个数据面上安装kubelet。\r\n\r\nkubeadm只需要安装在一台控制面主机上即可。但是为了方便下载镜像, 本文在每个节点都安装。\r\n\r\n版本选择：\r\n\r\n​\tkubeadm的版本是1.29，则安装的的kubernetes集群版本（或者说kubernetes核心组件）肯定是1.29 或 1.28。\r\n\r\n​\tkubelet的版本必须小于kube-apiserver的版本，一般kubelet的版本选用小于等于3个kube-apiserver版本。（例如：kube-apiserver版本本次安装的是**1.29**，那么kubelet版本支持 **1.29**, **1.28**, **1.27**, 和 **1.26**。）\r\n\r\n​\tkubectl的版本只允许在1个小版本的偏差与kube-apiserver的版本。（例如：kube-apiserver版本本次安装的是1.29，那么kubectl版本支持 **1.30**, **1.29**, 和**1.28**。）\r\n\r\n更多的版本偏差，查阅官网：[2]\r\n\r\n安装步骤 ( 控制面安装Kubeadm, Kubectl, Kubelet; 数据面安装Kubelet ): \r\n\r\n```shell\r\n# Aliyun Kubernetes Repo源 配置. 因官网的仓库地址国内访问不到, 使用阿里云提供的仓库, 内容都是一样的. \r\n# 三台机器相同配置: \r\ncat <<EOF | tee /etc/yum.repos.d/kubernetes.repo\r\n[kubernetes]\r\nname=Kubernetes\r\nbaseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.29/rpm/\r\nenabled=1\r\ngpgcheck=1\r\ngpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.29/rpm/repodata/repomd.xml.key\r\nEOF\r\n\r\ndnf makecache\r\n\r\n# 列出kubeadm等版本信息\r\n[root@k8s-mn01 ~]# dnf --showduplicates list kubeadm | grep x86\r\nkubeadm.x86_64                    1.29.0-150500.1.1                   kubernetes\r\nkubeadm.x86_64                    1.29.1-150500.1.1                   kubernetes\r\nkubeadm.x86_64                    1.29.2-150500.1.1                   kubernetes\r\nkubeadm.x86_64                    1.29.3-150500.1.1                   kubernetes\r\nkubeadm.x86_64                    1.29.4-150500.2.1                   kubernetes\r\nkubeadm.x86_64                    1.29.5-150500.1.1                   kubernetes\r\nkubeadm.x86_64                    1.29.6-150500.1.1                   kubernetes\r\n\r\n#安装kubeadm, kubelet, kubectl 均为1.29.6-150500.1.1版本\r\ndnf -y install kubeadm-1.29.6 kubectl-1.29.6 kubelet-1.29.6\r\n\r\nsystemctl enable kubelet && systemctl start kubelet\r\n```\r\n\r\n\r\n\r\n## 高可用安装\r\n\r\nKeepalived + Haproxy是长久以来被人熟知, 经过验证的高可用方案。\r\n\r\n前提: 需要申请一个未被使用过的IP, 作为虚拟IP (VIP)。\r\n\r\n只需在控制面节点上操作: \r\n\r\n```shell\r\n# 每个节点安装keepalived, haproxy\r\ndnf -y install keepalived haproxy\r\n\r\n# 创建keepalived.conf\r\n$ /etc/keepalived/keepalived.conf\r\n! Configuration File for keepalived\r\nglobal_defs {\r\n    router_id LVS_DEVEL\r\n}\r\n\r\n# 指定检测脚本: \r\n# script: 脚本路径; interval: 脚本执行时间; weight: 权重; fall: 连续检测失败多少次之后认定节点不可用; rise: 连续检测成功多少次认为节点恢复正常。\r\nvrrp_script check_apiserver {\r\n  script \"/etc/keepalived/check_apiserver.sh\"\r\n  interval 3\r\n  weight -2\r\n  fall 10\r\n  rise 2\r\n}\r\n\r\n# state: 指定MASTER身份, 另外两台Keepalived设置成BACKUP\r\n# interface: 指定网卡; \r\n# virtual_router_id: VRRP虚拟路由id, 同一集群的Keepalived节点要相同, 用来识别彼此\r\n# priority: 优先级, 另外两台Keepalived分别设置成90 70\r\n# auth_type: VRRP组节点之间认证方式为PASS铭文\r\n# auth_pass: VRRP组节点之间用来认证通信的密码\r\n# virtual_ipaddress: VIP\r\n# track_script: 指定使用的检测脚本名称\r\n\r\nvrrp_instance VI_1 {\r\n    state MASTER\r\n    interface ens33\r\n    virtual_router_id 51\r\n    priority 100\r\n    authentication {\r\n        auth_type PASS\r\n        auth_pass 1111\r\n    }\r\n    virtual_ipaddress {\r\n        192.168.10.240\r\n    }\r\n    track_script {\r\n        check_apiserver\r\n    }\r\n}\r\n\r\n# 创建检测脚本\r\n# 该脚本的逻辑是: 检测本地的8443端口(haproxy服务)是否正常, 若不正常, 则停止本地的Keepalived服务, VIP飘逸到其它haproxy可用的节点, 继续提供服务。\r\n$ vi /etc/keepalived/check_apiserver.sh\r\n#!/bin/sh\r\n\r\ncurl -sfk --max-time 2 https://localhost:8443/healthz -o /dev/null \r\nif [ $? -nq 0]\r\nthen\r\n        echo \"*** Error GET https://localhost:8443/healthz\" 1>&2\r\n        systemctl stop keepalived\r\nfi\r\n# 给脚本文件执行权限\r\nchmod +x /etc/keepalived/check_apiserver.sh \r\n\r\n# 创建haproxy.cfg\r\n$ vi /etc/haproxy/haproxy.cfg\r\n#---------------------------------------------------------------------\r\n# Global settings\r\n#---------------------------------------------------------------------\r\nglobal\r\n    log stdout format raw local0\r\n    chroot      /var/lib/haproxy\r\n    pidfile     /var/run/haproxy.pid\r\n    maxconn     4000\r\n    user        haproxy\r\n    group       haproxy\r\n    daemon\r\n\r\n#---------------------------------------------------------------------\r\n# common defaults that all the 'listen' and 'backend' sections will\r\n# use if not designated in their block\r\n#---------------------------------------------------------------------\r\ndefaults\r\n    log                     global\r\n    option                  httplog\r\n    option                  dontlognull\r\n    option                  forwardfor    except 127.0.0.0/8\r\n    timeout connect         5s\r\n    timeout client          35s\r\n    timeout server          35s\r\n\r\n#---------------------------------------------------------------------\r\n# apiserver frontend which proxys to the control plane nodes\r\n#---------------------------------------------------------------------\r\n# 主要是这里的bind: 定义haproxy的代理端口为8443。也可以是其它。\r\nfrontend apiserver\r\n    bind *:8443\r\n    mode tcp\r\n    option tcplog\r\n    default_backend apiserverbackend\r\n\r\n#---------------------------------------------------------------------\r\n# round robin balancing for apiserver\r\n#---------------------------------------------------------------------\r\n# 以下是后端相关配置, 关键参数解释如下\r\n# mode tcp: 设置与后端服务通信的模式为TCP\r\n# balance roundrobin: 轮询方式\r\n# inter 10s: 检查间隔为10秒。\r\n# downinter 5s: 当服务被标记为不可用后，每5秒检查一次是否恢复。\r\n# rise 2: 在将服务器标记为上线之前，服务器必须连续2次成功响应检查。\r\n# fall 2: 在将服务器标记为下线之前，服务器必须连续2次失败响应检查。\r\n# slowstart 60s: 慢启动时间为60秒，用于控制新服务器上线后逐渐增加其权重。\r\n# maxconn 250: 每个服务器的最大并发连接数为250。\r\n# maxqueue 256: 后端队列的最大长度为256。\r\n# weight 100: 服务器的默认权重为100。\r\n# server 定义后端的服务器列表。\r\nbackend apiserverbackend\r\n    option tcplog\r\n    option tcp-check\r\n    mode tcp\r\n    balance roundrobin\r\n    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100\r\n    server k8s-mn01 192.168.10.11:6443 check\r\n    server k8s-mn02 192.168.10.22:6443 check\r\n    server k8s-mn03 192.168.10.33:6443 check\r\n\r\n# 服务整体启动\r\nsystemctl enable --now keepalived\r\nsystemctl enable --now haproxy\r\n```\r\n\r\n\r\n\r\n## Etcd的高可用\r\n\r\n这里碍于资源, 并没有采用搭建外部单独的ETCD集群方案。\r\n\r\n当然还是很推荐使用外部的ETCD集群 (前提集群够大, 小集群使用默认提供的就好), 搭建方案目前也很成熟, 使用二进制搭建, 或者使用Docker容器的方式搭建, 再或者使用Kubernetes官方提供的方案: Kubeadm方式进行安装。也都是非常不错的选择。\r\n\r\n这里多说一句, 如果要搭建外部式ETCD, 则请不要把它同控制面安装部署在一起。我在调研Kubeadm安装ETCD高可用集群的时候, 心想尝试一下外部方式, 但是没有多余节点, 就部署到了控制面, 虽然安装上了, 但是kubeadm初始化失败, 报错为: k8s-mn01 not found。\r\n\r\n原因就是Kubernetes集群本身认为既然是外部的Etcd, 那么就不能与控制面安装在一起, 否则会报错。解决方法也挺奇葩, 有需要的可以参考: https://github.com/kubernetes/kubeadm/issues/1438#issuecomment-493004994\r\n\r\n\r\n\r\n## Kubeadm安装集群\r\n\r\n安装集群需要以下镜像: \r\n\r\n* kube-apiserver:v1.29.6\r\n* kube-controller-manager:v1.29.6\r\n* kube-scheduler:v1.29.6\r\n* kube-proxy:v1.29.6\r\n* etcd:3.5.12-0\r\n* pause:3.9\r\n* coredns:v1.11.1\r\n* calico-cni:\r\n\r\n以上的镜像可以自行下载, 也可以直接从Aliyun镜像仓库中下载: \r\n\r\n```shell\r\n# 所需镜像\r\nregistry.aliyuncs.com/google_containers/kube-apiserver:v1.29.6\r\nregistry.aliyuncs.com/google_containers/kube-controller-manager:v1.29.6\r\nregistry.aliyuncs.com/google_containers/kube-scheduler:v1.29.6\r\nregistry.aliyuncs.com/google_containers/kube-proxy:v1.29.6\r\nregistry.aliyuncs.com/google_containers/coredns:v1.11.1\r\nregistry.aliyuncs.com/google_containers/pause:3.9\r\nregistry.aliyuncs.com/google_containers/etcd:3.5.12-0\r\n\r\n# 或者直接一条命令, 依次全部下载\r\nkubeadm config images pull --image-repository registry.aliyuncs.com/google_containers\r\n```\r\n\r\n开始安装 (在第一台控制面节点操作):\r\n\r\n```shell\r\n# 生成默认的初始化配置文件\r\nkubeadm config print init-defaults >  kubeadm-config.yaml\r\n\r\n# 修改\r\n$ vi kubeadm-config.yaml\r\napiVersion: kubeadm.k8s.io/v1beta3\r\nbootstrapTokens:\r\n- groups:\r\n  - system:bootstrappers:kubeadm:default-node-token\r\n  token: abcdef.0123456789abcdef\r\n  ttl: 24h0m0s\r\n  usages:\r\n  - signing\r\n  - authentication\r\nkind: InitConfiguration\r\nlocalAPIEndpoint:\r\n  advertiseAddress: 192.168.10.11\r\n  bindPort: 6443\r\nnodeRegistration:\r\n  criSocket: unix:///var/run/containerd/containerd.sock\r\n  imagePullPolicy: IfNotPresent\r\n  name: k8s-mn01\r\n  taints: null\r\n---\r\napiServer:\r\n  timeoutForControlPlane: 4m0s\r\napiVersion: kubeadm.k8s.io/v1beta3\r\ncertificatesDir: /etc/kubernetes/pki\r\nclusterName: kubernetes\r\ncontrollerManager: {}\r\ndns: {}\r\ncontrolPlaneEndpoint: \"192.168.10.240:8443\"\r\netcd:\r\n  local:\r\n    dataDir: /var/lib/etcd\r\nimageRepository: registry.aliyuncs.com/google_containers\r\nkind: ClusterConfiguration\r\nkubernetesVersion: 1.29.6\r\nnetworking:\r\n  dnsDomain: cluster.local\r\n  serviceSubnet: 172.16.0.0/16\r\n  podSubnet: 10.96.0.0/8\r\nscheduler: {}\r\n```\r\n\r\n若是连接不是本地的, 而是外部的, 则需要将etcd那部分的配置, 修改为:\r\n\r\n```shell\r\nexternal:\r\n    endpoints:\r\n      - https://192.168.10.11:2379\r\n      - https://192.168.10.22:2379\r\n      - https://192.168.10.33:2379\r\n    caFile: /etc/kubernetes/pki/etcd/ca.crt\r\n    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt\r\n    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key\r\n```\r\n\r\n\r\n\r\n重要配置如下: \r\n\r\n| 配置项               | 描述                                                         |\r\n| -------------------- | ------------------------------------------------------------ |\r\n| advertiseAddress     | 指定本机地址                                                 |\r\n| bindPort             | 本机的Kube-Apiserver端口                                     |\r\n| criSocket            | 指定与容器运行时的通信文件                                   |\r\n| name                 | 指定本机的主机名                                             |\r\n| controlPlaneEndpoint | 指定控制面的通信地址, 这里写VIP地址                          |\r\n| imageRepository      | 指定下载Kubernetes组件的镜像仓库地址, 默认访问国外的仓库, 这里需要修改为国内的镜像仓库源 |\r\n| kubernetesVersion    | 指定安装的kubernetes版本                                     |\r\n| serviceSubnet        | 指定Kubernetes的Service资源分配的网段, 网段不能与真实机和Pod的网段冲突。 |\r\n| podSubnet            | 指定Kubernetes的Pod资源分配的网段, 网段不能与真实机和Service的网段冲突。 |\r\n\r\n\r\n\r\nKubeadm-config文件配置好之后, 执行下面的命令, 进行安装：\r\n\r\n```shell\r\nsudo kubeadm init --config kubeadm-config.yaml --upload-certs\r\n```\r\n\r\n安装结果如下:\r\n\r\n```shell\r\nYour Kubernetes control-plane has initialized successfully!\r\n\r\nTo start using your cluster, you need to run the following as a regular user:\r\n\r\n  mkdir -p $HOME/.kube\r\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\r\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\r\n\r\nAlternatively, if you are the root user, you can run:\r\n\r\n  export KUBECONFIG=/etc/kubernetes/admin.conf\r\n\r\nYou should now deploy a pod network to the cluster.\r\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\r\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\r\n\r\nYou can now join any number of the control-plane node running the following command on each as root:\r\n\r\n  kubeadm join 192.168.10.240:8443 --token abcdef.0123456789abcdef \\\r\n\t--discovery-token-ca-cert-hash sha256:ed4897f63cdf316b920d12913d0d45749788f8f85d34c3b81e9b4444dea9faab \\\r\n\t--control-plane --certificate-key ae20760aff597dc87e2ae67e2ab9d588f3eb9825b70ec1474855e44b849b78d3\r\n\r\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\r\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\r\n\"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward.\r\n\r\nThen you can join any number of worker nodes by running the following on each as root:\r\n\r\nkubeadm join 192.168.10.240:8443 --token abcdef.0123456789abcdef \\\r\n\t--discovery-token-ca-cert-hash sha256:ed4897f63cdf316b920d12913d0d45749788f8f85d34c3b81e9b4444dea9faab \r\n\r\n```\r\n\r\n根据上方提示:\r\n\r\n 1. 在执行kubeadm节点上执行:\r\n\r\n    ```shell\r\n      mkdir -p $HOME/.kube\r\n      sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\r\n      sudo chown $(id -u):$(id -g) $HOME/.kube/config\r\n    ```\r\n\r\n2. 在另外两台控制面节点上执行:\r\n\r\n   ```shell\r\n    kubeadm join 192.168.10.240:8443 --token abcdef.0123456789abcdef \\\r\n   \t--discovery-token-ca-cert-hash sha256:ed4897f63cdf316b920d12913d0d45749788f8f85d34c3b81e9b4444dea9faab \\\r\n   \t--control-plane --certificate-key ae20760aff597dc87e2ae67e2ab9d588f3eb9825b70ec1474855e44b849b78d3\r\n   ```\r\n\r\n3. 在数据面节点上执行加入集群的命令:\r\n\r\n   ```shell\r\n   kubeadm join 192.168.10.240:8443 --token abcdef.0123456789abcdef \\\r\n   \t--discovery-token-ca-cert-hash sha256:ed4897f63cdf316b920d12913d0d45749788f8f85d34c3b81e9b4444dea9faab \r\n   ```\r\n\r\n   若加入集群的令牌失效, 可以使用如下命令在控制面上重新生成: \r\n\r\n   ```shell\r\n   kubeadm token create --print-join-command\r\n   ```\r\n\r\n4. 检测安装是否正常: \r\n\r\n   ```shell\r\n   # 控制面节点\r\n   [root@k8s-mn01 ~]# kubectl get no\r\n   NAME       STATUS     ROLES           AGE   VERSION\r\n   k8s-mn01   NotReady   control-plane   24h   v1.29.6\r\n   k8s-mn02   NotReady   control-plane   24h   v1.29.6\r\n   k8s-mn03   NotReady   control-plane   24h   v1.29.6\r\n   k8s-wn01   NotReady   <none>          19m   v1.29.6\r\n   k8s-wn02   NotReady   <none>          9s    v1.29.6\r\n   ```\r\n\r\n\r\n\r\n安装网络插件Calico: \r\n\r\n​\tCalico截至目前为止, 最新的版本是3.28。该版本, 官方经过了与Kubernetes的充分测试, 支持: Kubernetesv1.27-1.30。\r\n\r\n​\tCalico3.27版本支持: Kubernetesv1.27-v1.29。\r\n\r\n​\t这里我们选用Calico:3.28。\r\n\r\n​\tCalico的安装方式目前有两种:\r\n\r\n​\t\t* 基于Operator方式安装, 能够管理Calico集群的安装, 升级, 生命周期管理等。\r\n\r\n​\t\t* 基于静态资源清单安装, 方便, 简单, 但无法像Opertaor一样能够自动管理Calico的生命周期。\r\n\r\n​\t\t  基于静态资源清单的部署常见的也分为两种: \r\n\r\n​\t\t\t\t*calico.yaml*: 当Calico使用Kubernetes API作为数据存储, 且集群节点少于50个。\r\n\r\n​\t\t\t\tcalico-typha.yaml: 当Calico使用Kubernetes API作为数据存储, 且集群节点大于50个。\r\n\r\n​\t这里我们选用基于清单的方式, 且使用calico-typha.yaml的方式部署, 对于一般的集群来说, 足够 (第一台控制面节点上操作)。\r\n\r\n```shell\r\nhttps://github.com/projectcalico/calico/blob/master/manifests/calico-typha.yaml\r\n\r\n# 重点配置:\r\nreplicas: 副本数 , 建议每200个节点1个副本, 生产的话建议3个副本。这里默认不变: 1个。\r\n# 以下配置: 用于给Pod分配IP的地址池范围, 因在kubeadm-config文件中定义过了Pod的IP地址池范围, 所以这里就不需要再配置了, Calico会根据kubeadm配置的来进行IP地址划分。\r\n- name: CALICO_IPV4POOL_CIDR  \r\n  value: \"192.168.0.0/16\"\r\n# Calico默认安装使用的IPIP模式。Always: 表示全网络覆盖; Cross-SubNet: 表示跨子网覆盖; Nerver: 表示不启用。\r\n- name: CALICO_IPV4POOL_IPIP\r\nvalue: \"Always\"\r\n\r\n# 镜像默认需要这些: \r\ndocker.io/calico/cni:master\r\ndocker.io/calico/node:master\r\ndocker.io/calico/kube-controllers:master\r\ndocker.io/calico/typha:master\r\n\r\n# 但由于镜像仓库在国外, 拉取不到, 可以换成我已经上传好的。\r\nsed -i \"s#docker.io/calico#registry.cn-hangzhou.aliyuncs.com/week-cnative#g\" calico-typha.yaml \r\nsed -i \"/week-cnative/ s/master/v3.28.0/g\" calico-typha.yaml\r\n\r\n# 安装Calico\r\n$ kubectl apply -f calico-typha.yaml \r\n```\r\n\r\n\r\n\r\n安装结束\r\n\r\n等待Calicao所有的Pod运行起来, 集群搭建成功。\r\n\r\n```shell\r\n[root@k8s-mn01 ~]# kubectl get no\r\nNAME       STATUS   ROLES           AGE   VERSION\r\nk8s-mn01   Ready    control-plane   25h   v1.29.6\r\nk8s-mn02   Ready    control-plane   25h   v1.29.6\r\nk8s-mn03   Ready    control-plane   25h   v1.29.6\r\nk8s-wn01   Ready    <none>          62m   v1.29.6\r\nk8s-wn02   Ready    <none>          43m   v1.29.6\r\n\r\n[root@k8s-mn01 ~]# kubectl get pod -n kube-system\r\nNAME                                       READY   STATUS    RESTARTS         AGE\r\ncalico-kube-controllers-67d65c9d9d-wqbv9   1/1     Running   2 (38m ago)      40m\r\ncalico-node-22r7n                          1/1     Running   7 (5m59s ago)    15m\r\ncalico-node-26kqv                          1/1     Running   7 (5m44s ago)    14m\r\ncalico-node-5w4lf                          1/1     Running   7 (5m57s ago)    14m\r\ncalico-node-nmqk9                          1/1     Running   0                26s\r\ncalico-node-qgxwr                          1/1     Running   13 (6m13s ago)   40m\r\ncalico-typha-75f8b94cd7-9ks9d              1/1     Running   0                5m17s\r\ncoredns-857d9ff4c9-t62xn                   1/1     Running   0                25h\r\ncoredns-857d9ff4c9-vxs8h                   1/1     Running   0                25h\r\netcd-k8s-mn01                              1/1     Running   2 (20h ago)      25h\r\netcd-k8s-mn02                              1/1     Running   1 (20h ago)      25h\r\netcd-k8s-mn03                              1/1     Running   1 (20h ago)      25h\r\nkube-apiserver-k8s-mn01                    1/1     Running   3 (75m ago)      25h\r\nkube-apiserver-k8s-mn02                    1/1     Running   1 (20h ago)      25h\r\nkube-apiserver-k8s-mn03                    1/1     Running   1 (20h ago)      25h\r\nkube-controller-manager-k8s-mn01           1/1     Running   4 (20h ago)      25h\r\nkube-controller-manager-k8s-mn02           1/1     Running   2 (26m ago)      25h\r\nkube-controller-manager-k8s-mn03           1/1     Running   1 (20h ago)      25h\r\nkube-proxy-h6s74                           1/1     Running   0                42m\r\nkube-proxy-jkphx                           1/1     Running   1 (20h ago)      25h\r\nkube-proxy-rn48p                           1/1     Running   1 (20h ago)      25h\r\nkube-proxy-wdj8w                           1/1     Running   0                61m\r\nkube-proxy-wv8jh                           1/1     Running   1 (20h ago)      25h\r\nkube-scheduler-k8s-mn01                    1/1     Running   3 (20h ago)      25h\r\nkube-scheduler-k8s-mn02                    1/1     Running   3 (25m ago)      25h\r\nkube-scheduler-k8s-mn03                    1/1     Running   1 (20h ago)      25h\r\n\r\n\r\n[root@k8s-mn01 ~]# nerdctl run --rm -it --net host -v /etc/kubernetes:/etc/kubernetes registry.aliyuncs.com/google_containers/etcd:3.5.12-0 etcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key --cacert /etc/kubernetes/pki/etcd/ca.crt --endpoints https://192.168.10.11:2379,https://192.168.10.22:2379,https://192.168.10.33:2379 endpoint health --write-out=table\r\n+----------------------------+--------+-------------+-------+\r\n|          ENDPOINT          | HEALTH |    TOOK     | ERROR |\r\n+----------------------------+--------+-------------+-------+\r\n| https://192.168.10.11:2379 |   true | 39.385214ms |       |\r\n| https://192.168.10.22:2379 |   true | 68.702829ms |       |\r\n| https://192.168.10.33:2379 |   true | 72.560861ms |       |\r\n+----------------------------+--------+-------------+-------+\r\n\r\n[root@k8s-mn01 ~]# nerdctl run --rm -it --net host -v /etc/kubernetes:/etc/kubernetes registry.aliyuncs.com/google_containers/etcd:3.5.12-0 etcdctl --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key --cacert /etc/kubernetes/pki/etcd/ca.crt --endpoints https://192.168.10.11:2379,https://192.168.10.22:2379,https://192.168.10.33:2379 endpoint status --write-out=table\r\n+----------------------------+------------------+---------+---------+-----------+------------+-----------+---------\r\n|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |\r\n+----------------------------+------------------+---------+---------+-----------+------------+-----------+--------+\r\n| https://192.168.10.11:2379 | 6571fb7574e87dba |  3.5.12 |  5.6 MB |     false |      false |        17 |      60236 |              60236 |        |\r\n| https://192.168.10.22:2379 | a2fae84dac15fbd1 |  3.5.12 |  5.6 MB |      true |      false |        17 |      60236 |              60236 |        |\r\n| https://192.168.10.33:2379 | 514d51979dd338dc |  3.5.12 |  5.6 MB |     false |      false |        17 |      60236 |              60236 |        |\r\n+----------------------------+------------------+---------+---------+-----------+------------+-----------+--------+\r\n\r\n```\r\n\r\n安装成功之后, Kubernetes会生成一系列的目录, 重要的目录, 解释如下: \r\n\r\n```shell\r\n[root@k8s-mn01 ~]# tree /etc/kubernetes/\r\n/etc/kubernetes/\t\t\t\t\t\t\t\r\n├── admin.conf\t\t\t\t\t\t\t\t\r\n├── controller-manager.conf\t\t\t\t\t\r\n├── kubelet.conf\r\n├── manifests\r\n│   ├── etcd.yaml\r\n│   ├── kube-apiserver.yaml\r\n│   ├── kube-controller-manager.yaml\r\n│   └── kube-scheduler.yaml\r\n├── pki\r\n│   ├── apiserver-etcd-client.crt\r\n│   ├── apiserver-etcd-client.key\r\n│   ├── apiserver-kubelet-client.crt\r\n│   ├── apiserver-kubelet-client.key\r\n│   ├── apiserver.crt\r\n│   ├── apiserver.key\r\n│   ├── ca.crt\r\n│   ├── ca.key\r\n│   ├── etcd\r\n│   │   ├── ca.crt\r\n│   │   ├── ca.key\r\n│   │   ├── healthcheck-client.crt\r\n│   │   ├── healthcheck-client.key\r\n│   │   ├── peer.crt\r\n│   │   ├── peer.key\r\n│   │   ├── server.crt\r\n│   │   └── server.key\r\n│   ├── front-proxy-ca.crt\r\n│   ├── front-proxy-ca.key\r\n│   ├── front-proxy-client.crt\r\n│   ├── front-proxy-client.key\r\n│   ├── sa.key\r\n│   └── sa.pub\r\n├── scheduler.conf\r\n└── super-admin.conf\r\n```\r\n\r\n解释如下: \r\n\r\n1. admin.conf, controller-manager.conf, kubelet.conf, scheduler.conf, super-admin.conf：\r\n\r\n   这些是不同角色的Kubernetes配置文件，每个文件包含了对应角色的认证信息、访问控制配置和集群连接信息。通常由kubeconfig工具生成。\r\n\r\n2. manifests: 这个目录包含了Kubernetes集群中各个核心组件的静态配置清单（YAML文件），用于指定各个组件如何启动和配置。\r\n\r\n3. pki: 这个目录中包含了Kubernetes集群的公钥和私钥文件，以及CA证书，用于保证集群内部通信的安全性。具体文件包括:\r\n\r\n   **apiserver-etcd-client.crt, apiserver-etcd-client.key**: 用于API服务器与etcd客户端通信的证书。\r\n\r\n   **apiserver-kubelet-client.crt, apiserver-kubelet-client.key**: 用于API服务器与kubelet客户端通信的证书。\r\n\r\n   **apiserver.crt, apiserver.key**: API服务器的证书和私钥。\r\n\r\n   **ca.crt, ca.key**: 集群的根CA证书和私钥，用于签发其他证书。\r\n\r\n   **etcd/**: etcd存储相关的证书和私钥。\r\n\r\n   **front-proxy-ca.crt, front-proxy-ca.key**: 用于前置代理的CA证书和私钥。\r\n\r\n   **front-proxy-client.crt, front-proxy-client.key**: 用于前置代理客户端的证书和私钥。\r\n\r\n   **sa.key, sa.pub**: Kubernetes中的Service Account的私钥和公钥。\r\n\r\n\r\n\r\n## 结语\r\n\r\nKubernetes的安装方式有很多, kubeadm只是其中的一种, 其它比如Rancher的RKE (个人也比较喜欢), Kubesphere的KubeKey, Sealos等等, 都是安装很便捷的工具, 但各自的高可用方案, 底层的实现原理, 都如本文类似, 大差不差。以后有时间再尝试一下其它的安装方案吧, 到时候可以做个类比文章, 了解每一种方案的优缺点, 不同场景下, 选择最适用的方案。\r\n\r\n---\r\n\r\n[1] https://mp.weixin.qq.com/s/Hl2seS_Xn9dQsynpbS6Jiw\r\n\r\n[2] https://v1-29.docs.kubernetes.io/releases/version-skew-policy\r\n\r\n[3] https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm\r\n\r\n[4] https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability\r\n\r\n[5] https://kubernetes.io/docs/setup/production-environment/container-runtimes\r\n\r\n[6] https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#bootstrap-the-cluster"
        },
        {
          "id": "Rocky Linux 9.3 系统安装",
          "metadata": {
            "permalink": "/Rocky Linux 9.3 系统安装",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/04-Rocky Linux 9.3 系统安装.md",
            "source": "@site/blog/04-Rocky Linux 9.3 系统安装.md",
            "title": "Rocky Linux 9.3 系统安装",
            "description": "rocky",
            "date": "2024-06-05T21:35:00.000Z",
            "formattedDate": "2024年6月5日",
            "tags": [
              {
                "label": "rocky",
                "permalink": "/tags/rocky"
              },
              {
                "label": "linux",
                "permalink": "/tags/linux"
              }
            ],
            "readingTime": 4.01,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "Rocky Linux 9.3 系统安装",
              "title": "Rocky Linux 9.3 系统安装",
              "date": "2024-06-05 21:35",
              "tags": [
                "rocky",
                "linux"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
              "permalink": "/一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜"
            },
            "nextItem": {
              "title": "Netstat命令运用之，深入理解网络连接",
              "permalink": "/Netstat命令运用之，深入理解网络连接"
            }
          },
          "content": "<!-- ![logo](/assets/images/rockylinux.jpg) -->\r\n![rocky](/assets/images/rockylinux.jpg)\r\n### 写在前面\r\n\r\n​\t\tCentos最为Linux开源发行版中最受人欢迎的系列，即将迎来它的黄昏。其所带来的价值是无限的，地位是不可替代的。我也是其中受益的一份子。\r\n\r\n​\t\t正因如此，需要找到能够平替的新系统：差异化小，稳定健壮，提供长支持能力。\r\n\r\n​\t\tUbuntu，Fedora，SUSE，RedHat，Rocky......都是能够考虑的。\r\n\r\n​\t\t今天推荐的是Rocky Linux，与Centos几乎无异，命令通用，安装简单，开源，对标RH而进行的代码重构，且作者是前Centos项目的创始人，提供长期支持，提供Centos迁移方案等，是我认为该系统是下一个Centos。\r\n\r\n\r\n<!-- truncate -->\r\n\r\n\r\n### 安装步骤：\r\n\r\n1. 官网下载Rocky Linux 9.3镜像\r\n\r\n   注意：在本文编写时间，Rocky最新版本9.4发布，官网的下载页面默认均为9.4，下载地址为：\r\n\r\n   ```shell\r\n   https://download.rockylinux.org/pub/rocky/9/isos/x86_64/Rocky-9.4-x86_64-minimal.iso\r\n   ```\r\n\r\n   ​\t\t\t如果想要下载历史版本的镜像，地址与上面并不相同。目前Rocky 9.3下载地址为：\r\n\r\n   ```shell\r\n   https://dl.rockylinux.org/vault/rocky/9.3/isos/x86_64/Rocky-9.3-x86_64-minimal.iso\r\n   ```\r\n\r\n   \r\n\r\n2. Rocky Linux 9.3 系统安装\r\n\r\n   * Vmware 17版本演示.\r\n\r\n   * 新建虚拟机步骤不再赘述. \r\n\r\n     * 磁盘容量：50G\r\n\r\n     * 内存：1G；CPU：1核\r\n\r\n     * 选用 步骤1 中现在的镜像.\r\n\r\n     * 需要注意的是，在`新建虚拟机的导向过程`中，客户机操作系统的选择，版本这里选用的是：其它 Linux 5.x 内核 64 位. \r\n\r\n     * 启动虚拟机，出现如下界面. （ 了解Centos7系统的，相信并不陌生. ）默认第一步即可. \r\n\r\n       ![image-20240604143043854](/images/04-rocky_linux/image-20240604143043854.png)\r\n\r\n     * 进入安装界面，语言选择 中文简体 ；继续，进入该界面: \r\n\r\n       ​\t![image-20240604143507063](/images/04-rocky_linux/image-20240604143507063.png)\r\n\r\n       * 进入：软件选择，勾选标准安装. （ 附带上系统环境软件 ）.\r\n       * 进入：安装目的地，自定义划分磁盘：/boot分区：512M-1024M；swap分区：内存的2倍（若真实服务器内存>16GB，则swap分区大小为内存的1倍）；/分区：剩余全部 .\r\n\r\n       ![image-20240604144215882](/images/04-rocky_linux/image-20240604144215882.png)\r\n\r\n       * 进入：Root密码，配置root密码，并勾选下方的`允许使用root用户进行ssh远程登录.` .\r\n\r\n       * 开始安装 . \r\n\r\n       * 安装结束，进入登录界面. \r\n\r\n         * 提示信息：activate the web console with:  systemctl enable --now cockpit.socket\r\n\r\n           该含义是：使用命令：\"systemctl enable --now cockpit.socket\" 激活web管理界面。即该系统安装了cockpit, 激活cockpit, 能够实现图形化界面管理Rocky Linux系统。\r\n\r\n     3. IP地址配置，SSH远程连接\r\n\r\n        不同于Centos7，Rocky Linux 9.3中配置IP地址的方式，有较大的差别。配置方式有三种：\r\n\r\n        1. nmtui：通过图形化界面配置网络\r\n        2. nmcli：通过命令/交互方式配置网络（官方推荐）\r\n        3. 网卡配置文件方式配置网络（官方不推荐）\r\n\r\n        但由于本人是Centos7系统的重度感染者，依旧习惯于编辑网卡配置文件的方式。\r\n\r\n        ```shell\r\n        # 编辑网络配置文件\r\n        vi /etc/NetworkManager/system-connections/ens33.nmconnection\r\n        \r\n        [connection]\r\n        id=ens33\r\n        uuid=a5d63f95-a602-3897-943a-f48238886e99\r\n        type=ethernet\r\n        autoconnect-priority=-999\r\n        interface-name=ens33\r\n        \r\n        [ethernet]\r\n        \r\n        [ipv4]\r\n        method=manual\r\n        address=192.168.10.166/24\r\n        gateway=192.168.10.2\r\n        dns=114.114.114.114;8.8.8.8\r\n        \r\n        [ipv6]\r\n        addr-gen-mode=eui64\r\n        method=auto\r\n        \r\n        [proxy]\r\n        \r\n        # 重启生效\r\n        nmcli connection  load /etc/NetworkManager/system-connections/ens33.nmconnection\r\n        nmcli connection  up /etc/NetworkManager/system-connections/ens33.nmconnection \r\n        \r\n        # 查看生效\r\n        ifconfig ens33 或者 ip addr\r\n        ```\r\n\r\n        上述网卡文件内容解释 ( 内容来自官网：[RL9 - network manager - Documentation (rockylinux.org)](https://docs.rockylinux.org/zh/gemstones/network/RL9_network_manager/?h=networkmanager) )：\r\n\r\n        | connection     |                                                              |\r\n        | -------------- | ------------------------------------------------------------ |\r\n        | 键名称         | 描述                                                         |\r\n        | id             | con-name 的别名，其值为字符串。                              |\r\n        | uuid           | 设备唯一表示。                                               |\r\n        | type           | 连接的类型，其值可以是 ethernet、bluetooth、vpn、vlan 等等。 您可以使用 `man nmcli` 查看所有支持的类型。 |\r\n        | interface-name | 此连接绑定到的网络接口的名称，其值为字符串。                 |\r\n        | timestamp      | Unix 时间戳，以秒为单位。 此处的值是自1970年1月1日以来的秒数。 |\r\n        | autoconnect    | 是否随系统开机自启动。 值为布尔型。                          |\r\n        |                |                                                              |\r\n        | **ethernet**   |                                                              |\r\n        | 键名称         | 描述                                                         |\r\n        | mac-address    | MAC 物理地址。                                               |\r\n        | mtu            | 最大传输单位。                                               |\r\n        | auto-negotiate | 是否自动协商。 值为布尔型。                                  |\r\n        | duplex         | 值可以是 half （半双工）、full（全双工）                     |\r\n        | speed          | 指定网卡的传输速率。 100 即 100Mbit/s。 如果**auto-negotiate=false**，则必须设置 **speed** 键和 **duplex** 键；如果 **auto-negotiate=true**，则使用的速率为协商速率，此处的写入不生效（仅适用于BASE-T 802.3规范）；当非零时，**duplex** 键必须有值。 |\r\n        |                |                                                              |\r\n        | **ipv4**       |                                                              |\r\n        | 键名称         | 描述                                                         |\r\n        | address        | 分配的IP地址。                                               |\r\n        | gateway        | 分配的网关地址。                                             |\r\n        | dns            | 分配的dns地址，多个dns地址之间使用；分隔。                   |\r\n        | method         | IP获取的方法。 值是字符串类型。 值可以是：auto、disabled、link-local、manual、shared【auto：自动获取；manual：静态获取】 |\r\n\r\n     \r\n\r\n     **至此，可以通过远程连接工具，连接使用。**\r\n### 写在最后\r\n\r\n​\t\t\t对于大多数企业来讲，更换一个操作系统，风险，精力是巨大的，可能更多的是选择后来者的业务部署在新的系统，逐渐的转变。\r\n\r\n​\t\t\tRocky Linux希望能够成为各位的选择.......\r\n\r\n​"
        },
        {
          "id": "Netstat命令运用之，深入理解网络连接",
          "metadata": {
            "permalink": "/Netstat命令运用之，深入理解网络连接",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/03-Netstat命令运用之，深入理解网络连接.md",
            "source": "@site/blog/03-Netstat命令运用之，深入理解网络连接.md",
            "title": "Netstat命令运用之，深入理解网络连接",
            "description": "假设你要给朋友打一个电话，输入电话号码，之后朋友接通了电话，那么，通话过程就算是建立成功。这个接通的状态我们称之为ESTABLISHED。",
            "date": "2024-03-10T21:50:00.000Z",
            "formattedDate": "2024年3月10日",
            "tags": [
              {
                "label": "tcp",
                "permalink": "/tags/tcp"
              },
              {
                "label": "netstat",
                "permalink": "/tags/netstat"
              }
            ],
            "readingTime": 7.1,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "Netstat命令运用之，深入理解网络连接",
              "title": "Netstat命令运用之，深入理解网络连接",
              "date": "2024-03-10 21:50",
              "tags": [
                "tcp",
                "netstat"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "Rocky Linux 9.3 系统安装",
              "permalink": "/Rocky Linux 9.3 系统安装"
            },
            "nextItem": {
              "title": "SSL证书详解",
              "permalink": "/ssl证书详解"
            }
          },
          "content": "<!-- ![logo](/assets/images/avatar300.png) -->\r\n\r\n\r\n\r\n假设你要给朋友打一个电话，输入电话号码，之后朋友接通了电话，那么，通话过程就算是建立成功。这个接通的状态我们称之为`ESTABLISHED`。\r\n\r\n不管是在Windows系统，还是Linux系统，功能的提供都依靠背后的进程。因此，读懂进程语言是重要的。\r\n\r\n在上面的例子中，如果建立通话过程中遇到了故障了，又会是哪些状态呢？\r\n\r\n<!-- truncate -->\r\n\r\n## 状态分类\r\n\r\n1. **LISTEN**：表示进程正在监听指定端口，等待其他进程发起连接请求。好比你的朋友家电话在等待其他人拨打过来。\r\n2. **ESTABLISHED**：表示连接已经建立并且数据可以传输。\r\n3. **TIME_WAIT**：表示进程在等待，以确保远程端接收到最后的确认。好比你在等待你的朋友接听。\r\n4. **CLOSE_WAIT**：表示连接已经关闭，但你还在等待朋友对电话做出回应。\r\n5. **FIN_WAIT1 和 FIN_WAIT2**：表示连接即将关闭或正在等待远程端的关闭信号，好比你正准备要挂断电话一样。\r\n6. **CLOSING**：表示连接关闭过程中可能出现问题，类似于拨打过程中出现异常情况。\r\n7. **LAST_ACK**：表示你拨打了电话，但还在等待朋友的确认。\r\n8. **SYN_SENT 和 SYN_RECV**：表示正在建立连接的过程中，好比你通话过程的建立中。\r\n9. **UNKNOWN**：表示状态未知，可能是系统出现异常，好比是信号塔出现了故障，导致电话信号无法发送，或者无法拨打。\r\n\r\n\r\n\r\n## 查看状态\r\n\r\n以Linux系统为例，使用`netstat`或者`ss`命令进行查看进程连接状态\r\n\r\n```shell\r\n# netstat 命令的基本用法和常用选项：\r\n-a（all）：显示所有连接和监听端口，包括那些处于等待连接的状态。\r\n-t（tcp）：仅显示 TCP 协议相关的连接信息。\r\n-u（udp）：仅显示 UDP 协议相关的连接信息。\r\n-n（numeric）：以数字形式显示地址和端口号，而不进行反向域名解析。\r\n-p（program）：显示与连接相关的进程信息。\r\n-e（extended）：显示额外的详细信息，如用户 ID 和 inode 等。\r\n-r（route）：显示路由表信息。\r\n-c（continuous）：持续显示网络状态信息，而非一次性输出。\r\n\r\n# 查看tcp协议的进程\r\n[root@localhost ~]# netstat -naplt\r\nActive Internet connections (servers and established)\r\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \r\ntcp        0      0 127.0.0.1:9000          0.0.0.0:*               LISTEN      8955/php-fpm: maste \r\ntcp        0      0 0.0.0.0:3306            0.0.0.0:*               LISTEN      8159/mysqld         \r\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      6768/sshd           \r\ntcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      7065/master         \r\ntcp        0      0 0.0.0.0:10050           0.0.0.0:*               LISTEN      9603/zabbix_agentd  \r\ntcp        0      0 0.0.0.0:10051           0.0.0.0:*               LISTEN      9722/zabbix_server  \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51314         TIME_WAIT   -                   \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51300         TIME_WAIT   -                 \r\ntcp        0      0 192.168.10.110:22       192.168.10.1:33819      ESTABLISHED 21143/sshd: root@pt \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51278         TIME_WAIT   -                   \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51296         TIME_WAIT   -                   \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51346         TIME_WAIT   -                   \r\ntcp        0      0 127.0.0.1:10050         127.0.0.1:51318         TIME_WAIT   -                   \r\ntcp        0      0 192.168.10.110:22       192.168.10.1:1834       ESTABLISHED 7250/sshd: root@pts \r\n\r\n# 输出信息说明：\r\nProto：协议类型，如tcp或udp。\r\nRecv-Q：接收队列中的数据量（以字节为单位）。\r\nSend-Q：发送队列中的数据量（以字节为单位）。\r\nLocal Address：本地IP地址和端口号。\r\nForeign Address：远程IP地址和端口号。\r\nState：连接状态。\r\nPID/Program name：与连接关联的进程ID和程序名称\r\n\r\n============================================================================================================\r\n\r\n# ss 命令的基本用法和常用选项：\r\n-t：显示 TCP 套接字信息。\r\n-u：显示 UDP 套接字信息。\r\n-l：仅显示监听状态的套接字。\r\n-a：显示所有套接字（包括监听和非监听状态）。\r\n-p：显示与套接字关联的进程信息。\r\n-n：以数字形式显示地址和端口号，而不进行反向域名解析。\r\n-s：按照协议统计套接字数量。\r\n-o：显示计时器信息。\r\n-i：显示套接字的详细信息。\r\n\r\n# 查看tcp协议的进程\r\n[root@localhost ~]# ss -naptn\r\nState     Recv-Q  Send-Q      Local Address:Port    Peer Address:Port              \r\nLISTEN      0      50               *:3306      \t\t*:*   \t\t   users:((\"mysqld\",pid=7740,fd=14))\r\n\r\n# 输出信息说明：\r\ntate：连接状态，这里是LISTEN，表示该端口正在监听来自其他计算机的连接请求。\r\nRecv-Q：接收队列中的数据量（以字节为单位），这里是0。\r\nSend-Q：发送队列中的数据量（以字节为单位），这里是50。\r\nLocal Address:Port：本地IP地址和端口号，*:3306表示所有网络接口上的3306端口。\r\nPeer Address:Port：对等方的IP地址和端口号，*:*表示任意远程地址和端口。\r\nusers：进程相关信息，包括进程名称（mysqld）、进程ID（pid=7740）和文件描述符（fd=14）。\r\n```\r\n\r\n\r\n\r\n## 底层原理\r\n\r\n进程之间的连接，都是依靠套接字来进行的，在linux系统中一般以.sock结尾的文件，称之为套接字文件。\r\n\r\n套接字：是进程与网络之间的接口，通过网络中不同主机上的一端，到另一端的通信，数据交换的机制。就好比是上方例子的电话本身就是套接字的抽象。在套接字文件中，会包含通信的基本信息：IP，端口等，就好比是打电话你要输入对方的电话号码。\r\n\r\n在建立过程当中，有TCP会话建立，UDP会话建立，这种建立过程，一般分为三步，熟称为`三次握手`；建立解除，一般分为四步，熟称为`四次挥手`。\r\n\r\n\r\n\r\n一图胜千言：\r\n\r\n![netstat](https://cdn.jsdelivr.net/gh/week2311/Images@main/netstat.png)结合上图，\r\n\r\n三次握手流程：\r\n\r\n1. 第一次握手。客户端向服务器发送一个SYN标志位置为1的包，并且包含一个随机生成的序列号（seq number），发送完毕后，客户端进入SYN_SENT状态，等待服务器的确认。\r\n2. 第二次握手。服务器收到客户端的SYN包后，会发送一个SYN和ACK标志位都置为1的包，其中ACK number设置为客户端的seq number加1，确定是回复的来自客户端的包，同时服务器也会生成一个随机数作为初始发送序号（initial sequence number）。发送完毕后，服务器进入SYN_RCVD状态。\r\n3. 第三次握手。客户端收到服务器的SYN+ACK包后，会发送一个ACK标志位置为1的包，其中ACK number设置为服务器的seq number加1，表示客户端确认收到了服务器的数据。发送完毕后，客户端和服务器进入ESTABLISHED状态，完成三次握手。\r\n\r\n四次挥手流程：\r\n\r\n1. 第一次挥手（FIN_WAIT_1）：客户端发送一个FIN报文，并指定一个序列号。这标志着客户端准备关闭从服务器到客户端的数据传输。客户端进入FIN_WAIT_1状态，此时客户端不再发送数据，但仍可接收数据。12345\r\n\r\n2. 第二次挥手（CLOSE_WAIT）：服务器收到客户端的FIN后，会发送一个ACK报文，确认号设置为收到的序列号加1。服务器进入CLOSE_WAIT状态，这意味着服务器已经收到客户端的关闭请求，但服务器可能还有数据需要发送给客户端。\r\n\r\n1. 第三次挥手（LAST_ACK）：服务器发送一个FIN报文，并指定一个序列号，表示服务器准备关闭从客户端到服务器的数据传输。服务器进入LAST_ACK状态。\r\n2. 第四次挥手（TIME_WAIT）：客户端收到服务器的FIN后，会发送一个ACK报文，确认号设置为收到的序列号加1。客户端进入TIME_WAIT状态，并等待一段时间（2MSL，即Maximum Segment Lifetime，报文段最大寿命），以确保服务器收到ACK报文。这段时间之后，客户端和服务器都进入CLOSED状态，完成四次挥手。\r\n\r\n注：在这里说明一下，网络查询的时候，会发现握手，或者挥手的过程中，有的连接会多seq序列的字段，比如四次挥手的服务器的第一次发送请求包，其实这里我的理解是seq作用是标记自己，回包的时候ack确认回复会使用到。但有的时候会发送两次连接，也就是两个seq发送，每一次的值不同，那么回包的时候也就以最后一次为准，所以有时图中未标记出，表示并未很大作用，没有展示出来。\r\n\r\n\r\n\r\n## 总结\r\n\r\n通过阅读本文，希望能够帮助你能更进一步的了解进程之间的网络连接，并且能够根据状态信息，在日常运维或者排查错误时，可以带来更多的思路，想法等等。"
        },
        {
          "id": "ssl证书详解",
          "metadata": {
            "permalink": "/ssl证书详解",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/02-SSL证书详解.md",
            "source": "@site/blog/02-SSL证书详解.md",
            "title": "SSL证书详解",
            "description": "在访问众多的网站中，相信大家都有遇到过下面的场景：",
            "date": "2024-02-25T12:20:00.000Z",
            "formattedDate": "2024年2月25日",
            "tags": [
              {
                "label": "ssl",
                "permalink": "/tags/ssl"
              },
              {
                "label": "ca",
                "permalink": "/tags/ca"
              }
            ],
            "readingTime": 5.523333333333333,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "ssl证书详解",
              "title": "SSL证书详解",
              "date": "2024-02-25 12:20",
              "tags": [
                "ssl",
                "ca"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "Netstat命令运用之，深入理解网络连接",
              "permalink": "/Netstat命令运用之，深入理解网络连接"
            },
            "nextItem": {
              "title": "Docusaurus博客搭建",
              "permalink": "/first-blog"
            }
          },
          "content": "<!-- ![logo](/assets/images/avatar300.png) -->\r\n\r\n在访问众多的网站中，相信大家都有遇到过下面的场景：\r\n\r\n![ssl](https://cdn.jsdelivr.net/gh/week2311/Images@main/ssl.png)\r\n\r\n一般遇到这种情况，大多数都是点击高级继续访问，类似那种`无视风险继续安装`的意思。但其实这是浏览器在向你发出的一种警示信号，告诉你：访问这个IP地址的网站是不安全的。那么浏览器是如何得知的呢，答案就是`证书`。\r\n\r\n\r\n<!-- truncate -->\r\n## SSL协议\r\n\r\n首先我们先了解一下ssl协议：\r\n\r\n> SSL（Secure Sockets Layer）协议是一种用于在计算机网络上安全传输数据的协议。它使用加密技术来确保在客户端和服务器之间传输的数据是安全的，不会被未经授权的人员窃取或篡改。通常用于保护网站上的敏感信息，例如登录凭据、信用卡信息等。\r\n\r\n也就是说，这是一个信息传输过程中的安全加密协议，防止黑客的攻击，信息的泄露等。上图中的提示信息是如此，也是http+ssl协议实现了这样的效果。\r\n\r\n工作原理是通过在客户端和服务器之间建立安全的连接，然后使用加密算法对数据进行加密和解密。这样可以确保在数据传输过程中，即使被截获，也无法被破解。\r\n\r\n那么此时就有一个问题，如何去让一个服务，或者服务器得到信任呢？\r\n\r\n\r\n\r\n## CA证书\r\n\r\nCA证书用来标识服务器的一个合法身份，可以理解为的居民身份证。\r\n\r\n如何判断一个人是否是合法的中国公民 ----- 身份证\r\n\r\n如何判断一台服务器是否是合法的 ----- CA证书。\r\n\r\n现实生活中，身份证是由公安机颁发，那么一个人要拿到身份证，一般要经过以下流程：\r\n\r\n![person_card](https://cdn.jsdelivr.net/gh/week2311/Images@main/person_card.png)\r\n\r\n服务器领域中，CA证书是由CA机构（Certificate Authority，凭证管理中心）颁发的，那么要拿到CA机构颁发下来的证书，与上方的流程类型：\r\n\r\n![ca_card](https://cdn.jsdelivr.net/gh/week2311/Images@main/ca_card.png)\r\n\r\n### 申请证书\r\n\r\n根据上图，申请证书步骤如下：\r\n\r\n注：以下都是模拟证书的生成过程，真实情况一般不需要这么复杂，掏钱就行。\r\n\r\n1. 我们需要生成私钥，即非对称加密技术（下文会讲解）。\r\n\r\n```shell\r\nopenssl genrsa -out cert.key 1024\r\n```\r\n\r\n2. 创建证书申请文件.csr\r\n\r\n```shell\r\nopenssl req -new -key cert.key -out cert.csr\r\n# 在这一步骤中， 需要我们输入申请信息：国家，地区，组织，email等。\r\n```\r\n\r\n![csr](https://cdn.jsdelivr.net/gh/week2311/Images@main/csr.png)\r\n\r\n3. CA签发证书\r\n\r\n```shell\r\nopenssl x509 -req -days 365 -sha256 -in cert.csr -signkey cert.key -out cert.pem\r\n# 注解：\r\n# -days 365：证书有限期365天\r\n# -sha256：使用 SHA-256 哈希算法来对证书进行签名\r\n# -in cert.csr：指定申请文件路径\r\n# -sign cert.key：指定用于签名 CSR 的私钥文件的路径和名称\r\n# -out cert.pem：指定生成的证书文件的路径和名称\r\n```\r\n\r\n至此，在你的目录下，会多出来以下文件：\r\n\r\n```shell\r\n# cert.key：服务器私钥文件。\r\n# cert.csr：证书申请文件。\r\n# cert.pem：pem格式的证书文件，其中包含私钥，证书等秘密数据。\r\n```\r\n\r\n\r\n\r\n### 证书后缀\r\n\r\n有的时候，你会遇到这两种后缀的证书：.pem；.crt。二者都是证书文件扩展名，主要区别如下：\r\n\r\n1. PEM格式是一种基于Base64编码的ASCII文本格式，可以包含证书、私钥等多种格式的加密信息。\r\n2. PEM格式的证书常常用在Unix/Linux系统中，在Apache、Nginx等服务器软件中也广泛使用。\r\n\r\n3. CRT格式是一种二进制编码格式，主要用于Windows操作系统中的程序中，并且通常只包含证书信息。\r\n\r\n所以当看到这两种后缀的证书时，不要懵，其实都是证书文件。\r\n\r\n\r\n\r\n## 非对称加密\r\n\r\n`对称`：左右两边的图案，符号，数据等是一致的，称之为互相对称。那么在这里的`左`和`右`，可以理解为`客户端`和`服务器`，需要一致的对象是密钥，可以理解为同一把钥匙，才能解开数据。这种方式我们称之为`对称加密`。\r\n\r\n`非对称`：左右两边的图案，符号，数据等是不一致的，称之为非对称。那么在这里的`左`和`右`，可以理解为`客户端`和`服务器`，需要的对象是不同的密钥，才能解开数据。这种方式我们称之为`非对称加密`。\r\n\r\n在非对称加密中，密钥需要两个：公钥，私钥。\r\n\r\n* 公钥：顾名思义，就是公开的密钥，大家都可以得到。公钥负责加密数据\r\n* 私钥：不对外公开，特殊加密过的密钥。私钥负责解密数据\r\n\r\n只要通过私钥解开了公钥加密过的数据，那么就表示是被信任的，数据安全隧道才会建立。\r\n\r\n![encryption](https://cdn.jsdelivr.net/gh/week2311/Images@main/encryption.png)\r\n\r\n## HTTPS工作原理\r\n\r\n至此，我们知道SSL证书是为了解决数据明文传输不安全的风险：窃听风险，篡改风险，冒充风险。\r\n\r\n以HTTP为例，在未通过SSL加密时，数据传输模式：\r\n\r\n![image-20240220093754487](https://cdn.jsdelivr.net/gh/week2311/Images@main/risk.png)\r\n\r\n为了解决上述的问题，使用SSL进行数据加密传输，一开始是采用的是对称加密，但是这一方式还是存在安全风险。\r\n\r\n因为对称加密使用一对相同的密钥进行加密，解密。且密钥的传输是放在报文当中，也会使得中间人拦截，并篡改报文中的真密钥，返回假密钥给到客户端。\r\n\r\n那么如何保证数据的加，解密是唯一性的，且到达客户端的公钥是正确的呢？答案就是上文说到的`非对称加密`和`证书`。\r\n\r\n一图胜千言：\r\n\r\n![procedure](https://cdn.jsdelivr.net/gh/week2311/Images@main/procedure.png)\r\n\r\n根据图中描述可知，文章开头出现的情况，属于Client（浏览器）得到的证书是未被验证通过的，未知的证书。\r\n\r\n## 结语\r\n\r\n随着时间的转变，技术的迭代更新，SSL也出现了安全问题。后续出现了它的继任者--TLS协议，该协议在SSL的基础之上发展而来，得到了进一步的改进和优化。\r\n\r\n总之，SSL/TLS协议可以实现加强数据在传输过程的安全，不被窃取，监听等。在对安全性要求高的场景下，还会用到专业的安全硬件设备。毕竟数据是所有。\r\n\r\n\r\n\r\n参考链接：\r\n\r\n* https://baijiahao.baidu.com/s?id=1685474345600994715&wfr=spider&for=pc\r\n\r\n* https://blog.csdn.net/qq_60243891/article/details/132530818\r\n\r\n* https://blog.csdn.net/keeppractice/article/details/126975243\r\n\r\n## 📝License\r\n\r\n[MIT](./LICENSE) © Week 100%"
        },
        {
          "id": "first-blog",
          "metadata": {
            "permalink": "/first-blog",
            "editUrl": "https://github.com/wrm244/docusaurus-theme-zen/edit/main/blog/01-first-blog.md",
            "source": "@site/blog/01-first-blog.md",
            "title": "Docusaurus博客搭建",
            "description": "网站由来",
            "date": "2024-02-15T19:20:00.000Z",
            "formattedDate": "2024年2月15日",
            "tags": [
              {
                "label": "docusaurus-theme-zen",
                "permalink": "/tags/docusaurus-theme-zen"
              },
              {
                "label": "lifestyle",
                "permalink": "/tags/lifestyle"
              }
            ],
            "readingTime": 2.84,
            "hasTruncateMarker": true,
            "authors": [
              {
                "name": "Week",
                "title": "静心",
                "url": "https://github.com/week2311",
                "imageURL": "/assets/images/social/avatar.png",
                "key": "Week"
              }
            ],
            "frontMatter": {
              "slug": "first-blog",
              "title": "Docusaurus博客搭建",
              "date": "2024-02-15 19:20",
              "tags": [
                "docusaurus-theme-zen",
                "lifestyle"
              ],
              "authors": "Week"
            },
            "prevItem": {
              "title": "SSL证书详解",
              "permalink": "/ssl证书详解"
            }
          },
          "content": "<!-- ![logo](/assets/images/avatar300.png) -->\n## 网站由来\n对于碎片化的信息，知识，一直都没有很好的整理。有做过文档笔记，但是查找起来很麻烦，有的时候还不如直接百度来的快，因此也出现了重复性的搜索，于是为了方便自己，提高效率，萌生了搭建博客的想法。\ndocusaurus博客框架符合我的美感，之前也看过其他的博客框架，例如Hero，Wiki，Wordpress，Hexo等，但都不太喜欢。\n于是开始了解docusaurus，但自身并不懂前端语言，于是就在看到了河山的博客，感觉很不错，接下来的工作就是进行二开了，经过了三，四天的修改，便有了今天的样貌。\n\n## 项目目录\n\n基于docusaurus搭建的主题，结合了简单易用与其他开源页面设计方案、支持MDX和React、可扩展和定制等优点，以及加上多设计美观、博客与文档一体的主题，为你提供了一个优秀的个人页面解决方案。该主题使用🦖 <a href=\"https://docusaurus.io/\">Docusaurus</a>搭建，遵循[MIT](./LICENSE)协议。\n\n<!-- truncate -->\n\n> This is a theme built with docusaurus, which combines the simplicity and ease of use of docusaurus with other open source page design solutions, supports MDX and React, is extensible and customizable, and also has a beautiful design, a blog and documentation integrated theme, providing you with an excellent personal page solution.\n\n\n## 项目目录\n\n```bash\n├── blog                           # 博客\n│   ├── first-blog.md\n│   └── authors.yml                # 作者信息(可以多个作者)\n├── docs                           # 文档/笔记\n│   └── stack\n│         └──introduction.md       # 笔记介绍\n├── data                           # 项目/导航/友链数据\n│   ├── friend.ts                  # 友链\n│   ├── project.ts                 # 项目\n│   └── resource.ts                # 资源导航\n├── i18n                           # 国际化\n├── src\n│   ├── components                 # 组件\n│   ├── css                        # 自定义CSS\n│   ├── pages                      # 自定义页面\n│   ├── plugin                     # 自定义插件\n│   └── theme                      # 自定义主题组件\n├── static                         # 静态资源文件\n│   └── assets                     # 静态文件\n├── docusaurus.config.js           # 站点的配置信息\n├── sidebars.js                    # 文档的侧边栏\n├── package.json\n└── yarn.lock                      # 建议使用yarn保留\n```\n\n## 安装\n\n克隆仓库并安装依赖\n```bash\ngit clone https://github.com/week2311/blog.git ./blog\ncd blog\nyarn\nyarn start\n```\n\n国内仓库备份\n```bash\ngit clone https://github.com/week2311/blog.git ./blog\ncd blog\nyarn\n```\n\n生成静态网页代码(./build)\n\n```bash\nyarn run build\n```\n\n启动服务\n```bash\nyarn run serve\n```\n\n## Netlify托管\n对于个人而言，购买一台服务器来运行项目无疑是一项不菲的支出，虽然有云服务器。\nNetlify很好的解决了这个问题，每个月有 免费的 100G 流量、300分钟的构建，还有全球的CDN节点，对于我来说已经很够用了。\n在Netlify官网，点击 Deploy to Netlify，根据官方步骤走了就行了，很方便快捷，但前提是你需要将项目放到github仓库，\n且需要有一个域名。\n\n## 域名访问\n虽然有白嫖的域名，但建议去买一个属于自己的域名，一是国内访问快速，稳定，二是不需要很多繁琐的申请，快速。\n而且一年也就一杯咖啡的钱，有的新用户也就1块钱，像我这个就是阿里云上购买申请的，还是很不错的。\nSSL证书的话，可以通过腾讯云上去申请免费1年的，获取证书文件，然后将文件内容替换netlify上的，即可实现https访问。\n\n## 📝License\n\n[MIT](./LICENSE) © Week 100%"
        }
      ],
      "tags": {
        "/tags/kubernetes": {
          "label": "kubernetes",
          "items": [
            "kwok",
            "K8s-quicktalk",
            "Service",
            "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜"
          ],
          "permalink": "/tags/kubernetes",
          "pages": [
            {
              "items": [
                "kwok",
                "K8s-quicktalk",
                "Service",
                "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜"
              ],
              "metadata": {
                "permalink": "/tags/kubernetes",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 4,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/kwok": {
          "label": "kwok",
          "items": [
            "kwok"
          ],
          "permalink": "/tags/kwok",
          "pages": [
            {
              "items": [
                "kwok"
              ],
              "metadata": {
                "permalink": "/tags/kwok",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/service": {
          "label": "service",
          "items": [
            "Service"
          ],
          "permalink": "/tags/service",
          "pages": [
            {
              "items": [
                "Service"
              ],
              "metadata": {
                "permalink": "/tags/service",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/rocky": {
          "label": "rocky",
          "items": [
            "CentOS7失宠，谁又会成为下一个甄嬛!",
            "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
            "Rocky Linux 9.3 系统安装"
          ],
          "permalink": "/tags/rocky",
          "pages": [
            {
              "items": [
                "CentOS7失宠，谁又会成为下一个甄嬛!",
                "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜",
                "Rocky Linux 9.3 系统安装"
              ],
              "metadata": {
                "permalink": "/tags/rocky",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 3,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/linux": {
          "label": "linux",
          "items": [
            "CentOS7失宠，谁又会成为下一个甄嬛!",
            "Rocky Linux 9.3 系统安装"
          ],
          "permalink": "/tags/linux",
          "pages": [
            {
              "items": [
                "CentOS7失宠，谁又会成为下一个甄嬛!",
                "Rocky Linux 9.3 系统安装"
              ],
              "metadata": {
                "permalink": "/tags/linux",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 2,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/centos": {
          "label": "centos",
          "items": [
            "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜"
          ],
          "permalink": "/tags/centos",
          "pages": [
            {
              "items": [
                "一键搞定！Kubernetes1.29.6高可用部署指南，老少皆宜"
              ],
              "metadata": {
                "permalink": "/tags/centos",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/tcp": {
          "label": "tcp",
          "items": [
            "Netstat命令运用之，深入理解网络连接"
          ],
          "permalink": "/tags/tcp",
          "pages": [
            {
              "items": [
                "Netstat命令运用之，深入理解网络连接"
              ],
              "metadata": {
                "permalink": "/tags/tcp",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/netstat": {
          "label": "netstat",
          "items": [
            "Netstat命令运用之，深入理解网络连接"
          ],
          "permalink": "/tags/netstat",
          "pages": [
            {
              "items": [
                "Netstat命令运用之，深入理解网络连接"
              ],
              "metadata": {
                "permalink": "/tags/netstat",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/ssl": {
          "label": "ssl",
          "items": [
            "ssl证书详解"
          ],
          "permalink": "/tags/ssl",
          "pages": [
            {
              "items": [
                "ssl证书详解"
              ],
              "metadata": {
                "permalink": "/tags/ssl",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/ca": {
          "label": "ca",
          "items": [
            "ssl证书详解"
          ],
          "permalink": "/tags/ca",
          "pages": [
            {
              "items": [
                "ssl证书详解"
              ],
              "metadata": {
                "permalink": "/tags/ca",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/docusaurus-theme-zen": {
          "label": "docusaurus-theme-zen",
          "items": [
            "first-blog"
          ],
          "permalink": "/tags/docusaurus-theme-zen",
          "pages": [
            {
              "items": [
                "first-blog"
              ],
              "metadata": {
                "permalink": "/tags/docusaurus-theme-zen",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        },
        "/tags/lifestyle": {
          "label": "lifestyle",
          "items": [
            "first-blog"
          ],
          "permalink": "/tags/lifestyle",
          "pages": [
            {
              "items": [
                "first-blog"
              ],
              "metadata": {
                "permalink": "/tags/lifestyle",
                "page": 1,
                "postsPerPage": 10,
                "totalPages": 1,
                "totalCount": 1,
                "blogDescription": "docusaurus-theme-zen",
                "blogTitle": "Blog"
              }
            }
          ]
        }
      }
    }
  }
}